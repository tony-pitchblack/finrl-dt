{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_sweep_rllib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "Y-J5mD_PTar9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "RfYDJoTXo6-J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# git clone https://github.com/tony-pitchblack/FinRL.git\n",
        "# cd ./FinRL\n",
        "# git checkout benchmarking\n",
        "# pip install -r FinRL/requirements.txt"
      ],
      "metadata": {
        "id": "Zw9eNgAjLqsV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tony-pitchblack/FinRL.git@benchmarking \\\n",
        "    # --force-reinstall --no-deps"
      ],
      "metadata": {
        "id": "w6nvjUhJQPQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d65cdf-0fb0-4f27-e43b-6d0e4a8fe5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tony-pitchblack/FinRL.git@benchmarking\n",
            "  Cloning https://github.com/tony-pitchblack/FinRL.git (to revision benchmarking) to /tmp/pip-req-build-m_p0ugwh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tony-pitchblack/FinRL.git /tmp/pip-req-build-m_p0ugwh\n",
            "  Running command git checkout -b benchmarking --track origin/benchmarking\n",
            "  Switched to a new branch 'benchmarking'\n",
            "  Branch 'benchmarking' set up to track remote branch 'benchmarking' from 'origin'.\n",
            "  Resolved https://github.com/tony-pitchblack/FinRL.git to commit 4a3c7f3f6a8e23c28f2ebd0f30796571ea448101\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-c8fsui_6/elegantrl_68b8d51ff50947128af8e7a0ebebf4de\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-c8fsui_6/elegantrl_68b8d51ff50947128af8e7a0ebebf4de\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 2fa34dd9236498beada8d8443d927970a9de1f7f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange-calendars<5,>=4 (from finrl==0.3.6)\n",
            "  Downloading exchange_calendars-4.9-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting jqdatasdk<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading jqdatasdk-1.9.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pyfolio<0.10,>=0.9 (from finrl==0.3.6)\n",
            "  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading pyportfolioopt-1.5.6-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ray<3,>=2 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.6) (1.6.1)\n",
            "Collecting stable-baselines3>=2.0.0a5 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading stable_baselines3-2.6.0a0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting stockstats<0.6,>=0.5 (from finrl==0.3.6)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl.metadata (26 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading wrds-3.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.6) (0.2.52)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.32.3)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.11.12)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (24.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (43.0.3)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (1.18.3)\n",
            "Collecting pyluach (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2025.1)\n",
            "Collecting korean_lunar_calendar (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (2.0.37)\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting thriftpy2!=0.5.1,>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2025.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.2)\n",
            "Collecting empyrical>=0.5.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.6.0)\n",
            "Collecting ecos<3.0.0,>=2.0.14 (from pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.5.0)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.10.6)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.70.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2024.10.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.1.0)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.6)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Collecting th (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6)\n",
            "  Downloading th-0.4.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading pycares-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (0.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.7.post3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.7.post2)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.11/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.6) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.11)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.22.3)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default,tune]<3,>=2->finrl==0.3.6) (1.17.2)\n",
            "Collecting niltype<2.0,>=0.3 (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6)\n",
            "  Downloading niltype-1.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.27.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.6.1)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.9-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.0/198.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyportfolioopt-1.5.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.6.0a0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading wrds-3.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
            "Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading th-0.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading niltype-1.0.2-py3-none-any.whl (5.3 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading pycares-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.6/288.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: finrl, msgpack, pyfolio, elegantrl, empyrical, thriftpy2\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.6-py3-none-any.whl size=4699705 sha256=bf4aa568213131dc008cb0a03ebe73d661cd57327a2b12ef97b976db6a7ca736\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjzk0vy0/wheels/e0/44/c0/13e3a1817a2032d4706226574adb60703205278f9a259301e9\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp311-cp311-linux_x86_64.whl size=15688 sha256=076fa5464dbf8a9c002191b554b306b62855763bc6f13275fa9cd186547c813b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/35/da/ed9b26b510235e00e3a3c3bab7bad97b59214729662255ab3d\n",
            "  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2-py3-none-any.whl size=88656 sha256=f0bca650c2859e3e8ed4d553613b58b6f7a7e65a6946086a1f197a8babc97ebb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/af/9e/7c343b822164a3147a3d395a1bcd05041c520a3bc6398fe88e\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=ElegantRL-0.3.10-py3-none-any.whl size=271935 sha256=b4e18adaf9e03dd8e1cb9dd16596978346f9a9efae070a33d8e87c32d05360ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjzk0vy0/wheels/9a/77/4d/6284111037b2dd64af9ef18d4d600d9c185cc2f6f09704e896\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39753 sha256=752dd1bf1951bb0c362dabfe60f247ccb230eb76fb78ead563141ced961165c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/1d/58/a7ae5ef5c8de7c4b769f24c2584f4706564921f031b16b9cb6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependencies listed in the requirements.txt file\n",
        "!wget https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt -O requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "1S3eA6pMHTxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU-vXqDRW3L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US_vB7hNSdeu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJsl_3tVre6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ],
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General funcs"
      ],
      "metadata": {
        "id": "oWn4ZCwkvtN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title YahooDownloader\n",
        "\n",
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
        "            )\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "\n",
        "        try:\n",
        "            # Convert wide to long format\n",
        "            # print(f\"DATA COLS: {data_df.columns}\")\n",
        "            data_df = data_df.sort_index(axis=1).set_index(['Date']).drop(columns=['tic']).stack(level='Ticker', future_stack=True)\n",
        "            data_df.reset_index(inplace=True)\n",
        "            data_df.columns.name = ''\n",
        "\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(columns={'Ticker': 'Tic', 'Adj Close': 'Adjcp'}, inplace=True)\n",
        "            data_df.rename(columns={col: col.lower() for col in data_df.columns}, inplace=True)\n",
        "\n",
        "            columns = [\n",
        "                \"date\",\n",
        "                \"tic\",\n",
        "                \"open\",\n",
        "                \"high\",\n",
        "                \"low\",\n",
        "                \"close\",\n",
        "                \"adjcp\",\n",
        "                \"volume\",\n",
        "            ]\n",
        "\n",
        "            data_df = data_df[columns]\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "gbd4N4QLPXlL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhIyXmfQ8EAS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title fix_daily_index\n",
        "\n",
        "def make_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    # Get unique dates and create a mapping to daily indices\n",
        "    total_dates = data_df[date_column].unique()\n",
        "    date_to_index = {date: idx for idx, date in enumerate(sorted(total_dates))}\n",
        "    return data_df[date_column].map(date_to_index)\n",
        "\n",
        "def set_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    \"\"\"\n",
        "    Constructs a daily index from unique dates in the specified column.\n",
        "\n",
        "    Parameters:\n",
        "        data_df (pd.DataFrame): The input DataFrame.\n",
        "        date_column (str): The name of the column containing dates.\n",
        "        new_index_name (str): The name for the new index.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a daily index.\n",
        "    \"\"\"\n",
        "\n",
        "    # Map dates to daily indices and set as index\n",
        "    data_df[new_index_name] = make_daily_index(data_df, date_column='date', new_index_name='date_index')\n",
        "\n",
        "    data_df.set_index(new_index_name, inplace=True)\n",
        "    data_df.index.name = ''  # Remove the index name for simplicity\n",
        "\n",
        "    return data_df\n",
        "\n",
        "def fix_daily_index(df):\n",
        "    if df.index.name == 'date':\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    daily_index = make_daily_index(df, date_column='date', new_index_name='date_index')\n",
        "    if (df.index.values != daily_index.values).any():\n",
        "\n",
        "        df.index = daily_index\n",
        "        df.index.name = ''\n",
        "\n",
        "    return df\n",
        "\n",
        "# trade = fix_daily_index(trade)\n",
        "# trade.index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get dataset name\n",
        "\n",
        "def get_quarterly_dataset_name(prefix, train_start_date, val_start_date, test_start_date):\n",
        "    get_quarter = lambda date: f'Q{(date.month - 1) // 3 + 1}'\n",
        "\n",
        "    val_quarter = get_quarter(val_start_date)\n",
        "    test_quarter = get_quarter(test_start_date)\n",
        "\n",
        "    # Extract year and month\n",
        "    train_start = f\"{train_start_date.year}-{train_start_date.month:02}\"\n",
        "    val_start = f\"{val_start_date.year}\"\n",
        "    test_start = f\"{test_start_date.year}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start} | {val_start} {val_quarter} | {test_start} {test_quarter}\"\n",
        "\n",
        "    return dataset_name\n",
        "\n",
        "def get_yearly_dataset_name(prefix, train_start, test_start, test_end):\n",
        "    # Extract year and month\n",
        "    train_start_str = f\"{train_start.year}-{train_start.month:02}\"\n",
        "    test_start_str = f\"{test_start.year}-{test_start.month:02}\"\n",
        "    test_end_str = f\"{test_end.year}-{test_end.month:02}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start_str} | {test_start_str} | {test_end_str}\"\n",
        "    return dataset_name\n"
      ],
      "metadata": {
        "id": "GQ6BIJxbwuVh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOfz3JlX-oG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title add_dataset\n",
        "\n",
        "def add_dataset(stock_index_name, train_df, test_df):\n",
        "    if 'datasets' not in globals():\n",
        "        global datasets\n",
        "        datasets = {}\n",
        "\n",
        "    # Ensure datetime format\n",
        "    if 'date' in train_df.columns:\n",
        "        train_df.set_index('date', inplace=True)\n",
        "    train_df.index = pd.to_datetime(train_df.index)\n",
        "\n",
        "    if 'date' in test_df.columns:\n",
        "        test_df.set_index('date', inplace=True)\n",
        "    test_df.index = pd.to_datetime(test_df.index)\n",
        "\n",
        "    train_start_date = train_df.index[0]\n",
        "    test_start_date = test_df.index[0]\n",
        "    test_end_date = test_df.index[-1]\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        stock_index_name,\n",
        "        train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    train_df.reset_index(inplace=True)\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    ticker_list = train_df.tic.unique().tolist()\n",
        "\n",
        "    datasets[dataset_name] = {\n",
        "        'train': train_df,\n",
        "        'test': test_df,\n",
        "        'metadata': dict(\n",
        "            stock_index_name = stock_index_name,\n",
        "            train_start_date = train_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "            num_tickers = len(ticker_list),\n",
        "            ticker_list = ticker_list,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUph5lzrTUS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA: DOW-30 (quarterly train/val/test)"
      ],
      "metadata": {
        "id": "LHIfVohUTAsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_date = '2015-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2016-10-01'"
      ],
      "metadata": {
        "id": "dTOtt7iQg7TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate_quarterly_date_ranges\n",
        "\n",
        "def generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=False,\n",
        "    finetune_previous_val=False\n",
        "):\n",
        "    is_quarter_start = lambda date: date.month in [1, 4, 7, 10] and date.day == 1\n",
        "\n",
        "    min_test_start_date = pd.Timestamp(min_test_start_date)\n",
        "    train_start_date = pd.Timestamp(train_start_date)\n",
        "    max_test_end_date = pd.Timestamp(max_test_end_date)\n",
        "\n",
        "    assert is_quarter_start(train_start_date), f\"train_start_date {train_start_date} is not a quarter start date.\"\n",
        "    assert is_quarter_start(min_test_start_date), f\"min_test_start_date {min_test_start_date} is not a quarter start date.\"\n",
        "\n",
        "    test_start_date = min_test_start_date\n",
        "    date_ranges = []\n",
        "    full_train_start_date = train_start_date\n",
        "\n",
        "    while True:\n",
        "        val_start_date = test_start_date - pd.DateOffset(months=3)\n",
        "        test_end_date = test_start_date + pd.DateOffset(months=3)\n",
        "\n",
        "        if test_end_date > max_test_end_date:\n",
        "            break\n",
        "\n",
        "        if len(date_ranges) == 0:\n",
        "            # The first date_range contains the full training period\n",
        "            train_start_date = full_train_start_date\n",
        "        elif finetune_previous_val:\n",
        "            # Use the previous validation range as the training range\n",
        "            train_start_date = date_ranges[-1]['val_start_date']\n",
        "\n",
        "        date_range = dict(\n",
        "            train_start_date=train_start_date,\n",
        "            val_start_date=val_start_date,\n",
        "            test_start_date=test_start_date,\n",
        "            test_end_date=test_end_date,\n",
        "        )\n",
        "\n",
        "        if return_strings:\n",
        "            date_range = {k: str(v) for k, v in date_range.items()}\n",
        "\n",
        "        date_ranges.append(date_range)\n",
        "\n",
        "        test_start_date = test_end_date\n",
        "\n",
        "    return date_ranges\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "# print(*date_ranges[:2], sep='\\n')\n",
        "print(*date_ranges, sep='\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DGMvKo0wxwYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7039d47a-bf35-4f76-8da9-15c5ae58b35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_start_date': Timestamp('2015-01-01 00:00:00'), 'val_start_date': Timestamp('2015-10-01 00:00:00'), 'test_start_date': Timestamp('2016-01-01 00:00:00'), 'test_end_date': Timestamp('2016-04-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2015-10-01 00:00:00'), 'val_start_date': Timestamp('2016-01-01 00:00:00'), 'test_start_date': Timestamp('2016-04-01 00:00:00'), 'test_end_date': Timestamp('2016-07-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2016-01-01 00:00:00'), 'val_start_date': Timestamp('2016-04-01 00:00:00'), 'test_start_date': Timestamp('2016-07-01 00:00:00'), 'test_end_date': Timestamp('2016-10-01 00:00:00')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8GdHjZWTcHu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title split_data\n",
        "\n",
        "def split_data(data_df, date_range):\n",
        "    def subset_date_range(df, start_date, end_date):\n",
        "        df = df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "        df = fix_daily_index(df)\n",
        "        return df\n",
        "\n",
        "    return {\n",
        "        'train': subset_date_range(data_df, date_range['train_start_date'], date_range['val_start_date']),\n",
        "        'val': subset_date_range(data_df, date_range['val_start_date'], date_range['test_start_date']),\n",
        "        'test': subset_date_range(data_df, date_range['test_start_date'], date_range['test_end_date']),\n",
        "    }\n",
        "\n",
        "# data_splits = split_data(preproc_df, date_ranges[0])\n",
        "# data_splits['train'].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prepare_data (for np env)\n",
        "import hashlib\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "import os\n",
        "\n",
        "CACHE_DIR = './cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def stable_hash(data):\n",
        "    return hashlib.sha256(str(data).encode()).hexdigest()\n",
        "\n",
        "def get_env_config(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    if_train,\n",
        "    ticker_list=DOW_30_TICKER,\n",
        "    technical_indicator_list=INDICATORS,\n",
        "    time_interval='1d',\n",
        "    if_vix=True,\n",
        "    **kwargs\n",
        "):\n",
        "    data_hash = stable_hash(tuple(sorted(ticker_list) + sorted(technical_indicator_list)))\n",
        "    file_path = Path(CACHE_DIR) / f\"{start_date}_{end_date}_{time_interval}_{data_hash}.csv\"\n",
        "    dp = DataProcessor(data_source='yahoofinance', tech_indicator=technical_indicator_list, vix=if_vix, **kwargs)\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Using cached data: {file_path}\")\n",
        "        data = pd.read_csv(file_path, index_col=0)\n",
        "    else:\n",
        "        print(\"Creating new data.\")\n",
        "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "        data = dp.clean_data(data)\n",
        "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "        if if_vix:\n",
        "            data = dp.add_vix(data)\n",
        "        data.to_csv(file_path)\n",
        "\n",
        "    (\n",
        "        price_array,\n",
        "        tech_array,\n",
        "        turbulence_array,\n",
        "        timestamp_array,\n",
        "    ) = dp.df_to_array(\n",
        "        data,\n",
        "        if_vix,\n",
        "        return_timestamps=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"timestamp_array\": timestamp_array,\n",
        "        \"if_train\": if_train\n",
        "    }\n",
        "\n",
        "    return env_config\n",
        "\n",
        "# date_range=date_ranges[0]\n",
        "# train_env_config = get_env_config(\n",
        "#     start_date=date_range['val_start_date'],\n",
        "#     end_date=date_range['test_start_date'],\n",
        "#     if_train=True\n",
        "# )\n",
        "# val_env_config = get_env_config(\n",
        "#     start_date=date_range['test_start_date'],\n",
        "#     end_date=date_range['test_end_date'],\n",
        "#     if_train=False\n",
        "# )"
      ],
      "metadata": {
        "id": "E34A0Yyhy4Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NYSbz9QQ7pS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb artifacts"
      ],
      "metadata": {
        "id": "KemUy0OKg-wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_artifact\n",
        "\n",
        "def update_artifact(folder_path, name_prefix, type):\n",
        "    \"\"\"\n",
        "    Create or update a W&B artifact consisting of a folder.\n",
        "\n",
        "    Args:\n",
        "        run: The current W&B run.\n",
        "        folder_path (str): Path to the folder to upload.\n",
        "        artifact_name (str): Name of the artifact.\n",
        "        artifact_type (str): Type of the artifact.\n",
        "    \"\"\"\n",
        "    run = wandb.run\n",
        "    artifact_name = f'{name_prefix}-{wandb.run.id}'\n",
        "\n",
        "    # Create a new artifact\n",
        "    artifact = wandb.Artifact(name=artifact_name, type=type)\n",
        "\n",
        "    # Add the folder to the artifact\n",
        "    artifact.add_dir(folder_path)\n",
        "\n",
        "    # Log the artifact to W&B\n",
        "    run.log_artifact(artifact)\n",
        "    print(f\"Artifact '{artifact_name}' has been updated and uploaded.\")"
      ],
      "metadata": {
        "id": "jF0Xbv9f631H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCmdRFmDh-CI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title update_model_artifacts\n",
        "\n",
        "def update_model_artifacts(log_results_folder=True):\n",
        "    if log_results_folder:\n",
        "        update_artifact(\n",
        "            folder_path = RESULTS_DIR,\n",
        "            name_prefix = 'results',\n",
        "            type = 'results'\n",
        "        )\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = TRAINED_MODEL_DIR,\n",
        "        name_prefix = 'trained_models',\n",
        "        type = 'trained_models'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_dataset_artifact\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def update_dataset_artifact(config, train_df, val_df=None, test_df=None):\n",
        "    DATASET_DIR = Path('./dataset')\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(DATASET_DIR / 'train_data.csv', index=False)\n",
        "\n",
        "    if test_df is not None:\n",
        "        test_df.to_csv(DATASET_DIR / 'test_data.csv', index=False)\n",
        "\n",
        "    if val_df is not None:\n",
        "        val_df.to_csv(DATASET_DIR / 'val_data.csv', index=False)\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = DATASET_DIR,\n",
        "        name_prefix = 'dataset',\n",
        "        type = 'dataset'\n",
        "    )"
      ],
      "metadata": {
        "id": "Prm8SfPo7CJY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_env_state_artifact\n",
        "\n",
        "def update_env_state_artifact(val_env_end_state):\n",
        "    file_path = 'val_env_end_state.csv'\n",
        "\n",
        "    df_last_state = pd.DataFrame({\"last_state\": val_env_end_state})\n",
        "    df_last_state.to_csv(\n",
        "        file_path, index=False\n",
        "    )\n",
        "\n",
        "    artifact = wandb.Artifact(name=f'val_env_end_state-{wandb.run.id}', type='env_state')\n",
        "    artifact.add_file(file_path)\n",
        "    wandb.run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "x66rQn0TGEkM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & helper funcs"
      ],
      "metadata": {
        "id": "_BZTsxX0tkDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_yearly_train_test\n",
        "def build_yearly_train_test(config):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        config['train_years_count'],\n",
        "        config['test_years_count'],\n",
        "        config['test_start_year']\n",
        "    )\n",
        "\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        train_start_date=train_start_date,\n",
        "        test_start_date=test_start_date,\n",
        "        test_end_date=test_end_date,\n",
        "        dataset_name=dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SmzfG0qaVP93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_quarterly_train_val_test\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "\n",
        "def build_quarterly_train_val_test(config):\n",
        "    date_range = {key: pd.Timestamp(date) for key, date in config['date_range'].items()}\n",
        "\n",
        "    train_start_date = date_range['train_start_date']\n",
        "    val_start_date = date_range['val_start_date']\n",
        "    test_start_date = date_range['test_start_date']\n",
        "    test_end_date = date_range['test_end_date']\n",
        "\n",
        "    train_env_config = get_env_config(\n",
        "        start_date=date_range['train_start_date'],\n",
        "        end_date=date_range['val_start_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    val_env_config = get_env_config(\n",
        "        start_date=date_range['val_start_date'],\n",
        "        end_date=date_range['test_start_date'],\n",
        "        if_train=True\n",
        "    )\n",
        "\n",
        "    test_env_config = get_env_config(\n",
        "        start_date=date_range['test_start_date'],\n",
        "        end_date=date_range['test_end_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    dataset_name = get_quarterly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, val_start_date, test_start_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        dataset_name = dataset_name\n",
        "    ))\n",
        "\n",
        "    return train_env_config, val_env_config, test_env_config\n",
        "\n",
        "# train_env_config, val_env_config, test_env_config = build_quarterly_train_val_test(config)"
      ],
      "metadata": {
        "id": "N2JCHB8ibDce",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init StockTradingEnv (numpy)\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.config import INDICATORS\n",
        "# from finrl.config import CACHE_DIR\n",
        "\n",
        "def init_env(\n",
        "    np_env_config,\n",
        "\n",
        "    # run_config,\n",
        "    initial_amount,\n",
        "    cost_pct,\n",
        "\n",
        "    mode,\n",
        "    turbulence_threshold=99,\n",
        "):\n",
        "    assert mode in ['train', 'val', 'test']\n",
        "\n",
        "    print('Initializing env...', end=' ')\n",
        "    env = StockTradingEnv(\n",
        "        config=np_env_config,\n",
        "        initial_capital=initial_amount,\n",
        "        buy_cost_pct=cost_pct,\n",
        "        sell_cost_pct=cost_pct,\n",
        "        turbulence_thresh=turbulence_threshold\n",
        "    )\n",
        "    print('Done.')\n",
        "\n",
        "    return env\n",
        "\n",
        "# env = init_env(\n",
        "#     train_np_env_config,\n",
        "#     run_config,\n",
        "#     'train'\n",
        "# )"
      ],
      "metadata": {
        "id": "Mp0H06urpp53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0-3fjeCG_GJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frdIBaq0pqe2"
      },
      "outputs": [],
      "source": [
        "#@title compute metrics\n",
        "import wandb\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(account_values: List[pd.DataFrame, pd.Series, np.array]):\n",
        "    \"\"\"\n",
        "    If DataFrame then should contain two columns - 'date' and name of algo, e.g. 'a2c'.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(account_values, pd.DataFrame):\n",
        "        assert isinstance(account_values, pd.DataFrame)\n",
        "        if 'date' not in account_values.columns:\n",
        "            if account_values.index.name == 'date':\n",
        "                account_values.reset_index(inplace=True)\n",
        "            else:\n",
        "                raise ValueError(\"should contain 'date' column or index\")\n",
        "        account_values = account_values.dropna().set_index('date').iloc[:, 0]\n",
        "    elif isinstance(account_values, np.ndarray):\n",
        "        account_values = pd.Series(account_values)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    sharpe = calculate_sharpe_ratio(account_values)\n",
        "    mdd = calculate_mdd(account_values)\n",
        "    cum_ret = (account_values.iloc[-1] - account_values.iloc[0]) / account_values.iloc[0] * 100\n",
        "    # num_days = (account_values.index.max() - account_values.index.min()).days\n",
        "    num_days = len(account_values)\n",
        "    ann_ret = ((1 + cum_ret / 100) ** (365 / num_days) - 1) * 100\n",
        "\n",
        "    return {\n",
        "            f'sharpe_ratio': sharpe,\n",
        "            f'mdd': mdd,\n",
        "            f'ann_return': ann_ret,\n",
        "            f'cum_return': cum_ret,\n",
        "        }\n",
        "\n",
        "def get_env_metrics(env):\n",
        "    end_total_asset = env.state[0] + sum(\n",
        "        np.array(env.state[1 : (env.stock_dim + 1)])\n",
        "        * np.array(env.state[(env.stock_dim + 1) : (env.stock_dim * 2 + 1)])\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'begin_total_asset': env.asset_memory[0],\n",
        "        'end_total_asset': end_total_asset,\n",
        "        'total_cost': env.cost,\n",
        "        'total_trades': env.trades,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG9DR-fz9e-s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_metrics\n",
        "\n",
        "def log_metrics(metrics, model_name, split_label, step=None):\n",
        "    print(f'log_metrics for {model_name}')\n",
        "\n",
        "    rename_metrics = lambda model_name: {\n",
        "        f\"{key}/{model_name}\": value for key, value in metrics.items()\n",
        "    }\n",
        "\n",
        "    renamed_metrics = rename_metrics(model_name)\n",
        "    wandb.log({split_label: renamed_metrics}, step=step)\n",
        "    # wandb.run.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_best_model_metrics\n",
        "\n",
        "def update_best_model_metrics(metrics, model_name, split_label):\n",
        "    if 'sharpe_ratios' not in wandb.run.config:\n",
        "        wandb.run.config['sharpe_ratios'] = {}\n",
        "\n",
        "    if split_label not in wandb.run.config['sharpe_ratios']:\n",
        "        wandb.run.config['sharpe_ratios'][split_label] = {}\n",
        "\n",
        "    sharpe_ratios = wandb.run.config['sharpe_ratios'][split_label]\n",
        "\n",
        "    print(f\"DEBUG ({split_label}): run.id = {wandb.run.id}\")\n",
        "    print(f\"DEBUG ({split_label}): sharpe_ratios = {sharpe_ratios}\")\n",
        "    print(f\"DEBUG ({split_label}): updating best model based on sharpe_ratios: {sharpe_ratios}\")\n",
        "    if len(sharpe_ratios) > 0:\n",
        "        best_model_name = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "        if metrics['sharpe_ratio'] > sharpe_ratios[best_model_name]:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" > {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                f\". New best model: {model_name}.\"\n",
        "            )\n",
        "            log_metrics(metrics, 'best_model', split_label)\n",
        "            wandb.log({split_label: {'best_model_name': model_name}})\n",
        "        else:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" <= {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                \". Not updating best model.\"\n",
        "            )\n",
        "    else:\n",
        "        print(f\"DEBUG ({split_label}): no models logged yet, new best model is current one: {model_name}\")\n",
        "        print(f\"DEBUG ({split_label}): wandb.run.config['sharpe_ratios'] = {wandb.run.config['sharpe_ratios']}\")\n",
        "        log_metrics(metrics, 'best_model', split_label)\n",
        "\n",
        "    wandb.run.config['sharpe_ratios'][split_label][model_name] = metrics['sharpe_ratio']"
      ],
      "metadata": {
        "cellView": "form",
        "id": "78Xhl6wmmY9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "znlXqpfQZkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title init\n",
        "parameters_dict = {}\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'test.sharpe_ratio/best_model'\n",
        "    },\n",
        "    'parameters': parameters_dict\n",
        "}"
      ],
      "metadata": {
        "id": "kF6LboMHB2xZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - yearly_train_test\n",
        "\n",
        "min_test_start_year = 2020\n",
        "max_test_start_year = 2025\n",
        "\n",
        "########################################################\n",
        "\n",
        "yearly_dataset_params = dict(\n",
        "    # dataset_period = {'value': 'year'},\n",
        "    # dataset_splits = {'parameters': {\n",
        "    #     'train': {'value': True },\n",
        "    #     'val': {'value': False },\n",
        "    #     'test': {'value': True },\n",
        "    # }},\n",
        "\n",
        "    dataset_type = {'value':'yearly_train_test'},\n",
        "\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_years_count = {'value': 10},\n",
        "    test_years_count = {'value': 1},\n",
        "    test_start_year = {\n",
        "        'values': list(range(min_test_start_year, max_test_start_year))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7RzkkJNiSkP7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - quarterly_train_test\n",
        "\n",
        "# Full date range (18 backtests)\n",
        "train_start_date = '2009-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2020-08-05'\n",
        "\n",
        "#################################################################\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date, min_test_start_date, max_test_end_date, return_strings=True\n",
        ")\n",
        "\n",
        "quarterly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'quarterly_train_val_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "    train_start_date = {'value': train_start_date},\n",
        "    min_test_start_date = {'value': min_test_start_date},\n",
        "    max_test_end_date = {'value': max_test_end_date},\n",
        "    date_range = {\n",
        "        'values': date_ranges\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "l-hEkHxAfYVv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: choose dataset\n",
        "parameters_dict.update(\n",
        "    # yearly_dataset_params,\n",
        "    quarterly_dataset_params\n",
        ")"
      ],
      "metadata": {
        "id": "GJDvneoY00zo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: env params\n",
        "parameters_dict.update(dict(\n",
        "    cost_pct = {'value': 1e-3},\n",
        "    initial_amount = {'value': 50_000},\n",
        "    turbulence_threshold = {\n",
        "        'value': 99,\n",
        "        # 'values': [30, 40, 50, 60, 70]\n",
        "    },\n",
        "    if_vix = {'value': True}\n",
        "))"
      ],
      "metadata": {
        "id": "ILF379nrW4YK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: models_used\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': True},\n",
        "    'if_using_ppo': {'value': True},\n",
        "    'if_using_td3': {'value': True},\n",
        "    'if_using_sac': {'value': True}\n",
        "})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PzWRRS6Prorh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: model and training params\n",
        "\n",
        "training_params = {\n",
        "    \"parameters\": {\n",
        "        \"ppo\": {\n",
        "            \"parameters\": dict(\n",
        "                steps={\"value\": 2048},\n",
        "                train_batch_size={\"value\": 2048},\n",
        "                num_epochs={\"value\": 10},\n",
        "                minibatch_size={\"value\": 128},\n",
        "                lr={\"value\": 5e-5},\n",
        "                gamma={\"value\": 0.99},\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "env_runners_params = {\n",
        "    'parameters': dict(\n",
        "        num_envs_per_env_runner = {'value': 1},\n",
        "        num_env_runners = {'value': 0},\n",
        "    )\n",
        "}\n",
        "\n",
        "parameters_dict.update({'env_runners_params': env_runners_params})\n",
        "parameters_dict.update({'training_params': training_params})"
      ],
      "metadata": {
        "id": "XaCu08TIKegP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & eval funcs"
      ],
      "metadata": {
        "id": "oqc-9r54tuXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kRSGp8MduO9_"
      },
      "outputs": [],
      "source": [
        "#@title MetricsLoggerCallback (class)\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from typing import Optional, Sequence\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "class MetricsLoggerCallback(DefaultCallbacks):\n",
        "    def __init__(self, model_name, ema_coeff=0.2, ma_window=20, log_to_wandb=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.ema_coeff = ema_coeff\n",
        "        self.ma_window = ma_window\n",
        "        self.metric_names = set()\n",
        "        self.log_to_wandb = log_to_wandb\n",
        "\n",
        "    def unwrap_env(self, env):\n",
        "        env = env.unwrapped\n",
        "        # print(type(env))\n",
        "        # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.vector.sync_vector_env.SyncVectorEnv'>\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            return env\n",
        "        else:\n",
        "            env = env.envs[0]\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.OrderEnforcing'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.PassiveEnvChecker'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'finrl.meta.env_stock_trading.env_stocktrading.StockTradingEnv'>\n",
        "        return env\n",
        "\n",
        "    def on_episode_step(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        env = self.unwrap_env(env)\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            asset_values = env.get_attr('asset_memory')\n",
        "            asset_values = pd.concat([pd.Series(av) for av in asset_values], axis=1).mean(axis=1)\n",
        "\n",
        "            # TODO: save_asset_memory\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            # asset_values = env.asset_memory\n",
        "            asset_values = env.save_asset_memory()\n",
        "\n",
        "        metrics = compute_metrics(asset_values)\n",
        "\n",
        "        # mode = env.mode\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            # metric_name = f\"{mode}/{metric_name}\" if mode != \"\" else metric_name\n",
        "            episode.add_temporary_timestep_data(metric_name, metric_value)\n",
        "            self.metric_names.update([metric_name])\n",
        "\n",
        "    def on_episode_end(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        for metric_name in self.metric_names:\n",
        "            metric_values = episode.get_temporary_timestep_data(metric_name)\n",
        "            metric_value = np.nanmean(np.array(metric_values))\n",
        "\n",
        "            # metrics_logger.log_value(\n",
        "            #     metric_name,\n",
        "            #     metric_value,\n",
        "            #     reduce='mean',\n",
        "            # )\n",
        "\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_EMA_{self.ema_coeff}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                ema_coeff=self.ema_coeff\n",
        "            )\n",
        "\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_ma_{self.ma_window}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                window=self.ma_window\n",
        "            )\n",
        "\n",
        "            if self.log_to_wandb:\n",
        "                mode = 'val' if env_runner.config.in_evaluation else 'train'\n",
        "                wandb.log({\n",
        "                    f\"{mode}.{metric_name}/{self.model_name}\": metric_value,\n",
        "                }) # TODO: log on every episode step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title print_result\n",
        "\n",
        "RESULT_KEYS_TO_INCLUDE = [\n",
        "    'sharpe_ratio_MA',\n",
        "    'ann_return_MA',\n",
        "    'mdd_MA',\n",
        "\n",
        "    'sharpe_ratio_EMA',\n",
        "    'ann_return_EMA',\n",
        "    'mdd_EMA',\n",
        "]\n",
        "\n",
        "def print_result(result):\n",
        "    print()\n",
        "    for key in result['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"train/{key}: {round(result['env_runners'][key], 2)}\")\n",
        "                break\n",
        "\n",
        "    for key in result['evaluation']['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"val/{key}: {round(result['evaluation']['env_runners'][key], 2)}\")\n",
        "                break\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gg8DIuPJ9FwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0iyYcwTOZYP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title benchmark_exec_time\n",
        "import pandas as pd\n",
        "from time import perf_counter\n",
        "from functools import wraps\n",
        "\n",
        "def benchmark_exec_time(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "\n",
        "        start = perf_counter()\n",
        "        output = func(*args, **kwargs)\n",
        "        end = perf_counter()\n",
        "\n",
        "        exec_time_sec = end - start\n",
        "\n",
        "        data = {\n",
        "            \"func_name\": func.__name__,\n",
        "            \"exec_time_sec\": exec_time_sec,\n",
        "        }\n",
        "        # print(f'\\nBenchmark results: {data}')\n",
        "        return output, exec_time_sec\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train_eval_models\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_models = {}\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    pretrained_models = {}\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            print(f\"Training {model_name.upper()} agent\")\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "            algo = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = pretrained_models.get(model_name, None)\n",
        "            )\n",
        "\n",
        "            print(f\"Evaluating {model_name.upper()} agent\")\n",
        "            val_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = val_np_env_config,\n",
        "                mode='val',\n",
        "            )\n",
        "            fig = plot_results(**val_result)\n",
        "            log_plot_as_artifact(fig, \"asset_value_comparison_validation\", artifact_type=\"plot\")\n",
        "\n",
        "            test_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = test_np_env_config,\n",
        "                mode='test',\n",
        "            )\n",
        "            fig = plot_results(**test_result)\n",
        "            log_plot_as_artifact(fig, \"test_asset_value_comparison_test\", artifact_type=\"plot\")\n",
        "            pretrained_models[model_name] = algo\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")\n",
        "\n",
        "    return pretrained_models"
      ],
      "metadata": {
        "id": "TLDeH1lN2XAS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15pANCivoIru",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train RLlib model\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from ray.tune.registry import register_env\n",
        "from time import perf_counter\n",
        "from math import ceil\n",
        "import ray\n",
        "\n",
        "def train_rllib_model(\n",
        "    run_config,\n",
        "    algo_name,\n",
        "    algo_cls_config,\n",
        "    train_np_env_config,\n",
        "    val_np_env_config,\n",
        "    pretrained_algo=None\n",
        "):\n",
        "    num_envs_per_env_runner = run_config['env_runners_params']['num_envs_per_env_runner']\n",
        "    num_env_runners = run_config['env_runners_params']['num_env_runners']\n",
        "\n",
        "    if pretrained_algo:\n",
        "        algo = pretrained_algo\n",
        "        print(f'Using pretrained {algo_name} agent.')\n",
        "    else:\n",
        "        algo_config = (\n",
        "            algo_cls_config()\n",
        "            .environment(\n",
        "                env=\"stock_trading_env\",\n",
        "                env_config={\n",
        "                    # \"df\": train,\n",
        "                    \"np_env_config\": train_np_env_config,\n",
        "\n",
        "                    # \"run_config\": run_config,\n",
        "                    \"initial_amount\": run_config['initial_amount'],\n",
        "                    \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                    \"mode\": 'train'\n",
        "                },\n",
        "            )\n",
        "            .env_runners(\n",
        "                num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "                num_env_runners=num_env_runners,\n",
        "                num_cpus_per_env_runner= (2/num_env_runners) if num_env_runners > 2 else None,\n",
        "\n",
        "                # gym_env_vectorize_mode=gym.envs.registration.VectorizeMode.ASYNC,\n",
        "            )\n",
        "            .training(\n",
        "                train_batch_size=2048,\n",
        "                num_epochs=10,\n",
        "                minibatch_size=128,\n",
        "            )\n",
        "            .evaluation(\n",
        "                # Set up the validation environment\n",
        "                evaluation_interval=1,  # Specify evaluation frequency (1=after each training step)\n",
        "                evaluation_config={\n",
        "                    \"env\": \"stock_trading_env\",\n",
        "                    \"env_config\": {\n",
        "                        # \"df\": val,\n",
        "                        \"np_env_config\": val_np_env_config,\n",
        "\n",
        "                        # \"run_config\": run_config,\n",
        "                        \"initial_amount\": run_config['initial_amount'],\n",
        "                        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                        \"mode\": 'val'\n",
        "                    },\n",
        "                },\n",
        "            )\n",
        "            # .callbacks(MetricsLoggerCallback)\n",
        "            .callbacks(partial(MetricsLoggerCallback, model_name='ppo', log_to_wandb=True))\n",
        "            .resources(\n",
        "                num_gpus=1 if torch.cuda.is_available() else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 2. build the algorithm ..\n",
        "        if algo_config.num_env_runners > 0:\n",
        "            ray.shutdown()\n",
        "            ray.init()\n",
        "\n",
        "            register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "        algo = algo_config.build()\n",
        "        print(f'Created new {algo_name} agent.')\n",
        "\n",
        "    print(f\"Envs: {algo.config.num_envs_per_env_runner}, Runners: {algo.config.num_env_runners}\")\n",
        "\n",
        "    # 3. .. train it ..\n",
        "    @benchmark_exec_time\n",
        "    def _train_rllib(total_timesteps):\n",
        "        print('Started training.')\n",
        "        results = []\n",
        "        total_batches = ceil(total_timesteps / algo.config.train_batch_size)\n",
        "        print(f\"total_batches: {total_batches}\")\n",
        "        print(f\"total_timesteps: {total_timesteps}\")\n",
        "        for _ in range(total_batches):\n",
        "            result = algo.train()\n",
        "            results.append(result)\n",
        "\n",
        "        print_result(result)\n",
        "        print('Training complete.')\n",
        "        return results\n",
        "\n",
        "    results, exec_time_sec = _train_rllib(run_config['training_params'][algo_name]['steps'])\n",
        "    print(f\"TRAINING DURATION: {exec_time_sec}\")\n",
        "\n",
        "    # Log train duration\n",
        "    duration_minutes = round(exec_time_sec / 60, 1)\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "    update_model_artifacts(log_results_folder=False)\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = (Path(TRAINED_MODEL_DIR) / algo_name).resolve()\n",
        "    algo.save(ckpt_path)\n",
        "\n",
        "    return algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk0_pf5m9s_M"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate RLlib model\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
        "from ray.rllib.models.torch.torch_distributions import TorchDiagGaussian\n",
        "\n",
        "import torch\n",
        "\n",
        "# Create the testing environment\n",
        "def evaluate_model(algo, model_name, mode, np_env_config, run_config, turbulence_thresh=None):\n",
        "    if turbulence_thresh is None:\n",
        "        turbulence_thresh = run_config.get('turbulence_thresh', 99)\n",
        "\n",
        "    turbulence_name = 'turbulence' if not run_config.get('if_vix', None) else '^VIX'\n",
        "\n",
        "    print(f\"Evaluating on `{mode}` using `{turbulence_name}`: {turbulence_thresh}\")\n",
        "\n",
        "    env_config = {\n",
        "        # \"df\": val,\n",
        "\n",
        "        \"np_env_config\": np_env_config,\n",
        "\n",
        "        # \"run_config\": run_config,\n",
        "        \"initial_amount\": run_config['initial_amount'],\n",
        "        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "        \"mode\": mode,\n",
        "        'turbulence_threshold': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    eval_env = create_stock_trading_env(env_config)\n",
        "    state, info = eval_env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Perform inference using the trained RLlib agent\n",
        "    rl_module = algo.env_runner.module\n",
        "    while not done:\n",
        "        # Compute action using the RLlib trained agent\n",
        "        input_dict = {Columns.OBS: torch.Tensor(state).unsqueeze(0)}\n",
        "        rl_module_out = rl_module.forward_inference(input_dict)\n",
        "        logits = rl_module_out[Columns.ACTION_DIST_INPUTS]\n",
        "\n",
        "        # Take mean of multivariate Gaussian distribution\n",
        "        mean, log_std = logits.chunk(2, dim=-1)\n",
        "\n",
        "        # action_distribution = TorchDiagGaussian.from_logits(logits)\n",
        "        # action_distribution = action_distribution.to_deterministic()\n",
        "        # assert np.allclose(mean, action_distribution.loc)\n",
        "        # assert np.allclose(log_std.exp(), action_distribution._dist.scale)\n",
        "        # action = action_distribution.sample()\n",
        "\n",
        "        action = mean.detach().numpy().squeeze()\n",
        "\n",
        "        # Clip the action to ensure it's within the action space bounds\n",
        "        action = np.clip(action, eval_env.action_space.low, eval_env.action_space.high)\n",
        "\n",
        "        # Perform action\n",
        "        state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "    df_account_value = eval_env.save_asset_memory()\n",
        "    metrics = compute_metrics(df_account_value)\n",
        "\n",
        "    turb_name = 'ti' if not run_config.get('if_vix', None) else 'vix'\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        metric_name = f\"{mode}.{turb_name}_{turbulence_thresh}.{metric_name}/{model_name}\"\n",
        "        wandb.run.summary[metric_name] = metric_value\n",
        "\n",
        "    turbulence_series = pd.Series(\n",
        "        np_env_config['turbulence_array'][:len(df_account_value)],\n",
        "        index=df_account_value['date'],\n",
        "        name=turbulence_name\n",
        "    )\n",
        "\n",
        "    eval_result = {\n",
        "        'account_value': df_account_value.rename(columns={'account_value': model_name}),\n",
        "        'turbulence_series': turbulence_series,\n",
        "        'turbulence_thresh': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    return eval_result\n",
        "\n",
        "# val_result = evaluate_model(\n",
        "#     algo,\n",
        "#     model_name,\n",
        "#     run_config=run_config,\n",
        "#     np_env_config = val_np_env_config,\n",
        "#     mode='val',\n",
        "# )\n",
        "\n",
        "# test_result = evaluate_model(\n",
        "#     algo,\n",
        "#     model_name,\n",
        "#     run_config=run_config,\n",
        "#     np_env_config = test_np_env_config,\n",
        "#     mode='test',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbLgPDKNPQX3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title plot_results (w/separate turbulence plot)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_results(\n",
        "        account_value,\n",
        "        turbulence_series,\n",
        "        turbulence_thresh,\n",
        "    ):\n",
        "\n",
        "    assert turbulence_series.name in ['turbulence', '^VIX']\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "    # Main plot\n",
        "    method_styles = {\n",
        "        'A2C': {'color': '#8c564b', 'linestyle': '--'},\n",
        "        'DDPG': {'color': '#e377c2', 'linestyle': '-'},\n",
        "        'PPO': {'color': '#7f7f7f', 'linestyle': '-'},\n",
        "        'TD3': {'color': '#bcbd22', 'linestyle': '--'},\n",
        "        'SAC': {'color': '#17becf', 'linestyle': '-'},\n",
        "        'DJIA': {'color': '#000000', 'linestyle': '-'},\n",
        "    }\n",
        "\n",
        "    if 'DJIA' in account_value:\n",
        "        ax1.plot(account_value.index, account_value['DJIA'], label=\"Dow Jones Index\",\n",
        "                linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "    if 'date' in account_value.columns:\n",
        "        account_value.set_index('date', inplace=True)\n",
        "\n",
        "    account_value.rename(columns={col: col.upper() for col in account_value.columns}, inplace=True)\n",
        "\n",
        "    for model_name in account_value.columns:\n",
        "        style = method_styles[model_name]\n",
        "        ax1.plot(account_value.index, account_value[model_name], label=model_name, **style)\n",
        "\n",
        "    # Customize main plot\n",
        "    ax1.set_title(\"Performance Comparison of DRL Agents\", fontsize=20, fontweight='bold')\n",
        "    ax1.set_ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Turbulence plot\n",
        "    if turbulence_series.name == 'turbulence':\n",
        "        turbulence_label = \"Turbulence Index\"\n",
        "    else:\n",
        "        turbulence_label = \"VIX Coefficient\"\n",
        "    ax2.plot(turbulence_series.index, turbulence_series, label=\"Turbulence Index\", color='red', linestyle='--', linewidth=2)\n",
        "    ax2.axhline(y=turbulence_thresh, color='red', linestyle=':', label='Turbulence Threshold')\n",
        "    ax2.set_ylabel(turbulence_label, fontsize=16, fontweight='bold')\n",
        "    ax2.legend(loc='lower left')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "    max_turbulence = max(turbulence_series.max(), turbulence_thresh)\n",
        "    ax2.set_ylim(0, max_turbulence + 10)\n",
        "    ax2.set_yticks(list(ax2.get_yticks()) + [turbulence_thresh])\n",
        "\n",
        "    # Shared x-axis label\n",
        "    ax2.set_xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# fig = plot_results(**val_result)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02OzYWBGMVNE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_plot_as_artifact\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "def log_plot_as_artifact(fig, artifact_name_prefix, artifact_type=\"plot\"):\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure without clipping and log it as a W&B artifact.\n",
        "\n",
        "    Parameters:\n",
        "        fig (matplotlib.figure.Figure): The Matplotlib figure to save and log.\n",
        "        artifact_name (str): The name of the W&B artifact.\n",
        "        artifact_type (str): The type of the artifact (default is \"plot\").\n",
        "        filename (str): The filename to save the plot as (default is \"plot.png\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get full artifact name\n",
        "        artifact_name = f'{artifact_name_prefix}-{wandb.run.id}'\n",
        "        filename = artifact_name + '.png'\n",
        "\n",
        "        # Save the figure with tight layout and proper padding\n",
        "        fig.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "        plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "        # Create and log the W&B artifact\n",
        "        artifact = wandb.Artifact(artifact_name, type=artifact_type)\n",
        "        artifact.add_file(filename, skip_cache=True, overwrite=True)\n",
        "        wandb.log_artifact(artifact)\n",
        "    finally:\n",
        "        # Ensure the file is deleted after use\n",
        "        if os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "\n",
        "# log_plot_as_artifact(fig, \"performance_comparison_DRL_agents\", artifact_type=\"plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run sweep"
      ],
      "metadata": {
        "id": "fXntnmmhuSsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSosu3u-YIIA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sweep Runner\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "def set_run_name(prefix, n=5):\n",
        "    run_name = f\"{prefix} | {wandb.run.id}\"\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "class SweepRunner:\n",
        "    def __init__(self, sweep_id):\n",
        "        self.sweep_id = sweep_id\n",
        "        self.pretrained_models = {}\n",
        "\n",
        "    def main(self, run_config=None):\n",
        "        run_timer_start = perf_counter()\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"START TIME:\", current_time)\n",
        "\n",
        "        with wandb.init(config=run_config):\n",
        "            run_config = wandb.config\n",
        "            # print('Debug copy run_config...', end=' ')\n",
        "            # _ = deepcopy(run_config)\n",
        "            # print(\"Done.\")\n",
        "\n",
        "            if run_config['dataset_type'] == 'yearly_train_test':\n",
        "                raise NotImplementedError\n",
        "            elif run_config['dataset_type'] == 'quarterly_train_val_test':\n",
        "                train_np_env_config, val_np_env_config, test_np_env_config = build_quarterly_train_val_test(run_config)\n",
        "                set_run_name(run_config['dataset_name'])\n",
        "\n",
        "                if self.pretrained_models:\n",
        "                    run_config.update({'finetune': True})\n",
        "                else:\n",
        "                    run_config.update({'finetune': False})\n",
        "\n",
        "                pretrained_models = train_eval_rllib_models(\n",
        "                    run_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config,\n",
        "                    pretrained_models=self.pretrained_models\n",
        "                )\n",
        "\n",
        "                self.pretrained_models = pretrained_models\n",
        "\n",
        "            ray.shutdown()\n",
        "\n",
        "            run_timer_end = perf_counter()\n",
        "            run_duration_minutes = round( (run_timer_end - run_timer_start) / 60, 1)\n",
        "            wandb.run.summary[f\"run.duration_minutes\"] = run_duration_minutes\n",
        "            print(f\"RUN DURATION: {run_duration_minutes}\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"END TIME:\", current_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN SWEEP\n",
        "def run_sweep(n_runs, sweep_config=None, sweep_id=None):\n",
        "    if sweep_id is None:\n",
        "        assert sweep_config is not None\n",
        "        sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "    else:\n",
        "        assert sweep_config is None\n",
        "\n",
        "    !rm -rf {TRAINED_MODEL_DIR}/*\n",
        "\n",
        "    sweep_runner = SweepRunner(sweep_id)\n",
        "    wandb.agent(sweep_id, sweep_runner.main, project=PROJECT, count=n_runs)\n",
        "\n",
        "run_sweep(\n",
        "    # sweep_id='50a5ela4',\n",
        "    sweep_config=sweep_config,\n",
        "\n",
        "    # n_runs=None\n",
        "    n_runs=2,\n",
        ")"
      ],
      "metadata": {
        "id": "_jqFSQXXM4Z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d55418c-217e-4391-f81b-13592ca5c2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: z7s4oarb\n",
            "Sweep URL: https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ld97xqqa with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcost_pct: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_type: quarterly_train_val_test\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdate_range: {'test_end_date': '2016-04-01 00:00:00', 'test_start_date': '2016-01-01 00:00:00', 'train_start_date': '2009-01-01 00:00:00', 'val_start_date': '2015-10-01 00:00:00'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_runners_params: {'num_env_runners': 0, 'num_envs_per_env_runner': 1}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_a2c: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ddpg: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ppo: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_sac: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_td3: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_vix: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_amount: 50000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_test_end_date: 2020-08-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_test_start_date: 2016-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tstock_index_name: DOW-30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_start_date: 2009-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_params: {'ppo': {'gamma': 0.99, 'lr': 5e-05, 'minibatch_size': 128, 'num_epochs': 10, 'steps': 2048, 'train_batch_size': 2048}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tturbulence_threshold: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TIME: 2025-02-10 12:04:35.587420\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250210_120436-ld97xqqa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/ld97xqqa' target=\"_blank\">peachy-sweep-1</a></strong> to <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/ld97xqqa' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/ld97xqqa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached data: cache/2009-01-01 00:00:00_2015-10-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:04:38,717\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached data: cache/2015-10-01 00:00:00_2016-01-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Using cached data: cache/2016-01-01 00:00:00_2016-04-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Training PPO agent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:04:40,049\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:04:41,944\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new ppo agent.\n",
            "Envs: 1, Runners: 0\n",
            "Started training.\n",
            "total_batches: 1\n",
            "total_timesteps: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:05:30,195\tWARNING worker.py:1504 -- SIGTERM handler is not set because current thread is not the main thread.\n",
            "2025-02-10 12:05:32,047\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./trained_models)... Done. 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train/mdd_EMA_0.2: -32.68\n",
            "train/sharpe_ratio_EMA_0.2: 0.78\n",
            "train/ann_return_EMA_0.2: 41.38\n",
            "val/ann_return_EMA_0.2: 1193.55\n",
            "val/mdd_EMA_0.2: -4.12\n",
            "val/sharpe_ratio_EMA_0.2: 5.14\n",
            "\n",
            "Training complete.\n",
            "TRAINING DURATION: 55.85129105800024\n",
            "Artifact 'trained_models-ld97xqqa' has been updated and uploaded.\n",
            "Evaluating PPO agent\n",
            "Evaluating on `val` using `^VIX`: 99\n",
            "Initializing env... Done.\n",
            "Evaluating on `test` using `^VIX`: 99\n",
            "Initializing env... Done.\n",
            "RUN DURATION: 1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train.ann_return/ppo</td><td>▁</td></tr><tr><td>train.cum_return/ppo</td><td>▁</td></tr><tr><td>train.mdd/ppo</td><td>▁</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>▁</td></tr><tr><td>val.ann_return/ppo</td><td>▁▁██▁▆▁█▁▁</td></tr><tr><td>val.cum_return/ppo</td><td>▂▃▄▆▁▇▄█▅▃</td></tr><tr><td>val.mdd/ppo</td><td>▁▆▅▇▂▆▇▂█▅</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>▂▂▃▄▁▅▂▄█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>run.duration_minutes</td><td>1.1</td></tr><tr><td>test.vix_99.ann_return/ppo</td><td>18.28185</td></tr><tr><td>test.vix_99.cum_return/ppo</td><td>2.89306</td></tr><tr><td>test.vix_99.mdd/ppo</td><td>-9.90202</td></tr><tr><td>test.vix_99.sharpe_ratio/ppo</td><td>0.62336</td></tr><tr><td>train.ann_return/ppo</td><td>41.37578</td></tr><tr><td>train.cum_return/ppo</td><td>137.05654</td></tr><tr><td>train.duration_minutes/ppo</td><td>0.9</td></tr><tr><td>train.mdd/ppo</td><td>-32.67896</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>0.78028</td></tr><tr><td>val.ann_return/ppo</td><td>234.95824</td></tr><tr><td>val.cum_return/ppo</td><td>4.71933</td></tr><tr><td>val.mdd/ppo</td><td>-4.08899</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>4.39421</td></tr><tr><td>val.vix_99.ann_return/ppo</td><td>33.30003</td></tr><tr><td>val.vix_99.cum_return/ppo</td><td>5.16907</td></tr><tr><td>val.vix_99.mdd/ppo</td><td>-4.06737</td></tr><tr><td>val.vix_99.sharpe_ratio/ppo</td><td>1.23193</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DOW-30 | 2009-01 | 2015 Q4 | 2016 Q1 | ld97xqqa</strong> at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/ld97xqqa' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/ld97xqqa</a><br> View project at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a><br>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250210_120436-ld97xqqa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END TIME: 2025-02-10 12:05:45.881430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mtijco5j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcost_pct: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_type: quarterly_train_val_test\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdate_range: {'test_end_date': '2016-07-01 00:00:00', 'test_start_date': '2016-04-01 00:00:00', 'train_start_date': '2009-01-01 00:00:00', 'val_start_date': '2016-01-01 00:00:00'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_runners_params: {'num_env_runners': 0, 'num_envs_per_env_runner': 1}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_a2c: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ddpg: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ppo: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_sac: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_td3: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_vix: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_amount: 50000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_test_end_date: 2020-08-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_test_start_date: 2016-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tstock_index_name: DOW-30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_start_date: 2009-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_params: {'ppo': {'gamma': 0.99, 'lr': 5e-05, 'minibatch_size': 128, 'num_epochs': 10, 'steps': 2048, 'train_batch_size': 2048}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tturbulence_threshold: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TIME: 2025-02-10 12:05:49.731676\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250210_120550-mtijco5j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/mtijco5j' target=\"_blank\">eternal-sweep-2</a></strong> to <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/z7s4oarb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/mtijco5j' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/mtijco5j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached data: cache/2009-01-01 00:00:00_2016-01-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:05:52,493\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cached data: cache/2016-01-01 00:00:00_2016-04-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Using cached data: cache/2016-04-01 00:00:00_2016-07-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Training PPO agent\n",
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "2025-02-10 12:05:53,444\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:05:55,194\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new ppo agent.\n",
            "Envs: 1, Runners: 0\n",
            "Started training.\n",
            "total_batches: 1\n",
            "total_timesteps: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-10 12:06:44,290\tWARNING worker.py:1504 -- SIGTERM handler is not set because current thread is not the main thread.\n",
            "2025-02-10 12:06:47,509\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./trained_models)... Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train/mdd_EMA_0.2: -23.46\n",
            "train/sharpe_ratio_EMA_0.2: 1.11\n",
            "train/ann_return_EMA_0.2: 78.77\n",
            "val/ann_return_EMA_0.2: -62.37\n",
            "val/mdd_EMA_0.2: -13.84\n",
            "val/sharpe_ratio_EMA_0.2: -105.68\n",
            "\n",
            "Training complete.\n",
            "TRAINING DURATION: 57.03280731299992\n",
            "Artifact 'trained_models-mtijco5j' has been updated and uploaded.\n",
            "Evaluating PPO agent\n",
            "Evaluating on `val` using `^VIX`: 99\n",
            "Initializing env... Done.\n",
            "Evaluating on `test` using `^VIX`: 99\n",
            "Initializing env... Done.\n",
            "RUN DURATION: 1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train.ann_return/ppo</td><td>▁</td></tr><tr><td>train.cum_return/ppo</td><td>▁</td></tr><tr><td>train.mdd/ppo</td><td>▁</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>▁</td></tr><tr><td>val.ann_return/ppo</td><td>▃▂▂▂▁█▃▃▂▃</td></tr><tr><td>val.cum_return/ppo</td><td>▇▆▄▃▁█▇▇▆█</td></tr><tr><td>val.mdd/ppo</td><td>█▇▃▂▁▆▆▇▅▇</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>████████▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>run.duration_minutes</td><td>1.1</td></tr><tr><td>test.vix_99.ann_return/ppo</td><td>-13.49468</td></tr><tr><td>test.vix_99.cum_return/ppo</td><td>-2.54852</td></tr><tr><td>test.vix_99.mdd/ppo</td><td>-9.81418</td></tr><tr><td>test.vix_99.sharpe_ratio/ppo</td><td>-0.58084</td></tr><tr><td>train.ann_return/ppo</td><td>78.76649</td></tr><tr><td>train.cum_return/ppo</td><td>231.21964</td></tr><tr><td>train.duration_minutes/ppo</td><td>1</td></tr><tr><td>train.mdd/ppo</td><td>-23.45924</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>1.11113</td></tr><tr><td>val.ann_return/ppo</td><td>-57.6273</td></tr><tr><td>val.cum_return/ppo</td><td>-6.43009</td></tr><tr><td>val.mdd/ppo</td><td>-12.15365</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>-4.73927</td></tr><tr><td>val.vix_99.ann_return/ppo</td><td>-4.21772</td></tr><tr><td>val.vix_99.cum_return/ppo</td><td>-0.72931</td></tr><tr><td>val.vix_99.mdd/ppo</td><td>-12.8556</td></tr><tr><td>val.vix_99.sharpe_ratio/ppo</td><td>-0.06121</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DOW-30 | 2009-01 | 2016 Q1 | 2016 Q2 | mtijco5j</strong> at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/mtijco5j' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/mtijco5j</a><br> View project at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a><br>Synced 5 W&B file(s), 0 media file(s), 104 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250210_120550-mtijco5j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END TIME: 2025-02-10 12:07:14.344546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_baselines3\n",
        "stable_baselines3.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3Cr9g7hQ4HmX",
        "outputId": "78bd9591-719e-4feb-a0b6-79c5ba0951b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0a0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.logger import configure"
      ],
      "metadata": {
        "id": "0TZJKFBjtCTC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y-J5mD_PTar9",
        "SIU-vXqDRW3L",
        "oWn4ZCwkvtN3",
        "hWUph5lzrTUS",
        "LHIfVohUTAsG",
        "NYSbz9QQ7pS8",
        "KemUy0OKg-wX",
        "znlXqpfQZkzU"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPjIMUfYIhLa1jwtQGBWOfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}