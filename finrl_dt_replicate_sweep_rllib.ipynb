{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_sweep_rllib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "Y-J5mD_PTar9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3=='2.6.0a0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRswNd6LH7lw",
        "outputId": "238ffbae-72c2-409f-f08a-c2103b58e019"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3==2.6.0a0\n",
            "  Downloading stable_baselines3-2.6.0a0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3==2.6.0a0) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3==2.6.0a0) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.6.0a0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.6.0a0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.6.0a0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0a0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "RfYDJoTXo6-J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# git clone https://github.com/tony-pitchblack/FinRL.git\n",
        "# cd ./FinRL\n",
        "# git checkout benchmarking\n",
        "# pip install -r FinRL/requirements.txt"
      ],
      "metadata": {
        "id": "Zw9eNgAjLqsV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install git+https://github.com/tony-pitchblack/FinRL.git@benchmarking --no-deps \\\n",
        "    # --force-reinstall --no-deps"
      ],
      "metadata": {
        "id": "w6nvjUhJQPQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d7e3db-861b-473e-ca44-685d3aa423c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tony-pitchblack/FinRL.git@benchmarking\n",
            "  Cloning https://github.com/tony-pitchblack/FinRL.git (to revision benchmarking) to /tmp/pip-req-build-6512gyvx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tony-pitchblack/FinRL.git /tmp/pip-req-build-6512gyvx\n",
            "  Running command git checkout -b benchmarking --track origin/benchmarking\n",
            "  Switched to a new branch 'benchmarking'\n",
            "  Branch 'benchmarking' set up to track remote branch 'benchmarking' from 'origin'.\n",
            "  Resolved https://github.com/tony-pitchblack/FinRL.git to commit f7dab0a4cda4ae68bfe3b45c3be038bf01327878\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: finrl\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.6-py3-none-any.whl size=4699705 sha256=bf4aa568213131dc008cb0a03ebe73d661cd57327a2b12ef97b976db6a7ca736\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rimz1_su/wheels/e0/44/c0/13e3a1817a2032d4706226574adb60703205278f9a259301e9\n",
            "Successfully built finrl\n",
            "Installing collected packages: finrl\n",
            "Successfully installed finrl-0.3.6\n",
            "CPU times: user 137 ms, sys: 12.4 ms, total: 150 ms\n",
            "Wall time: 16.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!wget https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt -O requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "1S3eA6pMHTxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ca7d934-2324-4ca3-df8b-5336767ecb3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-11 20:23:05--  https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 764 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     764  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-11 20:23:06 (36.2 MB/s) - ‘requirements.txt’ saved [764/764]\n",
            "\n",
            "Collecting alpaca_trade_api>=2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting alpaca-py (from -r requirements.txt (line 2))\n",
            "  Downloading alpaca_py-0.38.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting selenium (from -r requirements.txt (line 3))\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting webdriver-manager (from -r requirements.txt (line 4))\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting ccxt>=1.66.32 (from -r requirements.txt (line 5))\n",
            "  Downloading ccxt-4.4.58-py2.py3-none-any.whl.metadata (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange_calendars==3.6.3 (from -r requirements.txt (line 7))\n",
            "  Downloading exchange_calendars-3.6.3.tar.gz (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gputil (from -r requirements.txt (line 8))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.0.0)\n",
            "Collecting importlib-metadata==4.13.0 (from -r requirements.txt (line 10))\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jqdatasdk (from -r requirements.txt (line 11))\n",
            "  Downloading jqdatasdk-1.9.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting lz4 (from -r requirements.txt (line 12))\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
            "Collecting pre-commit (from -r requirements.txt (line 20))\n",
            "  Downloading pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (8.3.4)\n",
            "Collecting recommonmark (from -r requirements.txt (line 29))\n",
            "  Downloading recommonmark-0.7.1-py2.py3-none-any.whl.metadata (463 bytes)\n",
            "Requirement already satisfied: dm_tree in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.1.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=65.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (75.1.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (8.1.3)\n",
            "Collecting sphinx_rtd_theme (from -r requirements.txt (line 41))\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (2.0.37)\n",
            "Collecting stockstats>=0.4.0 (from -r requirements.txt (line 46))\n",
            "  Downloading stockstats-0.6.4-py2.py3-none-any.whl.metadata (39 kB)\n",
            "Collecting swig (from -r requirements.txt (line 47))\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 49))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: wheel>=0.33.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (0.45.1)\n",
            "Collecting wrds (from -r requirements.txt (line 51))\n",
            "  Downloading wrds-3.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.2.52)\n",
            "Collecting ray[default] (from -r requirements.txt (line 26))\n",
            "  Downloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (2.6.0a0)\n",
            "Collecting pyluach (from exchange_calendars==3.6.3->-r requirements.txt (line 7))\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2025.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (0.12.1)\n",
            "Collecting korean_lunar_calendar (from exchange_calendars==3.6.3->-r requirements.txt (line 7))\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata==4.13.0->-r requirements.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Collecting urllib3<2,>1.24 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack==1.0.3 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.11.12)\n",
            "Collecting PyYAML==6.0.1 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py->-r requirements.txt (line 2)) (2.10.6)\n",
            "Collecting sseclient-py<2.0.0,>=1.7.2 (from alpaca-py->-r requirements.txt (line 2))\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting trio~=0.17 (from selenium->-r requirements.txt (line 3))\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->-r requirements.txt (line 3))\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (4.12.2)\n",
            "Collecting python-dotenv (from webdriver-manager->-r requirements.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (43.0.3)\n",
            "Collecting aiohttp<4,>=3.8.3 (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting aiodns>=1.1.1 (from ccxt>=1.66.32->-r requirements.txt (line 5))\n",
            "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (1.18.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (1.17.0)\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk->-r requirements.txt (line 11))\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting thriftpy2!=0.5.1,>=0.3.9 (from jqdatasdk->-r requirements.txt (line 11))\n",
            "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (3.2.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->-r requirements.txt (line 17)) (2025.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading identify-2.6.7-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (1.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.5.0)\n",
            "Collecting aiohttp-cors (from ray[default]->-r requirements.txt (line 26))\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]->-r requirements.txt (line 26))\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default]->-r requirements.txt (line 26))\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (7.1.0)\n",
            "Collecting py-spy>=0.2.0 (from ray[default]->-r requirements.txt (line 26))\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.70.0)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (2024.10.0)\n",
            "Collecting ormsgpack==1.7.0 (from ray[rllib]->-r requirements.txt (line 27))\n",
            "  Downloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (1.13.1)\n",
            "Collecting commonmark>=0.8.1 (from recommonmark->-r requirements.txt (line 29))\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.11/dist-packages (from recommonmark->-r requirements.txt (line 29)) (0.21.2)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (25.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (1.17.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (3.5.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (3.1.5)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.4.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx_rtd_theme->-r requirements.txt (line 41))\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy->-r requirements.txt (line 44)) (3.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.5.1+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (0.10.1)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds->-r requirements.txt (line 51))\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (1.1)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt>=1.66.32->-r requirements.txt (line 5))\n",
            "  Downloading pycares-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements.txt (line 54)) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 54)) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx->-r requirements.txt (line 40)) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.0.11)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (1.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->-r requirements.txt (line 3))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->-r requirements.txt (line 3))\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->-r requirements.txt (line 3)) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r requirements.txt (line 3))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 3)) (1.7.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl>=1.7.2->ccxt>=1.66.32->-r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.22.3)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]->-r requirements.txt (line 26))\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]->-r requirements.txt (line 26)) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (2.27.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]->-r requirements.txt (line 45)) (0.1.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r requirements.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.6.1)\n",
            "Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alpaca_py-0.38.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading ccxt-4.4.58-py2.py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading recommonmark-0.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.6.4-py2.py3-none-any.whl (31 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrds-3.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
            "Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading identify-2.6.7-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.29.2-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.42.0-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading pycares-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.6/288.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: exchange_calendars, msgpack, gputil, thriftpy2\n",
            "  Building wheel for exchange_calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for exchange_calendars: filename=exchange_calendars-3.6.3-py3-none-any.whl size=182609 sha256=61adcadfde50bfcb315345b4fa64200e762945d80825c1a8d552883701b0e66e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/56/fc/ac3dbf596f81748aa41efee8608b8effb6ef4453862dd0e045\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp311-cp311-linux_x86_64.whl size=15688 sha256=3bb51e7b9b4f77ab111b4f3cc6925f60e4909efba712ffe32f0938a51fe96406\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/35/da/ed9b26b510235e00e3a3c3bab7bad97b59214729662255ab3d\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=cae64a442c351f7a0195229775266a148cb7cbe2c07b2fcaeac4c60a9662aac6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp311-cp311-linux_x86_64.whl size=1824782 sha256=4a90d815cfa0f3ce9ac928d7a4bf7f745f3ca9f5427bf8213dbc6d07ce74d09d\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/32/fa/51ae0364792430fb80858f4705c9e0cba3d6900f591f0c4495\n",
            "Successfully built exchange_calendars msgpack gputil thriftpy2\n",
            "Installing collected packages: swig, sseclient-py, sortedcontainers, py-spy, opencensus-context, msgpack, korean_lunar_calendar, gputil, distlib, commonmark, colorful, wsproto, websockets, virtualenv, urllib3, thriftpy2, tensorboardX, PyYAML, python-dotenv, pymysql, pyluach, psycopg2-binary, outcome, ormsgpack, nodeenv, lz4, importlib-metadata, identify, deprecation, cfgv, trio, pycares, pre-commit, aiohttp, wrds, webdriver-manager, trio-websocket, stockstats, jqdatasdk, exchange_calendars, alpaca_trade_api, alpaca-py, aiohttp-cors, aiodns, sphinxcontrib-jquery, selenium, recommonmark, ray, opencensus, ccxt, sphinx_rtd_theme\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.12\n",
            "    Uninstalling aiohttp-3.11.12:\n",
            "      Successfully uninstalled aiohttp-3.11.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "finrl 0.3.6 requires elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git, which is not installed.\n",
            "finrl 0.3.6 requires pyfolio<0.10,>=0.9, which is not installed.\n",
            "finrl 0.3.6 requires pyportfolioopt<2,>=1, which is not installed.\n",
            "finrl 0.3.6 requires ccxt<4,>=3, but you have ccxt 4.4.58 which is incompatible.\n",
            "finrl 0.3.6 requires exchange-calendars<5,>=4, but you have exchange-calendars 3.6.3 which is incompatible.\n",
            "finrl 0.3.6 requires stockstats<0.6,>=0.5, but you have stockstats 0.6.4 which is incompatible.\n",
            "google-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 aiodns-3.2.0 aiohttp-3.10.11 aiohttp-cors-0.7.0 alpaca-py-0.38.0 alpaca_trade_api-3.2.0 ccxt-4.4.58 cfgv-3.4.0 colorful-0.5.6 commonmark-0.9.1 deprecation-2.1.0 distlib-0.3.9 exchange_calendars-3.6.3 gputil-1.4.0 identify-2.6.7 importlib-metadata-4.13.0 jqdatasdk-1.9.7 korean_lunar_calendar-0.3.1 lz4-4.4.3 msgpack-1.0.3 nodeenv-1.9.1 opencensus-0.11.4 opencensus-context-0.1.3 ormsgpack-1.7.0 outcome-1.3.0.post0 pre-commit-4.1.0 psycopg2-binary-2.9.10 py-spy-0.4.0 pycares-4.5.0 pyluach-2.2.0 pymysql-1.1.1 python-dotenv-1.0.1 ray-2.42.0 recommonmark-0.7.1 selenium-4.28.1 sortedcontainers-2.4.0 sphinx_rtd_theme-3.0.2 sphinxcontrib-jquery-4.1 sseclient-py-1.8.0 stockstats-0.6.4 swig-4.3.0 tensorboardX-2.6.2.2 thriftpy2-0.5.2 trio-0.28.0 trio-websocket-0.11.1 urllib3-1.26.20 virtualenv-20.29.2 webdriver-manager-4.0.2 websockets-10.4 wrds-3.3.0 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata",
                  "urllib3",
                  "yaml"
                ]
              },
              "id": "8b400b2736574cd5a972132e9614c842"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 774 ms, sys: 173 ms, total: 948 ms\n",
            "Wall time: 1min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU-vXqDRW3L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "US_vB7hNSdeu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HJsl_3tVre6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ],
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General funcs"
      ],
      "metadata": {
        "id": "oWn4ZCwkvtN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title YahooDownloader\n",
        "\n",
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
        "            )\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "\n",
        "        try:\n",
        "            # Convert wide to long format\n",
        "            # print(f\"DATA COLS: {data_df.columns}\")\n",
        "            data_df = data_df.sort_index(axis=1).set_index(['Date']).drop(columns=['tic']).stack(level='Ticker', future_stack=True)\n",
        "            data_df.reset_index(inplace=True)\n",
        "            data_df.columns.name = ''\n",
        "\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(columns={'Ticker': 'Tic', 'Adj Close': 'Adjcp'}, inplace=True)\n",
        "            data_df.rename(columns={col: col.lower() for col in data_df.columns}, inplace=True)\n",
        "\n",
        "            columns = [\n",
        "                \"date\",\n",
        "                \"tic\",\n",
        "                \"open\",\n",
        "                \"high\",\n",
        "                \"low\",\n",
        "                \"close\",\n",
        "                \"adjcp\",\n",
        "                \"volume\",\n",
        "            ]\n",
        "\n",
        "            data_df = data_df[columns]\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "gbd4N4QLPXlL",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YhIyXmfQ8EAS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title fix_daily_index\n",
        "\n",
        "def make_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    # Get unique dates and create a mapping to daily indices\n",
        "    total_dates = data_df[date_column].unique()\n",
        "    date_to_index = {date: idx for idx, date in enumerate(sorted(total_dates))}\n",
        "    return data_df[date_column].map(date_to_index)\n",
        "\n",
        "def set_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    \"\"\"\n",
        "    Constructs a daily index from unique dates in the specified column.\n",
        "\n",
        "    Parameters:\n",
        "        data_df (pd.DataFrame): The input DataFrame.\n",
        "        date_column (str): The name of the column containing dates.\n",
        "        new_index_name (str): The name for the new index.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a daily index.\n",
        "    \"\"\"\n",
        "\n",
        "    # Map dates to daily indices and set as index\n",
        "    data_df[new_index_name] = make_daily_index(data_df, date_column='date', new_index_name='date_index')\n",
        "\n",
        "    data_df.set_index(new_index_name, inplace=True)\n",
        "    data_df.index.name = ''  # Remove the index name for simplicity\n",
        "\n",
        "    return data_df\n",
        "\n",
        "def fix_daily_index(df):\n",
        "    if df.index.name == 'date':\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    daily_index = make_daily_index(df, date_column='date', new_index_name='date_index')\n",
        "    if (df.index.values != daily_index.values).any():\n",
        "\n",
        "        df.index = daily_index\n",
        "        df.index.name = ''\n",
        "\n",
        "    return df\n",
        "\n",
        "# trade = fix_daily_index(trade)\n",
        "# trade.index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get dataset name\n",
        "\n",
        "def get_quarterly_dataset_name(prefix, train_start_date, val_start_date, test_start_date):\n",
        "    get_quarter = lambda date: f'Q{(date.month - 1) // 3 + 1}'\n",
        "\n",
        "    val_quarter = get_quarter(val_start_date)\n",
        "    test_quarter = get_quarter(test_start_date)\n",
        "\n",
        "    # Extract year and month\n",
        "    train_start = f\"{train_start_date.year}-{train_start_date.month:02}\"\n",
        "    val_start = f\"{val_start_date.year}\"\n",
        "    test_start = f\"{test_start_date.year}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start} | {val_start} {val_quarter} | {test_start} {test_quarter}\"\n",
        "\n",
        "    return dataset_name\n",
        "\n",
        "def get_yearly_dataset_name(prefix, train_start, test_start, test_end):\n",
        "    # Extract year and month\n",
        "    train_start_str = f\"{train_start.year}-{train_start.month:02}\"\n",
        "    test_start_str = f\"{test_start.year}-{test_start.month:02}\"\n",
        "    test_end_str = f\"{test_end.year}-{test_end.month:02}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start_str} | {test_start_str} | {test_end_str}\"\n",
        "    return dataset_name\n"
      ],
      "metadata": {
        "id": "GQ6BIJxbwuVh",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TOfz3JlX-oG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title add_dataset\n",
        "\n",
        "def add_dataset(stock_index_name, train_df, test_df):\n",
        "    if 'datasets' not in globals():\n",
        "        global datasets\n",
        "        datasets = {}\n",
        "\n",
        "    # Ensure datetime format\n",
        "    if 'date' in train_df.columns:\n",
        "        train_df.set_index('date', inplace=True)\n",
        "    train_df.index = pd.to_datetime(train_df.index)\n",
        "\n",
        "    if 'date' in test_df.columns:\n",
        "        test_df.set_index('date', inplace=True)\n",
        "    test_df.index = pd.to_datetime(test_df.index)\n",
        "\n",
        "    train_start_date = train_df.index[0]\n",
        "    test_start_date = test_df.index[0]\n",
        "    test_end_date = test_df.index[-1]\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        stock_index_name,\n",
        "        train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    train_df.reset_index(inplace=True)\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    ticker_list = train_df.tic.unique().tolist()\n",
        "\n",
        "    datasets[dataset_name] = {\n",
        "        'train': train_df,\n",
        "        'test': test_df,\n",
        "        'metadata': dict(\n",
        "            stock_index_name = stock_index_name,\n",
        "            train_start_date = train_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "            num_tickers = len(ticker_list),\n",
        "            ticker_list = ticker_list,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUph5lzrTUS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA: DOW-30 (quarterly train/val/test)"
      ],
      "metadata": {
        "id": "LHIfVohUTAsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_date = '2015-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2016-10-01'"
      ],
      "metadata": {
        "id": "dTOtt7iQg7TU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate_quarterly_date_ranges\n",
        "\n",
        "def generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=False,\n",
        "    finetune_previous_val=False\n",
        "):\n",
        "    is_quarter_start = lambda date: date.month in [1, 4, 7, 10] and date.day == 1\n",
        "\n",
        "    min_test_start_date = pd.Timestamp(min_test_start_date)\n",
        "    train_start_date = pd.Timestamp(train_start_date)\n",
        "    max_test_end_date = pd.Timestamp(max_test_end_date)\n",
        "\n",
        "    assert is_quarter_start(train_start_date), f\"train_start_date {train_start_date} is not a quarter start date.\"\n",
        "    assert is_quarter_start(min_test_start_date), f\"min_test_start_date {min_test_start_date} is not a quarter start date.\"\n",
        "\n",
        "    test_start_date = min_test_start_date\n",
        "    date_ranges = []\n",
        "    full_train_start_date = train_start_date\n",
        "\n",
        "    while True:\n",
        "        val_start_date = test_start_date - pd.DateOffset(months=3)\n",
        "        test_end_date = test_start_date + pd.DateOffset(months=3)\n",
        "\n",
        "        if test_end_date > max_test_end_date:\n",
        "            break\n",
        "\n",
        "        if len(date_ranges) == 0:\n",
        "            # The first date_range contains the full training period\n",
        "            train_start_date = full_train_start_date\n",
        "        elif finetune_previous_val:\n",
        "            # Use the previous validation range as the training range\n",
        "            train_start_date = date_ranges[-1]['val_start_date']\n",
        "\n",
        "        date_range = dict(\n",
        "            train_start_date=train_start_date,\n",
        "            val_start_date=val_start_date,\n",
        "            test_start_date=test_start_date,\n",
        "            test_end_date=test_end_date,\n",
        "        )\n",
        "\n",
        "        if return_strings:\n",
        "            date_range = {k: str(v) for k, v in date_range.items()}\n",
        "\n",
        "        date_ranges.append(date_range)\n",
        "\n",
        "        test_start_date = test_end_date\n",
        "\n",
        "    return date_ranges\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "# print(*date_ranges[:2], sep='\\n')\n",
        "print(*date_ranges, sep='\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DGMvKo0wxwYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28536f5a-78c2-48bd-9e32-d3b16a199233"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_start_date': Timestamp('2015-01-01 00:00:00'), 'val_start_date': Timestamp('2015-10-01 00:00:00'), 'test_start_date': Timestamp('2016-01-01 00:00:00'), 'test_end_date': Timestamp('2016-04-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2015-10-01 00:00:00'), 'val_start_date': Timestamp('2016-01-01 00:00:00'), 'test_start_date': Timestamp('2016-04-01 00:00:00'), 'test_end_date': Timestamp('2016-07-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2016-01-01 00:00:00'), 'val_start_date': Timestamp('2016-04-01 00:00:00'), 'test_start_date': Timestamp('2016-07-01 00:00:00'), 'test_end_date': Timestamp('2016-10-01 00:00:00')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S8GdHjZWTcHu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title split_data\n",
        "\n",
        "def split_data(data_df, date_range):\n",
        "    def subset_date_range(df, start_date, end_date):\n",
        "        df = df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "        df = fix_daily_index(df)\n",
        "        return df\n",
        "\n",
        "    return {\n",
        "        'train': subset_date_range(data_df, date_range['train_start_date'], date_range['val_start_date']),\n",
        "        'val': subset_date_range(data_df, date_range['val_start_date'], date_range['test_start_date']),\n",
        "        'test': subset_date_range(data_df, date_range['test_start_date'], date_range['test_end_date']),\n",
        "    }\n",
        "\n",
        "# data_splits = split_data(preproc_df, date_ranges[0])\n",
        "# data_splits['train'].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prepare_data (for np env)\n",
        "import hashlib\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "import os\n",
        "\n",
        "CACHE_DIR = './cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def stable_hash(data):\n",
        "    return hashlib.sha256(str(data).encode()).hexdigest()\n",
        "\n",
        "def get_env_config(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    if_train,\n",
        "    ticker_list=DOW_30_TICKER,\n",
        "    technical_indicator_list=INDICATORS,\n",
        "    time_interval='1d',\n",
        "    if_vix=True,\n",
        "    **kwargs\n",
        "):\n",
        "    print(f\"Loading {'train' if if_train else 'eval'} data from {start_date} to {end_date}.\")\n",
        "\n",
        "    data_hash = stable_hash(tuple(sorted(ticker_list) + sorted(technical_indicator_list)))\n",
        "    file_path = Path(CACHE_DIR) / f\"{start_date}_{end_date}_{time_interval}_{data_hash}.csv\"\n",
        "    dp = DataProcessor(data_source='yahoofinance', tech_indicator=technical_indicator_list, vix=if_vix, **kwargs)\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Using cached data: {file_path}\")\n",
        "        data = pd.read_csv(file_path, index_col=0)\n",
        "    else:\n",
        "        print(\"Creating new data.\")\n",
        "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "        data = dp.clean_data(data)\n",
        "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "        if if_vix:\n",
        "            data = dp.add_vix(data)\n",
        "        data.to_csv(file_path)\n",
        "\n",
        "    (\n",
        "        price_array,\n",
        "        tech_array,\n",
        "        turbulence_array,\n",
        "        timestamp_array,\n",
        "    ) = dp.df_to_array(\n",
        "        data,\n",
        "        if_vix,\n",
        "        return_timestamps=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"timestamp_array\": timestamp_array,\n",
        "        \"if_train\": if_train\n",
        "    }\n",
        "\n",
        "    return env_config\n",
        "\n",
        "# date_range=date_ranges[0]\n",
        "# train_env_config = get_env_config(\n",
        "#     start_date=date_range['val_start_date'],\n",
        "#     end_date=date_range['test_start_date'],\n",
        "#     if_train=True\n",
        "# )\n",
        "# val_env_config = get_env_config(\n",
        "#     start_date=date_range['test_start_date'],\n",
        "#     end_date=date_range['test_end_date'],\n",
        "#     if_train=False\n",
        "# )"
      ],
      "metadata": {
        "id": "E34A0Yyhy4Sy",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NYSbz9QQ7pS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb artifacts"
      ],
      "metadata": {
        "id": "KemUy0OKg-wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_artifact\n",
        "\n",
        "def update_artifact(folder_path, name_prefix, type):\n",
        "    \"\"\"\n",
        "    Create or update a W&B artifact consisting of a folder.\n",
        "\n",
        "    Args:\n",
        "        run: The current W&B run.\n",
        "        folder_path (str): Path to the folder to upload.\n",
        "        artifact_name (str): Name of the artifact.\n",
        "        artifact_type (str): Type of the artifact.\n",
        "    \"\"\"\n",
        "    run = wandb.run\n",
        "    artifact_name = f'{name_prefix}-{wandb.run.id}'\n",
        "\n",
        "    # Create a new artifact\n",
        "    artifact = wandb.Artifact(name=artifact_name, type=type)\n",
        "\n",
        "    # Add the folder to the artifact\n",
        "    artifact.add_dir(folder_path)\n",
        "\n",
        "    # Log the artifact to W&B\n",
        "    run.log_artifact(artifact)\n",
        "    print(f\"Artifact '{artifact_name}' has been updated and uploaded.\")"
      ],
      "metadata": {
        "id": "jF0Xbv9f631H",
        "cellView": "form"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bCmdRFmDh-CI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title update_model_artifacts\n",
        "\n",
        "def update_model_artifacts(log_results_folder=True):\n",
        "    if log_results_folder:\n",
        "        update_artifact(\n",
        "            folder_path = RESULTS_DIR,\n",
        "            name_prefix = 'results',\n",
        "            type = 'results'\n",
        "        )\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = TRAINED_MODEL_DIR,\n",
        "        name_prefix = 'trained_models',\n",
        "        type = 'trained_models'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_dataset_artifact\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def update_dataset_artifact(config, train_df, val_df=None, test_df=None):\n",
        "    DATASET_DIR = Path('./dataset')\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(DATASET_DIR / 'train_data.csv', index=False)\n",
        "\n",
        "    if test_df is not None:\n",
        "        test_df.to_csv(DATASET_DIR / 'test_data.csv', index=False)\n",
        "\n",
        "    if val_df is not None:\n",
        "        val_df.to_csv(DATASET_DIR / 'val_data.csv', index=False)\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = DATASET_DIR,\n",
        "        name_prefix = 'dataset',\n",
        "        type = 'dataset'\n",
        "    )"
      ],
      "metadata": {
        "id": "Prm8SfPo7CJY",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_env_state_artifact\n",
        "\n",
        "def update_env_state_artifact(val_env_end_state):\n",
        "    file_path = 'val_env_end_state.csv'\n",
        "\n",
        "    df_last_state = pd.DataFrame({\"last_state\": val_env_end_state})\n",
        "    df_last_state.to_csv(\n",
        "        file_path, index=False\n",
        "    )\n",
        "\n",
        "    artifact = wandb.Artifact(name=f'val_env_end_state-{wandb.run.id}', type='env_state')\n",
        "    artifact.add_file(file_path)\n",
        "    wandb.run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "x66rQn0TGEkM",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & helper funcs"
      ],
      "metadata": {
        "id": "_BZTsxX0tkDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_yearly_train_test\n",
        "def build_yearly_train_test(config):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        config['train_years_count'],\n",
        "        config['test_years_count'],\n",
        "        config['test_start_year']\n",
        "    )\n",
        "\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        train_start_date=train_start_date,\n",
        "        test_start_date=test_start_date,\n",
        "        test_end_date=test_end_date,\n",
        "        dataset_name=dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SmzfG0qaVP93"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_quarterly_train_val_test\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "\n",
        "def build_quarterly_train_val_test(config):\n",
        "    date_range = {key: pd.Timestamp(date) for key, date in config['date_range'].items()}\n",
        "\n",
        "    train_start_date = date_range['train_start_date']\n",
        "    val_start_date = date_range['val_start_date']\n",
        "    test_start_date = date_range['test_start_date']\n",
        "    test_end_date = date_range['test_end_date']\n",
        "\n",
        "    train_env_config = get_env_config(\n",
        "        start_date=date_range['train_start_date'],\n",
        "        end_date=date_range['val_start_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    val_env_config = get_env_config(\n",
        "        start_date=date_range['val_start_date'],\n",
        "        end_date=date_range['test_start_date'],\n",
        "        if_train=True\n",
        "    )\n",
        "\n",
        "    test_env_config = get_env_config(\n",
        "        start_date=date_range['test_start_date'],\n",
        "        end_date=date_range['test_end_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    dataset_name = get_quarterly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, val_start_date, test_start_date\n",
        "    )\n",
        "\n",
        "    config.update({\n",
        "        \"dataset_name\": dataset_name,\n",
        "        \"train.num_datapoints\": len(train_env_config['price_array']),\n",
        "        \"val.num_datapoints\": len(val_env_config['price_array']),\n",
        "        \"test.num_datapoints\": len(test_env_config['price_array']),\n",
        "    })\n",
        "\n",
        "    return train_env_config, val_env_config, test_env_config\n",
        "\n",
        "# train_env_config, val_env_config, test_env_config = build_quarterly_train_val_test(config)"
      ],
      "metadata": {
        "id": "N2JCHB8ibDce"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init StockTradingEnv (numpy)\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.config import INDICATORS\n",
        "# from finrl.config import CACHE_DIR\n",
        "\n",
        "def init_env(\n",
        "    np_env_config,\n",
        "\n",
        "    # run_config,\n",
        "    initial_amount,\n",
        "    cost_pct,\n",
        "\n",
        "    mode,\n",
        "    turbulence_threshold=99,\n",
        "):\n",
        "    assert mode in ['train', 'val', 'test']\n",
        "\n",
        "    print('Initializing env...', end=' ')\n",
        "    env = StockTradingEnv(\n",
        "        config=np_env_config,\n",
        "        initial_capital=initial_amount,\n",
        "        buy_cost_pct=cost_pct,\n",
        "        sell_cost_pct=cost_pct,\n",
        "        turbulence_thresh=turbulence_threshold\n",
        "    )\n",
        "    print('Done.')\n",
        "\n",
        "    return env\n",
        "\n",
        "# env = init_env(\n",
        "#     train_np_env_config,\n",
        "#     run_config,\n",
        "#     'train'\n",
        "# )"
      ],
      "metadata": {
        "id": "Mp0H06urpp53",
        "cellView": "form"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t0-3fjeCG_GJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "S4pwu5RcQz_F"
      },
      "outputs": [],
      "source": [
        "#@title compute metrics\n",
        "import wandb\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(account_values: List[pd.DataFrame, pd.Series, np.array], use_round=True):\n",
        "    \"\"\"\n",
        "    If DataFrame then should contain two columns - 'date' and name of algo, e.g. 'a2c'.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(account_values, pd.DataFrame):\n",
        "        assert isinstance(account_values, pd.DataFrame)\n",
        "        if 'date' not in account_values.columns:\n",
        "            if account_values.index.name == 'date':\n",
        "                account_values.reset_index(inplace=True)\n",
        "            else:\n",
        "                raise ValueError(\"should contain 'date' column or index\")\n",
        "        account_values = account_values.dropna().set_index('date').iloc[:, 0]\n",
        "    elif isinstance(account_values, np.ndarray):\n",
        "        account_values = pd.Series(account_values)\n",
        "\n",
        "    sharpe = calculate_sharpe_ratio(account_values)\n",
        "    mdd = calculate_mdd(account_values)\n",
        "    cum_ret = (account_values.iloc[-1] - account_values.iloc[0]) / account_values.iloc[0] * 100\n",
        "    # num_days = (account_values.index.max() - account_values.index.min()).days\n",
        "    num_days = len(account_values)\n",
        "    ann_ret = ((1 + cum_ret / 100) ** (365 / num_days) - 1) * 100\n",
        "\n",
        "    metrics = {\n",
        "            f'sharpe_ratio': sharpe,\n",
        "            f'mdd': mdd,\n",
        "            f'ann_return': ann_ret,\n",
        "            f'cum_return': cum_ret,\n",
        "        }\n",
        "\n",
        "    if use_round:\n",
        "        metrics = {k: round(v, 2) for k, v in metrics.items()}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_env_metrics(env):\n",
        "    end_total_asset = env.state[0] + sum(\n",
        "        np.array(env.state[1 : (env.stock_dim + 1)])\n",
        "        * np.array(env.state[(env.stock_dim + 1) : (env.stock_dim * 2 + 1)])\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'begin_total_asset': env.asset_memory[0],\n",
        "        'end_total_asset': end_total_asset,\n",
        "        'total_cost': env.cost,\n",
        "        'total_trades': env.trades,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SG9DR-fz9e-s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_metrics\n",
        "\n",
        "def log_metrics(metrics, model_name, split_label, step=None):\n",
        "    print(f'log_metrics for {model_name}')\n",
        "\n",
        "    rename_metrics = lambda model_name: {\n",
        "        f\"{key}/{model_name}\": value for key, value in metrics.items()\n",
        "    }\n",
        "\n",
        "    renamed_metrics = rename_metrics(model_name)\n",
        "    wandb.log({split_label: renamed_metrics}, step=step)\n",
        "    # wandb.run.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_best_model_metrics\n",
        "\n",
        "def update_best_model_metrics(metrics, model_name, split_label):\n",
        "    if 'sharpe_ratios' not in wandb.run.config:\n",
        "        wandb.run.config['sharpe_ratios'] = {}\n",
        "\n",
        "    if split_label not in wandb.run.config['sharpe_ratios']:\n",
        "        wandb.run.config['sharpe_ratios'][split_label] = {}\n",
        "\n",
        "    sharpe_ratios = wandb.run.config['sharpe_ratios'][split_label]\n",
        "\n",
        "    print(f\"DEBUG ({split_label}): run.id = {wandb.run.id}\")\n",
        "    print(f\"DEBUG ({split_label}): sharpe_ratios = {sharpe_ratios}\")\n",
        "    print(f\"DEBUG ({split_label}): updating best model based on sharpe_ratios: {sharpe_ratios}\")\n",
        "    if len(sharpe_ratios) > 0:\n",
        "        best_model_name = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "        if metrics['sharpe_ratio'] > sharpe_ratios[best_model_name]:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" > {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                f\". New best model: {model_name}.\"\n",
        "            )\n",
        "            log_metrics(metrics, 'best_model', split_label)\n",
        "            wandb.log({split_label: {'best_model_name': model_name}})\n",
        "        else:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" <= {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                \". Not updating best model.\"\n",
        "            )\n",
        "    else:\n",
        "        print(f\"DEBUG ({split_label}): no models logged yet, new best model is current one: {model_name}\")\n",
        "        print(f\"DEBUG ({split_label}): wandb.run.config['sharpe_ratios'] = {wandb.run.config['sharpe_ratios']}\")\n",
        "        log_metrics(metrics, 'best_model', split_label)\n",
        "\n",
        "    wandb.run.config['sharpe_ratios'][split_label][model_name] = metrics['sharpe_ratio']"
      ],
      "metadata": {
        "cellView": "form",
        "id": "78Xhl6wmmY9m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "znlXqpfQZkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title init\n",
        "parameters_dict = {}\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'test.sharpe_ratio/best_model'\n",
        "    },\n",
        "    'parameters': parameters_dict\n",
        "}"
      ],
      "metadata": {
        "id": "kF6LboMHB2xZ",
        "cellView": "form"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - yearly_train_test\n",
        "\n",
        "min_test_start_year = 2020\n",
        "max_test_start_year = 2025\n",
        "\n",
        "########################################################\n",
        "\n",
        "yearly_dataset_params = dict(\n",
        "    # dataset_period = {'value': 'year'},\n",
        "    # dataset_splits = {'parameters': {\n",
        "    #     'train': {'value': True },\n",
        "    #     'val': {'value': False },\n",
        "    #     'test': {'value': True },\n",
        "    # }},\n",
        "\n",
        "    dataset_type = {'value':'yearly_train_test'},\n",
        "\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_years_count = {'value': 10},\n",
        "    test_years_count = {'value': 1},\n",
        "    test_start_year = {\n",
        "        'values': list(range(min_test_start_year, max_test_start_year))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7RzkkJNiSkP7",
        "cellView": "form"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - quarterly_train_test\n",
        "\n",
        "# Full date range (18 backtests)\n",
        "train_start_date = '2009-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2020-08-05'\n",
        "\n",
        "#################################################################\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=True,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "quarterly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'quarterly_train_val_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "    train_start_date = {'value': train_start_date},\n",
        "    min_test_start_date = {'value': min_test_start_date},\n",
        "    max_test_end_date = {'value': max_test_end_date},\n",
        "    date_range = {\n",
        "        'values': date_ranges\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "l-hEkHxAfYVv",
        "cellView": "form"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: choose dataset\n",
        "parameters_dict.update(\n",
        "    # yearly_dataset_params,\n",
        "    quarterly_dataset_params\n",
        ")"
      ],
      "metadata": {
        "id": "GJDvneoY00zo",
        "cellView": "form"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: env params\n",
        "parameters_dict.update(dict(\n",
        "    cost_pct = {'value': 1e-3},\n",
        "    initial_amount = {'value': 50_000},\n",
        "    turbulence_threshold = {\n",
        "        'value': 99,\n",
        "        # 'values': [30, 40, 50, 60, 70]\n",
        "    },\n",
        "    eval_turbulence_thresh = {'value': 25},\n",
        "    if_vix = {'value': True}\n",
        "))"
      ],
      "metadata": {
        "id": "ILF379nrW4YK",
        "cellView": "form"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: models_used\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': True},\n",
        "    'if_using_ppo': {'value': True},\n",
        "    'if_using_td3': {'value': True},\n",
        "    'if_using_sac': {'value': True}\n",
        "})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PzWRRS6Prorh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: model and training params\n",
        "\n",
        "training_params = {\n",
        "    \"parameters\": {\n",
        "        \"ppo\": {\n",
        "            \"parameters\": dict(\n",
        "                steps={\"value\": 2048},\n",
        "                train_batch_size={\"value\": 2048},\n",
        "                num_epochs={\"value\": 10},\n",
        "                minibatch_size={\"value\": 128},\n",
        "                lr={\"value\": 5e-5},\n",
        "                gamma={\"value\": 0.99},\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "env_runners_params = {\n",
        "    'parameters': dict(\n",
        "        num_envs_per_env_runner = {'value': 1},\n",
        "        num_env_runners = {'value': 0},\n",
        "    )\n",
        "}\n",
        "\n",
        "parameters_dict.update({'env_runners_params': env_runners_params})\n",
        "parameters_dict.update({'training_params': training_params})"
      ],
      "metadata": {
        "id": "XaCu08TIKegP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & eval funcs"
      ],
      "metadata": {
        "id": "oqc-9r54tuXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "kRSGp8MduO9_"
      },
      "outputs": [],
      "source": [
        "#@title MetricsLoggerCallback (class)\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from typing import Optional, Sequence\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "class MetricsLoggerCallback(DefaultCallbacks):\n",
        "    def __init__(self, model_name, ema_coeff=0.2, ma_window=20, log_to_wandb=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.ema_coeff = ema_coeff\n",
        "        self.ma_window = ma_window\n",
        "        self.metric_names = set()\n",
        "        self.log_to_wandb = log_to_wandb\n",
        "\n",
        "    def unwrap_env(self, env):\n",
        "        env = env.unwrapped\n",
        "        # print(type(env))\n",
        "        # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.vector.sync_vector_env.SyncVectorEnv'>\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            return env\n",
        "        else:\n",
        "            env = env.envs[0]\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.OrderEnforcing'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.PassiveEnvChecker'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'finrl.meta.env_stock_trading.env_stocktrading.StockTradingEnv'>\n",
        "        return env\n",
        "\n",
        "    def on_episode_step(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        env = self.unwrap_env(env)\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            asset_values = env.get_attr('asset_memory')\n",
        "            asset_values = pd.concat([pd.Series(av) for av in asset_values], axis=1).mean(axis=1)\n",
        "\n",
        "            # TODO: save_asset_memory\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            # asset_values = env.asset_memory\n",
        "            asset_values = env.save_asset_memory()\n",
        "\n",
        "        metrics = compute_metrics(asset_values)\n",
        "\n",
        "        # mode = env.mode\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            # metric_name = f\"{mode}/{metric_name}\" if mode != \"\" else metric_name\n",
        "            episode.add_temporary_timestep_data(metric_name, metric_value)\n",
        "            self.metric_names.update([metric_name])\n",
        "\n",
        "    def on_episode_end(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        for metric_name in self.metric_names:\n",
        "            metric_values = episode.get_temporary_timestep_data(metric_name)\n",
        "            metric_value = np.nanmean(np.array(metric_values))\n",
        "\n",
        "            # metrics_logger.log_value(\n",
        "            #     metric_name,\n",
        "            #     metric_value,\n",
        "            #     reduce='mean',\n",
        "            # )\n",
        "\n",
        "            # Log EMA metrics locally\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_EMA_{self.ema_coeff}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                ema_coeff=self.ema_coeff\n",
        "            )\n",
        "\n",
        "            # Log MA metrics locally\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_ma_{self.ma_window}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                window=self.ma_window\n",
        "            )\n",
        "\n",
        "            # Log unsmoothed metrics to wandb\n",
        "            if self.log_to_wandb:\n",
        "                mode = 'val' if env_runner.config.in_evaluation else 'train'\n",
        "                wandb.log({\n",
        "                    f\"{mode}.{metric_name}/{self.model_name}\": metric_value,\n",
        "                }) # TODO: log on every episode step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title print_result\n",
        "\n",
        "RESULT_KEYS_TO_INCLUDE = [\n",
        "    'sharpe_ratio_MA',\n",
        "    'ann_return_MA',\n",
        "    'mdd_MA',\n",
        "\n",
        "    'sharpe_ratio_EMA',\n",
        "    'ann_return_EMA',\n",
        "    'mdd_EMA',\n",
        "]\n",
        "\n",
        "def print_result(result):\n",
        "    print()\n",
        "    for key in result['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"train/{key}: {round(result['env_runners'][key], 2)}\")\n",
        "                break\n",
        "\n",
        "    for key in result['evaluation']['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"val/{key}: {round(result['evaluation']['env_runners'][key], 2)}\")\n",
        "                break\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gg8DIuPJ9FwH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "V0iyYcwTOZYP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title benchmark_exec_time\n",
        "import pandas as pd\n",
        "from time import perf_counter\n",
        "from functools import wraps\n",
        "\n",
        "def benchmark_exec_time(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "\n",
        "        start = perf_counter()\n",
        "        output = func(*args, **kwargs)\n",
        "        end = perf_counter()\n",
        "\n",
        "        exec_time_sec = end - start\n",
        "\n",
        "        data = {\n",
        "            \"func_name\": func.__name__,\n",
        "            \"exec_time_sec\": exec_time_sec,\n",
        "        }\n",
        "        # print(f'\\nBenchmark results: {data}')\n",
        "        return output, exec_time_sec\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train_eval_models\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_models = {}\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            print(f\"Training {model_name.upper()} agent\")\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "            algo = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = pretrained_models.get(model_name, None)\n",
        "            )\n",
        "\n",
        "            print(f\"Evaluating {model_name.upper()} agent\")\n",
        "            val_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = val_np_env_config,\n",
        "                mode='val',\n",
        "            )\n",
        "            fig = plot_results(**val_result)\n",
        "            log_plot_as_artifact(fig, \"val_cumulative_return\", artifact_type=\"plot\")\n",
        "\n",
        "            test_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = test_np_env_config,\n",
        "                mode='test',\n",
        "            )\n",
        "            fig = plot_results(**test_result)\n",
        "            log_plot_as_artifact(fig, \"test_cumulative_return\", artifact_type=\"plot\")\n",
        "            pretrained_models[model_name] = algo\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")\n",
        "\n",
        "    return pretrained_models"
      ],
      "metadata": {
        "id": "TLDeH1lN2XAS",
        "cellView": "form"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train_eval_models (w/ threshold gridsearch)\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_models = {}\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            print(f\"Training {model_name.upper()} agent\")\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "            algo = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = pretrained_models.get(model_name, None)\n",
        "            )\n",
        "\n",
        "            print(f\"Evaluating {model_name.upper()} agent\")\n",
        "\n",
        "            # Grid evaluation\n",
        "            evaluate_threshold_grid(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config,\n",
        "                val_np_env_config,\n",
        "                split_label='val',\n",
        "            )\n",
        "\n",
        "            evaluate_threshold_grid(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config,\n",
        "                test_np_env_config,\n",
        "                split_label='test',\n",
        "            )\n",
        "            pretrained_models[model_name] = algo\n",
        "\n",
        "            # Baseline evaluation\n",
        "            eval_th = run_config['eval_turbulence_thresh']\n",
        "            if run_config.get('if_vix', True):\n",
        "                turbulence_name = 'vix'\n",
        "            else:\n",
        "                turbulence_name = 'turbulence'\n",
        "                raise NotImplementedError\n",
        "\n",
        "            val_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config = run_config,\n",
        "                np_env_config = val_np_env_config,\n",
        "                mode='val',\n",
        "                log_to_wandb=True\n",
        "            )\n",
        "            fig = plot_results(**val_result)\n",
        "            log_plot_as_artifact(\n",
        "                fig,\n",
        "                f\"cum_return-val-{turbulence_name}-{eval_th}\",\n",
        "                artifact_type=\"plot\"\n",
        "            )\n",
        "\n",
        "            test_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config = run_config,\n",
        "                np_env_config = test_np_env_config,\n",
        "                mode='test',\n",
        "                log_to_wandb=True\n",
        "            )\n",
        "            fig = plot_results(**test_result)\n",
        "            log_plot_as_artifact(\n",
        "                fig,\n",
        "                f\"cum_return-test-{turbulence_name}-{eval_th}\",\n",
        "                artifact_type=\"plot\"\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")\n",
        "\n",
        "    return pretrained_models"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qH-OEXfqEaFR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "15pANCivoIru",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train RLlib model\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from ray.tune.registry import register_env\n",
        "from time import perf_counter\n",
        "from math import ceil\n",
        "import ray\n",
        "\n",
        "def train_rllib_model(\n",
        "    run_config,\n",
        "    algo_name,\n",
        "    algo_cls_config,\n",
        "    train_np_env_config,\n",
        "    val_np_env_config,\n",
        "    pretrained_algo=None\n",
        "):\n",
        "    num_envs_per_env_runner = run_config['env_runners_params']['num_envs_per_env_runner']\n",
        "    num_env_runners = run_config['env_runners_params']['num_env_runners']\n",
        "\n",
        "    if pretrained_algo:\n",
        "        algo = pretrained_algo\n",
        "        print(f'Using pretrained {algo_name} agent. Number of pretrain iterations: {algo.training_iteration}')\n",
        "    else:\n",
        "        algo_config = (\n",
        "            algo_cls_config()\n",
        "            .environment(\n",
        "                env=\"stock_trading_env\",\n",
        "                env_config={\n",
        "                    # \"df\": train,\n",
        "                    \"np_env_config\": train_np_env_config,\n",
        "\n",
        "                    # \"run_config\": run_config,\n",
        "                    \"initial_amount\": run_config['initial_amount'],\n",
        "                    \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                    \"mode\": 'train'\n",
        "                },\n",
        "            )\n",
        "            .env_runners(\n",
        "                num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "                num_env_runners=num_env_runners,\n",
        "                num_cpus_per_env_runner= (2/num_env_runners) if num_env_runners > 2 else None,\n",
        "\n",
        "                # gym_env_vectorize_mode=gym.envs.registration.VectorizeMode.ASYNC,\n",
        "            )\n",
        "            .training(\n",
        "                train_batch_size=2048,\n",
        "                num_epochs=10,\n",
        "                minibatch_size=128,\n",
        "            )\n",
        "            .evaluation(\n",
        "                # Set up the validation environment\n",
        "                evaluation_interval=1,  # Specify evaluation frequency (1=after each training step)\n",
        "                evaluation_config={\n",
        "                    \"env\": \"stock_trading_env\",\n",
        "                    \"env_config\": {\n",
        "                        # \"df\": val,\n",
        "                        \"np_env_config\": val_np_env_config,\n",
        "\n",
        "                        # \"run_config\": run_config,\n",
        "                        \"initial_amount\": run_config['initial_amount'],\n",
        "                        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                        \"mode\": 'val'\n",
        "                    },\n",
        "                },\n",
        "            )\n",
        "            # .callbacks(MetricsLoggerCallback)\n",
        "            .callbacks(partial(MetricsLoggerCallback, model_name='ppo', log_to_wandb=True))\n",
        "            .resources(\n",
        "                num_gpus=1 if torch.cuda.is_available() else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 2. build the algorithm ..\n",
        "        if algo_config.num_env_runners > 0:\n",
        "            ray.shutdown()\n",
        "            ray.init()\n",
        "\n",
        "            register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "        algo = algo_config.build()\n",
        "        print(f'Created new {algo_name} agent.')\n",
        "\n",
        "    print(f\"Envs: {algo.config.num_envs_per_env_runner}, Runners: {algo.config.num_env_runners}\")\n",
        "\n",
        "    # 3. .. train it ..\n",
        "    @benchmark_exec_time\n",
        "    def _train_rllib(total_timesteps):\n",
        "        print('Started training.')\n",
        "        results = []\n",
        "        total_batches = ceil(total_timesteps / algo.config.train_batch_size)\n",
        "        print(f\"total_batches: {total_batches}\")\n",
        "        print(f\"total_timesteps: {total_timesteps}\")\n",
        "        for _ in range(total_batches):\n",
        "            result = algo.train()\n",
        "            results.append(result)\n",
        "\n",
        "        print_result(result)\n",
        "        print('Training complete.')\n",
        "        return results\n",
        "\n",
        "    results, exec_time_sec = _train_rllib(run_config['training_params'][algo_name]['steps'])\n",
        "    print(f\"TRAINING DURATION: {exec_time_sec}\")\n",
        "\n",
        "    # Log train duration\n",
        "    duration_minutes = round(exec_time_sec / 60, 1)\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = (Path(TRAINED_MODEL_DIR) / algo_name).resolve()\n",
        "    algo.save(ckpt_path)\n",
        "    update_model_artifacts(log_results_folder=False)\n",
        "\n",
        "    return algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "cellView": "form",
        "id": "lDbFMTrZHugP"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate RLlib model\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
        "from ray.rllib.models.torch.torch_distributions import TorchDiagGaussian\n",
        "import torch\n",
        "\n",
        "# Create the testing environment\n",
        "def evaluate_model(algo, model_name, mode, np_env_config, run_config, turbulence_thresh=None,\n",
        "                   log_to_wandb=False, rl_module=None, return_metrics=False):\n",
        "    assert \\\n",
        "     (algo is None and rl_module is not None) or \\\n",
        "     (rl_module is None and algo is not None)\n",
        "\n",
        "    if turbulence_thresh is None:\n",
        "        turbulence_thresh = run_config.get('eval_turbulence_thresh', 30)\n",
        "    turbulence_name = 'turbulence' if not run_config.get('if_vix', None) else '^VIX'\n",
        "    print(f\"Evaluating for `{mode}` using `{turbulence_name}`: {turbulence_thresh}\")\n",
        "\n",
        "    env_config = {\n",
        "        # \"df\": val,\n",
        "        \"np_env_config\": np_env_config,\n",
        "\n",
        "        # \"run_config\": run_config,\n",
        "        \"initial_amount\": run_config['initial_amount'],\n",
        "        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "        \"mode\": mode,\n",
        "        'turbulence_threshold': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    eval_env = create_stock_trading_env(env_config)\n",
        "    state, info = eval_env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Perform inference using the trained RLlib agent\n",
        "    if rl_module is None:\n",
        "        rl_module = algo.env_runner.module\n",
        "\n",
        "    while not done:\n",
        "        # Compute action using the RLlib trained agent\n",
        "        input_dict = {Columns.OBS: torch.Tensor(state).unsqueeze(0)}\n",
        "        rl_module_out = rl_module.forward_inference(input_dict)\n",
        "        logits = rl_module_out[Columns.ACTION_DIST_INPUTS]\n",
        "\n",
        "        # Take mean of multivariate Gaussian distribution\n",
        "        mean, log_std = logits.chunk(2, dim=-1)\n",
        "\n",
        "        # action_distribution = TorchDiagGaussian.from_logits(logits)\n",
        "        # action_distribution = action_distribution.to_deterministic()\n",
        "        # assert np.allclose(mean, action_distribution.loc)\n",
        "        # assert np.allclose(log_std.exp(), action_distribution._dist.scale)\n",
        "        # action = action_distribution.sample()\n",
        "\n",
        "        action = mean.detach().numpy().squeeze()\n",
        "\n",
        "        # Clip the action to ensure it's within the action space bounds\n",
        "        action = np.clip(action, eval_env.action_space.low, eval_env.action_space.high)\n",
        "\n",
        "        # Perform action\n",
        "        state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "    df_account_value = eval_env.save_asset_memory()\n",
        "    metrics = compute_metrics(df_account_value)\n",
        "    print(metrics)\n",
        "\n",
        "    if log_to_wandb:\n",
        "        turbulence_log_name = 'ti' if not run_config.get('if_vix', None) else 'vix'\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            metric_name = f\"{mode}.{turbulence_log_name}_{turbulence_thresh}.{metric_name}/{model_name}\"\n",
        "            wandb.run.summary[metric_name] = metric_value\n",
        "\n",
        "    turbulence_series = pd.Series(\n",
        "        np_env_config['turbulence_array'][:len(df_account_value)],\n",
        "        index=df_account_value['date'],\n",
        "        name=turbulence_name\n",
        "    )\n",
        "\n",
        "    eval_result = {\n",
        "        'account_value': df_account_value.rename(columns={'account_value': model_name}),\n",
        "        'turbulence_series': turbulence_series,\n",
        "        'turbulence_thresh': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    if return_metrics:\n",
        "        return eval_result, metrics\n",
        "    else:\n",
        "        return eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CbLgPDKNPQX3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title plot_results (w/separate turbulence plot)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_results(\n",
        "        account_value,\n",
        "        turbulence_series,\n",
        "        turbulence_thresh,\n",
        "    ):\n",
        "\n",
        "    assert turbulence_series.name in ['turbulence', '^VIX']\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "    # Main plot\n",
        "    method_styles = {\n",
        "        'A2C': {'color': '#8c564b', 'linestyle': '--'},\n",
        "        'DDPG': {'color': '#e377c2', 'linestyle': '-'},\n",
        "        'PPO': {'color': '#7f7f7f', 'linestyle': '-'},\n",
        "        'TD3': {'color': '#bcbd22', 'linestyle': '--'},\n",
        "        'SAC': {'color': '#17becf', 'linestyle': '-'},\n",
        "        'DJIA': {'color': '#000000', 'linestyle': '-'},\n",
        "    }\n",
        "\n",
        "    if 'DJIA' in account_value:\n",
        "        ax1.plot(account_value.index, account_value['DJIA'], label=\"Dow Jones Index\",\n",
        "                linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "    if 'date' in account_value.columns:\n",
        "        account_value.set_index('date', inplace=True)\n",
        "\n",
        "    account_value.rename(columns={col: col.upper() for col in account_value.columns}, inplace=True)\n",
        "\n",
        "    for model_name in account_value.columns:\n",
        "        style = method_styles[model_name]\n",
        "        ax1.plot(account_value.index, account_value[model_name], label=model_name, **style)\n",
        "\n",
        "    # Customize main plot\n",
        "    ax1.set_title(\"Performance Comparison of DRL Agents\", fontsize=20, fontweight='bold')\n",
        "    ax1.set_ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Turbulence plot\n",
        "    if turbulence_series.name == 'turbulence':\n",
        "        turbulence_label = \"Turbulence Index\"\n",
        "    else:\n",
        "        turbulence_label = \"VIX Coefficient\"\n",
        "    ax2.plot(turbulence_series.index, turbulence_series, label=\"Turbulence Index\", color='red', linestyle='--', linewidth=2)\n",
        "    ax2.axhline(y=turbulence_thresh, color='red', linestyle=':', label='Turbulence Threshold')\n",
        "    ax2.set_ylabel(turbulence_label, fontsize=16, fontweight='bold')\n",
        "    ax2.legend(loc='lower left')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "    max_turbulence = max(turbulence_series.max(), turbulence_thresh)\n",
        "    ax2.set_ylim(0, max_turbulence + 10)\n",
        "    ax2.set_yticks(list(ax2.get_yticks()) + [turbulence_thresh])\n",
        "\n",
        "    # Shared x-axis label\n",
        "    ax2.set_xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# fig = plot_results(**val_result)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "02OzYWBGMVNE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_plot_as_artifact\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "def log_plot_as_artifact(fig, artifact_name_prefix, artifact_type=\"plot\"):\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure without clipping and log it as a W&B artifact.\n",
        "\n",
        "    Parameters:\n",
        "        fig (matplotlib.figure.Figure): The Matplotlib figure to save and log.\n",
        "        artifact_name (str): The name of the W&B artifact.\n",
        "        artifact_type (str): The type of the artifact (default is \"plot\").\n",
        "        filename (str): The filename to save the plot as (default is \"plot.png\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get full artifact name\n",
        "        artifact_name = f'{artifact_name_prefix}-{wandb.run.id}'\n",
        "        filename = artifact_name + '.png'\n",
        "\n",
        "        # Save the figure with tight layout and proper padding\n",
        "        fig.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "        plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "        # Create and log the W&B artifact\n",
        "        artifact = wandb.Artifact(artifact_name, type=artifact_type)\n",
        "        artifact.add_file(filename, skip_cache=True, overwrite=True)\n",
        "        wandb.log_artifact(artifact)\n",
        "    finally:\n",
        "        # Ensure the file is deleted after use\n",
        "        if os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "\n",
        "# log_plot_as_artifact(fig, \"performance_comparison_DRL_agents\", artifact_type=\"plot\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title evaluate_threshold_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "def evaluate_threshold_grid(\n",
        "    algo,\n",
        "    model_name,\n",
        "    run_config,\n",
        "    np_env_config,\n",
        "    num_intervals = 10,\n",
        "    split_label='val',\n",
        "):\n",
        "\n",
        "    if run_config.get('if_vix', True):\n",
        "        turbulence_name = 'vix'\n",
        "    else:\n",
        "        turbulence_name = 'turbulence'\n",
        "        raise NotImplementedError\n",
        "\n",
        "    turbulence_name += '_thresh'\n",
        "\n",
        "    # Calculate threshold grid\n",
        "    turb_ary = np_env_config['turbulence_array']\n",
        "    turb_values = np.linspace(turb_ary.min(), turb_ary.max(), num_intervals).astype(int)\n",
        "    upper_th_value = turb_values.max() + (turb_ary.max() - turb_ary.min()) / num_intervals\n",
        "    threshold_grid = list(set(turb_values.tolist())) + [upper_th_value.astype(int)]\n",
        "\n",
        "    th_metrics = []  # List to store metrics for all thresholds\n",
        "    th_results = {}  # Dictionary to store results per threshold\n",
        "\n",
        "    for th in threshold_grid:\n",
        "        result, metrics = evaluate_model(\n",
        "            turbulence_thresh=th,\n",
        "            model_name=model_name,\n",
        "            algo=algo,\n",
        "            run_config=run_config,\n",
        "            np_env_config=np_env_config,\n",
        "            mode='val',\n",
        "            log_to_wandb=False,\n",
        "            return_metrics=True\n",
        "        )\n",
        "\n",
        "        metrics = {turbulence_name: th, **metrics}\n",
        "        th_metrics.append(metrics)\n",
        "        th_results[th] = result\n",
        "\n",
        "    # Convert metrics to a DataFrame and log to WandB\n",
        "    df = pd.DataFrame(th_metrics).astype({turbulence_name: int})\n",
        "    metric_cols = df.drop(columns=[turbulence_name]).columns # take only true metric columns\n",
        "\n",
        "    # drop duplicate rows\n",
        "    df.drop_duplicates(\n",
        "        metric_cols,\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    # drop rows with all metrics equal 0\n",
        "    df.drop(\n",
        "        df[\n",
        "            (df[metric_cols] == 0).all(axis=1)\n",
        "        ].index,\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    wandb_table = wandb.Table(dataframe=df)\n",
        "    wandb.log({f\"threshold_grid_metrics-{split_label}\": wandb_table})\n",
        "\n",
        "    # Plot returns\n",
        "    for th_value in df[turbulence_name].values:\n",
        "        result = th_results[th_value]\n",
        "        fig = plot_results(**result)\n",
        "        log_plot_as_artifact(\n",
        "            fig,\n",
        "            f\"cum_return-{split_label}-{turbulence_name}_{th_value}\",\n",
        "            artifact_type=\"plot\"\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2KiSe6zgPZnM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run sweep"
      ],
      "metadata": {
        "id": "fXntnmmhuSsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uSosu3u-YIIA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sweep Runner\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "def set_run_name(prefix, n=5):\n",
        "    run_name = f\"{prefix} | {wandb.run.id}\"\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "class SweepRunner:\n",
        "    def __init__(self, sweep_id):\n",
        "        self.sweep_id = sweep_id\n",
        "        self.pretrained_models = {}\n",
        "\n",
        "    def main(self, run_config=None):\n",
        "        run_timer_start = perf_counter()\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"START TIME:\", current_time)\n",
        "\n",
        "        with wandb.init(config=run_config):\n",
        "            run_config = wandb.config\n",
        "            # print('Debug copy run_config...', end=' ')\n",
        "            # _ = deepcopy(run_config)\n",
        "            # print(\"Done.\")\n",
        "\n",
        "            if run_config['dataset_type'] == 'yearly_train_test':\n",
        "                raise NotImplementedError\n",
        "            elif run_config['dataset_type'] == 'quarterly_train_val_test':\n",
        "                (\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config\n",
        "                ) = build_quarterly_train_val_test(run_config)\n",
        "                set_run_name(run_config['dataset_name'])\n",
        "\n",
        "                if self.pretrained_models:\n",
        "                    run_config.update({'finetune': True})\n",
        "                else:\n",
        "                    run_config.update({'finetune': False})\n",
        "\n",
        "                pretrained_models = train_eval_rllib_models(\n",
        "                    run_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config,\n",
        "                    pretrained_models=self.pretrained_models\n",
        "                )\n",
        "\n",
        "                self.pretrained_models = pretrained_models\n",
        "\n",
        "            ray.shutdown()\n",
        "\n",
        "            run_timer_end = perf_counter()\n",
        "            run_duration_minutes = round( (run_timer_end - run_timer_start) / 60, 1)\n",
        "            wandb.run.summary[f\"run.duration_minutes\"] = run_duration_minutes\n",
        "            print(f\"RUN DURATION: {run_duration_minutes}\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"END TIME:\", current_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN SWEEP\n",
        "def run_sweep(n_runs, sweep_config=None, sweep_id=None):\n",
        "    if sweep_id is None:\n",
        "        assert sweep_config is not None\n",
        "        sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "    else:\n",
        "        assert sweep_config is None\n",
        "\n",
        "    !rm -rf {TRAINED_MODEL_DIR}/*\n",
        "\n",
        "    sweep_runner = SweepRunner(sweep_id)\n",
        "    wandb.agent(sweep_id, sweep_runner.main, project=PROJECT, count=n_runs)\n",
        "\n",
        "run_sweep(\n",
        "    # sweep_id='50a5ela4',\n",
        "    sweep_config=sweep_config,\n",
        "\n",
        "    # n_runs=None\n",
        "    n_runs=2,\n",
        ")"
      ],
      "metadata": {
        "id": "_jqFSQXXM4Z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c60d73c-9f28-4aba-cd3d-b9a994d97eb3",
        "cellView": "form"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: xvy3vpxx\n",
            "Sweep URL: https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tg5s4i3e with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcost_pct: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_type: quarterly_train_val_test\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdate_range: {'test_end_date': '2016-04-01 00:00:00', 'test_start_date': '2016-01-01 00:00:00', 'train_start_date': '2009-01-01 00:00:00', 'val_start_date': '2015-10-01 00:00:00'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_runners_params: {'num_env_runners': 0, 'num_envs_per_env_runner': 1}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_turbulence_thresh: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_a2c: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ddpg: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ppo: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_sac: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_td3: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_vix: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_amount: 50000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_test_end_date: 2020-08-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_test_start_date: 2016-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tstock_index_name: DOW-30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_start_date: 2009-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_params: {'ppo': {'gamma': 0.99, 'lr': 5e-05, 'minibatch_size': 128, 'num_epochs': 10, 'steps': 2048, 'train_batch_size': 2048}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tturbulence_threshold: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TIME: 2025-02-11 21:09:24.245409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtony-pitchblack\u001b[0m (\u001b[33moverfit1010\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250211_210924-tg5s4i3e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/tg5s4i3e' target=\"_blank\">polished-sweep-1</a></strong> to <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/tg5s4i3e' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/tg5s4i3e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading eval data from 2009-01-01 00:00:00 to 2015-10-01 00:00:00.\n",
            "Using cached data: cache/2009-01-01 00:00:00_2015-10-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "2025-02-11 21:09:26,676\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
            "2025-02-11 21:09:26,699\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train data from 2015-10-01 00:00:00 to 2016-01-01 00:00:00.\n",
            "Using cached data: cache/2015-10-01 00:00:00_2016-01-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Loading eval data from 2016-01-01 00:00:00 to 2016-04-01 00:00:00.\n",
            "Using cached data: cache/2016-01-01 00:00:00_2016-04-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Training PPO agent\n",
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-11 21:09:27,574\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
            "2025-02-11 21:09:27,725\tWARNING algorithm_config.py:4726 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing env... Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-11 21:09:29,324\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new ppo agent.\n",
            "Envs: 1, Runners: 0\n",
            "Started training.\n",
            "total_batches: 1\n",
            "total_timesteps: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-11 21:10:38,808\tWARNING worker.py:1504 -- SIGTERM handler is not set because current thread is not the main thread.\n",
            "2025-02-11 21:10:42,115\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train/ann_return_EMA_0.2: 38.62\n",
            "train/sharpe_ratio_EMA_0.2: 0.84\n",
            "train/mdd_EMA_0.2: -31.34\n",
            "val/ann_return_EMA_0.2: 22731.67\n",
            "val/mdd_EMA_0.2: -4.56\n",
            "val/sharpe_ratio_EMA_0.2: 4.35\n",
            "\n",
            "Training complete.\n",
            "TRAINING DURATION: 81.95250681800007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./trained_models)... Done. 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifact 'trained_models-tg5s4i3e' has been updated and uploaded.\n",
            "Evaluating PPO agent\n",
            "Evaluating for `val` using `^VIX`: 14\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.0, 'mdd': 0.0, 'ann_return': 4.22, 'cum_return': 0.73}\n",
            "Evaluating for `val` using `^VIX`: 15\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -4.32, 'mdd': -1.42, 'ann_return': -7.82, 'cum_return': -1.42}\n",
            "Evaluating for `val` using `^VIX`: 16\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -3.8, 'mdd': -6.87, 'ann_return': -33.35, 'cum_return': -6.87}\n",
            "Evaluating for `val` using `^VIX`: 17\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -2.09, 'mdd': -5.71, 'ann_return': -21.03, 'cum_return': -4.06}\n",
            "Evaluating for `val` using `^VIX`: 18\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -0.82, 'mdd': -6.93, 'ann_return': -14.0, 'cum_return': -2.61}\n",
            "Evaluating for `val` using `^VIX`: 19\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -1.41, 'mdd': -7.08, 'ann_return': -20.47, 'cum_return': -3.94}\n",
            "Evaluating for `val` using `^VIX`: 20\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -1.75, 'mdd': -7.66, 'ann_return': -24.58, 'cum_return': -4.83}\n",
            "Evaluating for `val` using `^VIX`: 22\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.62, 'mdd': -7.81, 'ann_return': 51.01, 'cum_return': 7.49}\n",
            "Evaluating for `val` using `^VIX`: 23\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.51, 'mdd': -7.07, 'ann_return': 10.41, 'cum_return': 1.75}\n",
            "Evaluating for `val` using `^VIX`: 24\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.8, 'mdd': -7.41, 'ann_return': 18.3, 'cum_return': 2.99}\n",
            "Evaluating for `val` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.9, 'mdd': -6.07, 'ann_return': 21.02, 'cum_return': 3.4}\n",
            "Evaluating for `val` using `^VIX`: 13\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.0, 'mdd': 0.0, 'ann_return': 0.0, 'cum_return': 0.0}\n",
            "Evaluating for `val` using `^VIX`: 15\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -0.56, 'mdd': -1.17, 'ann_return': -1.56, 'cum_return': -0.27}\n",
            "Evaluating for `val` using `^VIX`: 16\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.67, 'mdd': -1.04, 'ann_return': 2.01, 'cum_return': 0.34}\n",
            "Evaluating for `val` using `^VIX`: 18\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 3.65, 'mdd': -0.8, 'ann_return': 21.75, 'cum_return': 3.4}\n",
            "Evaluating for `val` using `^VIX`: 20\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 3.05, 'mdd': -1.3, 'ann_return': 22.35, 'cum_return': 3.49}\n",
            "Evaluating for `val` using `^VIX`: 21\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.98, 'mdd': -2.3, 'ann_return': 37.41, 'cum_return': 5.55}\n",
            "Evaluating for `val` using `^VIX`: 23\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.83, 'mdd': -3.24, 'ann_return': 45.44, 'cum_return': 6.57}\n",
            "Evaluating for `val` using `^VIX`: 24\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.93, 'mdd': -3.23, 'ann_return': 34.32, 'cum_return': 5.14}\n",
            "Evaluating for `val` using `^VIX`: 26\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.48, 'mdd': -6.04, 'ann_return': 31.73, 'cum_return': 4.79}\n",
            "Evaluating for `val` using `^VIX`: 28\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.13, 'mdd': -9.38, 'ann_return': 1.17, 'cum_return': 0.2}\n",
            "Evaluating for `val` using `^VIX`: 29\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.18, 'mdd': -9.27, 'ann_return': 31.5, 'cum_return': 4.76}\n",
            "Evaluating for `val` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.47, 'mdd': -4.8, 'ann_return': 9.74, 'cum_return': 1.64}\n",
            "Evaluating for `test` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.7, 'mdd': -5.09, 'ann_return': 34.61, 'cum_return': 5.18}\n",
            "RUN DURATION: 2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train.ann_return/ppo</td><td>▁</td></tr><tr><td>train.cum_return/ppo</td><td>▁</td></tr><tr><td>train.mdd/ppo</td><td>▁</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>▁</td></tr><tr><td>val.ann_return/ppo</td><td>▁▁▂█▅▁▂▁▁▃</td></tr><tr><td>val.cum_return/ppo</td><td>▃▁▄█▆▃▆▁▅▆</td></tr><tr><td>val.mdd/ppo</td><td>▇▆▅█▁▆▄▄█▇</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>▅▁▆█▇▆▇▁██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>run.duration_minutes</td><td>2.2</td></tr><tr><td>test.vix_25.ann_return/ppo</td><td>34.61</td></tr><tr><td>test.vix_25.cum_return/ppo</td><td>5.18</td></tr><tr><td>test.vix_25.mdd/ppo</td><td>-5.09</td></tr><tr><td>test.vix_25.sharpe_ratio/ppo</td><td>1.7</td></tr><tr><td>train.ann_return/ppo</td><td>38.62218</td></tr><tr><td>train.cum_return/ppo</td><td>165.72326</td></tr><tr><td>train.duration_minutes/ppo</td><td>1.4</td></tr><tr><td>train.mdd/ppo</td><td>-31.34059</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>0.84321</td></tr><tr><td>val.ann_return/ppo</td><td>37094.91413</td></tr><tr><td>val.cum_return/ppo</td><td>10.45063</td></tr><tr><td>val.mdd/ppo</td><td>-4.19095</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>5.62645</td></tr><tr><td>val.vix_25.ann_return/ppo</td><td>9.74</td></tr><tr><td>val.vix_25.cum_return/ppo</td><td>1.64</td></tr><tr><td>val.vix_25.mdd/ppo</td><td>-4.8</td></tr><tr><td>val.vix_25.sharpe_ratio/ppo</td><td>0.47</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DOW-30 | 2009-01 | 2015 Q4 | 2016 Q1 | tg5s4i3e</strong> at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/tg5s4i3e' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/tg5s4i3e</a><br> View project at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a><br>Synced 5 W&B file(s), 2 media file(s), 150 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250211_210924-tg5s4i3e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END TIME: 2025-02-11 21:11:37.284620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gki28whq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcost_pct: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_type: quarterly_train_val_test\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdate_range: {'test_end_date': '2016-07-01 00:00:00', 'test_start_date': '2016-04-01 00:00:00', 'train_start_date': '2015-10-01 00:00:00', 'val_start_date': '2016-01-01 00:00:00'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_runners_params: {'num_env_runners': 0, 'num_envs_per_env_runner': 1}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_turbulence_thresh: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_a2c: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ddpg: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_ppo: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_sac: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_using_td3: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_vix: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_amount: 50000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_test_end_date: 2020-08-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_test_start_date: 2016-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tstock_index_name: DOW-30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_start_date: 2009-01-01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttraining_params: {'ppo': {'gamma': 0.99, 'lr': 5e-05, 'minibatch_size': 128, 'num_epochs': 10, 'steps': 2048, 'train_batch_size': 2048}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tturbulence_threshold: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TIME: 2025-02-11 21:11:40.718869\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250211_211141-gki28whq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/gki28whq' target=\"_blank\">dashing-sweep-2</a></strong> to <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/sweeps/xvy3vpxx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/gki28whq' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/gki28whq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading eval data from 2015-10-01 00:00:00 to 2016-01-01 00:00:00.\n",
            "Using cached data: cache/2015-10-01 00:00:00_2016-01-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Loading train data from 2016-01-01 00:00:00 to 2016-04-01 00:00:00.\n",
            "Using cached data: cache/2016-01-01 00:00:00_2016-04-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Loading eval data from 2016-04-01 00:00:00 to 2016-07-01 00:00:00.\n",
            "Using cached data: cache/2016-04-01 00:00:00_2016-07-01 00:00:00_1d_27efcb16e570e962e2bc737bc60d8622859af4aa72ead85f4298e9b4c41ba6a2.csv\n",
            "Training PPO agent\n",
            "Using pretrained ppo agent. Number of pretrain iterations: 1\n",
            "Envs: 1, Runners: 0\n",
            "Started training.\n",
            "total_batches: 1\n",
            "total_timesteps: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-11 21:12:51,984\tWARNING worker.py:1504 -- SIGTERM handler is not set because current thread is not the main thread.\n",
            "2025-02-11 21:12:55,067\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./trained_models)... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train/ann_return_EMA_0.2: 38.89\n",
            "train/sharpe_ratio_EMA_0.2: 0.85\n",
            "train/mdd_EMA_0.2: -31.5\n",
            "val/ann_return_EMA_0.2: 19411.15\n",
            "val/mdd_EMA_0.2: -4.55\n",
            "val/sharpe_ratio_EMA_0.2: 4.18\n",
            "\n",
            "Training complete.\n",
            "TRAINING DURATION: 82.46765845599975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifact 'trained_models-gki28whq' has been updated and uploaded.\n",
            "Evaluating PPO agent\n",
            "Evaluating for `val` using `^VIX`: 13\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.03, 'mdd': 0.0, 'ann_return': 16.61, 'cum_return': 2.65}\n",
            "Evaluating for `val` using `^VIX`: 15\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -1.21, 'mdd': -1.2, 'ann_return': -3.82, 'cum_return': -0.66}\n",
            "Evaluating for `val` using `^VIX`: 16\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -1.65, 'mdd': -2.61, 'ann_return': -11.98, 'cum_return': -2.14}\n",
            "Evaluating for `val` using `^VIX`: 18\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': -0.43, 'mdd': -4.04, 'ann_return': -5.89, 'cum_return': -1.03}\n",
            "Evaluating for `val` using `^VIX`: 20\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.54, 'mdd': -2.44, 'ann_return': 49.86, 'cum_return': 7.11}\n",
            "Evaluating for `val` using `^VIX`: 21\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.94, 'mdd': -5.0, 'ann_return': 36.28, 'cum_return': 5.4}\n",
            "Evaluating for `val` using `^VIX`: 23\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 2.17, 'mdd': -5.4, 'ann_return': 43.56, 'cum_return': 6.33}\n",
            "Evaluating for `val` using `^VIX`: 24\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.41, 'mdd': -5.14, 'ann_return': 26.66, 'cum_return': 4.1}\n",
            "Evaluating for `val` using `^VIX`: 26\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.7, 'mdd': -8.55, 'ann_return': 14.72, 'cum_return': 2.36}\n",
            "Evaluating for `val` using `^VIX`: 28\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.23, 'mdd': -10.24, 'ann_return': 3.67, 'cum_return': 0.61}\n",
            "Evaluating for `val` using `^VIX`: 29\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.16, 'mdd': -6.9, 'ann_return': 40.62, 'cum_return': 5.96}\n",
            "Evaluating for `val` using `^VIX`: 13\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.0, 'mdd': 0.0, 'ann_return': 0.0, 'cum_return': 0.0}\n",
            "Evaluating for `val` using `^VIX`: 14\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.6, 'mdd': -1.29, 'ann_return': 2.11, 'cum_return': 0.37}\n",
            "Evaluating for `val` using `^VIX`: 15\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.08, 'mdd': -3.26, 'ann_return': 0.4, 'cum_return': 0.07}\n",
            "Evaluating for `val` using `^VIX`: 17\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.4, 'mdd': -4.24, 'ann_return': 19.26, 'cum_return': 3.19}\n",
            "Evaluating for `val` using `^VIX`: 18\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.45, 'mdd': -4.24, 'ann_return': 5.73, 'cum_return': 1.0}\n",
            "Evaluating for `val` using `^VIX`: 20\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.81, 'mdd': -4.24, 'ann_return': 11.24, 'cum_return': 1.91}\n",
            "Evaluating for `val` using `^VIX`: 21\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.64, 'mdd': -4.24, 'ann_return': 8.78, 'cum_return': 1.51}\n",
            "Evaluating for `val` using `^VIX`: 22\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.08, 'mdd': -5.08, 'ann_return': 0.37, 'cum_return': 0.07}\n",
            "Evaluating for `val` using `^VIX`: 24\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.52, 'mdd': -5.08, 'ann_return': 8.91, 'cum_return': 1.53}\n",
            "Evaluating for `val` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.52, 'mdd': -5.08, 'ann_return': 8.91, 'cum_return': 1.53}\n",
            "Evaluating for `val` using `^VIX`: 26\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.55, 'mdd': -6.72, 'ann_return': 10.06, 'cum_return': 1.72}\n",
            "Evaluating for `val` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 1.47, 'mdd': -7.59, 'ann_return': 36.96, 'cum_return': 5.49}\n",
            "Evaluating for `test` using `^VIX`: 25\n",
            "Initializing env... Done.\n",
            "{'sharpe_ratio': 0.52, 'mdd': -5.08, 'ann_return': 8.91, 'cum_return': 1.53}\n",
            "RUN DURATION: 2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train.ann_return/ppo</td><td>▁</td></tr><tr><td>train.cum_return/ppo</td><td>▁</td></tr><tr><td>train.mdd/ppo</td><td>▁</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>▁</td></tr><tr><td>val.ann_return/ppo</td><td>▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>val.cum_return/ppo</td><td>▄▃█▂▁█▆▄▅▂</td></tr><tr><td>val.mdd/ppo</td><td>▅▁▅▃▄█▂▃▅▆</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>▇▂█▁▁▆▆▆▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>run.duration_minutes</td><td>2.1</td></tr><tr><td>test.vix_25.ann_return/ppo</td><td>8.91</td></tr><tr><td>test.vix_25.cum_return/ppo</td><td>1.53</td></tr><tr><td>test.vix_25.mdd/ppo</td><td>-5.08</td></tr><tr><td>test.vix_25.sharpe_ratio/ppo</td><td>0.52</td></tr><tr><td>train.ann_return/ppo</td><td>45.43935</td></tr><tr><td>train.cum_return/ppo</td><td>197.55052</td></tr><tr><td>train.duration_minutes/ppo</td><td>1.4</td></tr><tr><td>train.mdd/ppo</td><td>-35.32087</td></tr><tr><td>train.sharpe_ratio/ppo</td><td>1.05979</td></tr><tr><td>val.ann_return/ppo</td><td>31.89984</td></tr><tr><td>val.cum_return/ppo</td><td>1.66635</td></tr><tr><td>val.mdd/ppo</td><td>-4.15619</td></tr><tr><td>val.sharpe_ratio/ppo</td><td>1.10258</td></tr><tr><td>val.vix_25.ann_return/ppo</td><td>36.96</td></tr><tr><td>val.vix_25.cum_return/ppo</td><td>5.49</td></tr><tr><td>val.vix_25.mdd/ppo</td><td>-7.59</td></tr><tr><td>val.vix_25.sharpe_ratio/ppo</td><td>1.47</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DOW-30 | 2015-10 | 2016 Q1 | 2016 Q2 | gki28whq</strong> at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate/runs/gki28whq' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate/runs/gki28whq</a><br> View project at: <a href='https://wandb.ai/overfit1010/finrl-dt-replicate' target=\"_blank\">https://wandb.ai/overfit1010/finrl-dt-replicate</a><br>Synced 5 W&B file(s), 2 media file(s), 148 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250211_211141-gki28whq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END TIME: 2025-02-11 21:13:50.472828\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y-J5mD_PTar9",
        "SIU-vXqDRW3L",
        "oWn4ZCwkvtN3",
        "hWUph5lzrTUS",
        "LHIfVohUTAsG",
        "NYSbz9QQ7pS8",
        "znlXqpfQZkzU",
        "oqc-9r54tuXG"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNAFIGxBBNahULnssmrNvFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}