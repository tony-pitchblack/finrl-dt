{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_sweep_rllib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "Y-J5mD_PTar9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "RfYDJoTXo6-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# git clone https://github.com/tony-pitchblack/FinRL.git\n",
        "# cd ./FinRL\n",
        "# git checkout benchmarking\n",
        "# pip install -r FinRL/requirements.txt"
      ],
      "metadata": {
        "id": "Zw9eNgAjLqsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tony-pitchblack/FinRL.git@benchmarking \\\n",
        "    # --force-reinstall --no-deps"
      ],
      "metadata": {
        "id": "w6nvjUhJQPQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependencies listed in the requirements.txt file\n",
        "!wget https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt -O requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "1S3eA6pMHTxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a44b179-0a6a-47ae-b33e-ba6c4b2f1047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[autoreload of packaging.utils failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: canonicalize_version() requires a code object with 2 free vars, not 0\n",
            "]\n",
            "[autoreload of urllib3.exceptions failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n",
            "[autoreload of urllib3.util.connection failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
            "    module = reload(module)\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/imp.py\", line 315, in reload\n",
            "    return importlib.reload(module)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 8, in <module>\n",
            "    from .wait import NoWayToWaitForSocketError, wait_for_read\n",
            "ImportError: cannot import name 'NoWayToWaitForSocketError' from 'urllib3.util.wait' (/usr/local/lib/python3.11/dist-packages/urllib3/util/wait.py)\n",
            "]\n",
            "[autoreload of urllib3._collections failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n",
            "[autoreload of urllib3.connection failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
            "    module = reload(module)\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/imp.py\", line 315, in reload\n",
            "    return importlib.reload(module)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 15, in <module>\n",
            "    from .util.proxy import create_proxy_ssl_context\n",
            "ImportError: cannot import name 'create_proxy_ssl_context' from 'urllib3.util.proxy' (/usr/local/lib/python3.11/dist-packages/urllib3/util/proxy.py)\n",
            "]\n",
            "[autoreload of urllib3.response failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n",
            "[autoreload of urllib3.connectionpool failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n",
            "[autoreload of urllib3.poolmanager failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
            "    update_generic(old_obj, new_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
            "    if update_generic(old_obj, new_obj): continue\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
            "    update(a, b)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
            "    setattr(old, name, getattr(new, name))\n",
            "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-02 10:44:46--  https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     766  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-02 10:44:46 (36.7 MB/s) - ‘requirements.txt’ saved [766/766]\n",
            "\n",
            "Requirement already satisfied: alpaca_trade_api>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.2.0)\n",
            "Collecting alpaca-py (from -r requirements.txt (line 2))\n",
            "  Downloading alpaca_py-0.37.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting selenium (from -r requirements.txt (line 3))\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting webdriver-manager (from -r requirements.txt (line 4))\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: ccxt>=1.66.32 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.60)\n",
            "Collecting exchange_calendars==3.6.3 (from -r requirements.txt (line 7))\n",
            "  Downloading exchange_calendars-3.6.3.tar.gz (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gputil (from -r requirements.txt (line 8))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.0.0)\n",
            "Collecting importlib-metadata==4.13.0 (from -r requirements.txt (line 10))\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: jqdatasdk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.9.7)\n",
            "Collecting lz4 (from -r requirements.txt (line 12))\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
            "Collecting pre-commit (from -r requirements.txt (line 20))\n",
            "  Downloading pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (8.3.4)\n",
            "Collecting recommonmark (from -r requirements.txt (line 29))\n",
            "  Downloading recommonmark-0.7.1-py2.py3-none-any.whl.metadata (463 bytes)\n",
            "Requirement already satisfied: dm_tree in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.1.8)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=65.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (75.1.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (8.1.3)\n",
            "Collecting sphinx_rtd_theme (from -r requirements.txt (line 41))\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (2.0.37)\n",
            "Requirement already satisfied: stockstats>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (0.5.4)\n",
            "Collecting swig (from -r requirements.txt (line 47))\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (2.6.2.2)\n",
            "Requirement already satisfied: wheel>=0.33.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (0.45.1)\n",
            "Requirement already satisfied: wrds in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (3.2.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.2.52)\n",
            "Requirement already satisfied: ray[default] in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (2.41.0)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (0.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata==4.13.0->-r requirements.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.11.11)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py->-r requirements.txt (line 2)) (2.10.6)\n",
            "Collecting sseclient-py<2.0.0,>=1.7.2 (from alpaca-py->-r requirements.txt (line 2))\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting trio~=0.17 (from selenium->-r requirements.txt (line 3))\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->-r requirements.txt (line 3))\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (4.12.2)\n",
            "Collecting python-dotenv (from webdriver-manager->-r requirements.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (43.0.3)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (1.18.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (3.2.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->-r requirements.txt (line 17)) (2025.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading identify-2.6.6-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->-r requirements.txt (line 20))\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit->-r requirements.txt (line 20)) (20.29.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (1.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.5.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (7.1.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.70.0)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (2024.10.0)\n",
            "Collecting ormsgpack==1.7.0 (from ray[rllib]->-r requirements.txt (line 27))\n",
            "  Downloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (1.12.0)\n",
            "Collecting commonmark>=0.8.1 (from recommonmark->-r requirements.txt (line 29))\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.11/dist-packages (from recommonmark->-r requirements.txt (line 29)) (0.21.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (3.5.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (3.1.5)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.4.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx_rtd_theme->-r requirements.txt (line 41))\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy->-r requirements.txt (line 44)) (3.1.1)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/dist-packages (from wrds->-r requirements.txt (line 51)) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (1.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from aiodns>=1.1.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements.txt (line 54)) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 54)) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx->-r requirements.txt (line 40)) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.0.11)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.11)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->-r requirements.txt (line 3))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->-r requirements.txt (line 3))\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->-r requirements.txt (line 3)) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r requirements.txt (line 3))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 3)) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 20)) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.22.3)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]->-r requirements.txt (line 26)) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]->-r requirements.txt (line 26)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default]->-r requirements.txt (line 26)) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (2.27.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r requirements.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.6.1)\n",
            "Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Downloading alpaca_py-0.37.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading recommonmark-0.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading identify-2.6.6-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: exchange_calendars, gputil\n",
            "  Building wheel for exchange_calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for exchange_calendars: filename=exchange_calendars-3.6.3-py3-none-any.whl size=182609 sha256=b7b8ae35fdc6c3e998b8c0082a3f71470e9e3090ee037707aee44d6792fc637b\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/56/fc/ac3dbf596f81748aa41efee8608b8effb6ef4453862dd0e045\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=d4ff2161594633edc5d58f9df9f041b23d56187a246a7511c9c081b172049a19\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built exchange_calendars gputil\n",
            "Installing collected packages: swig, sseclient-py, sortedcontainers, gputil, commonmark, wsproto, python-dotenv, outcome, ormsgpack, nodeenv, lz4, importlib-metadata, identify, cfgv, webdriver-manager, trio, pre-commit, trio-websocket, sphinxcontrib-jquery, recommonmark, exchange_calendars, alpaca-py, sphinx_rtd_theme, selenium\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: exchange_calendars\n",
            "    Found existing installation: exchange_calendars 4.9\n",
            "    Uninstalling exchange_calendars-4.9:\n",
            "      Successfully uninstalled exchange_calendars-4.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "finrl 0.3.6 requires exchange-calendars<5,>=4, but you have exchange-calendars 3.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alpaca-py-0.37.0 cfgv-3.4.0 commonmark-0.9.1 exchange_calendars-3.6.3 gputil-1.4.0 identify-2.6.6 importlib-metadata-4.13.0 lz4-4.4.3 nodeenv-1.9.1 ormsgpack-1.7.0 outcome-1.3.0.post0 pre-commit-4.1.0 python-dotenv-1.0.1 recommonmark-0.7.1 selenium-4.28.1 sortedcontainers-2.4.0 sphinx_rtd_theme-3.0.2 sphinxcontrib-jquery-4.1 sseclient-py-1.8.0 swig-4.3.0 trio-0.28.0 trio-websocket-0.11.1 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "06044b88adf0479bb8bf7928835ed086"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU-vXqDRW3L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US_vB7hNSdeu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJsl_3tVre6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ],
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General funcs"
      ],
      "metadata": {
        "id": "oWn4ZCwkvtN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title YahooDownloader\n",
        "\n",
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
        "            )\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "\n",
        "        try:\n",
        "            # Convert wide to long format\n",
        "            # print(f\"DATA COLS: {data_df.columns}\")\n",
        "            data_df = data_df.sort_index(axis=1).set_index(['Date']).drop(columns=['tic']).stack(level='Ticker', future_stack=True)\n",
        "            data_df.reset_index(inplace=True)\n",
        "            data_df.columns.name = ''\n",
        "\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(columns={'Ticker': 'Tic', 'Adj Close': 'Adjcp'}, inplace=True)\n",
        "            data_df.rename(columns={col: col.lower() for col in data_df.columns}, inplace=True)\n",
        "\n",
        "            columns = [\n",
        "                \"date\",\n",
        "                \"tic\",\n",
        "                \"open\",\n",
        "                \"high\",\n",
        "                \"low\",\n",
        "                \"close\",\n",
        "                \"adjcp\",\n",
        "                \"volume\",\n",
        "            ]\n",
        "\n",
        "            data_df = data_df[columns]\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "gbd4N4QLPXlL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhIyXmfQ8EAS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title fix_daily_index\n",
        "\n",
        "def make_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    # Get unique dates and create a mapping to daily indices\n",
        "    total_dates = data_df[date_column].unique()\n",
        "    date_to_index = {date: idx for idx, date in enumerate(sorted(total_dates))}\n",
        "    return data_df[date_column].map(date_to_index)\n",
        "\n",
        "def set_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    \"\"\"\n",
        "    Constructs a daily index from unique dates in the specified column.\n",
        "\n",
        "    Parameters:\n",
        "        data_df (pd.DataFrame): The input DataFrame.\n",
        "        date_column (str): The name of the column containing dates.\n",
        "        new_index_name (str): The name for the new index.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a daily index.\n",
        "    \"\"\"\n",
        "\n",
        "    # Map dates to daily indices and set as index\n",
        "    data_df[new_index_name] = make_daily_index(data_df, date_column='date', new_index_name='date_index')\n",
        "\n",
        "    data_df.set_index(new_index_name, inplace=True)\n",
        "    data_df.index.name = ''  # Remove the index name for simplicity\n",
        "\n",
        "    return data_df\n",
        "\n",
        "def fix_daily_index(df):\n",
        "    if df.index.name == 'date':\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    daily_index = make_daily_index(df, date_column='date', new_index_name='date_index')\n",
        "    if (df.index.values != daily_index.values).any():\n",
        "\n",
        "        df.index = daily_index\n",
        "        df.index.name = ''\n",
        "\n",
        "    return df\n",
        "\n",
        "# trade = fix_daily_index(trade)\n",
        "# trade.index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get dataset name\n",
        "\n",
        "def get_quarterly_dataset_name(prefix, train_start_date, val_start_date, test_start_date):\n",
        "    get_quarter = lambda date: f'Q{(date.month - 1) // 3 + 1}'\n",
        "\n",
        "    val_quarter = get_quarter(val_start_date)\n",
        "    test_quarter = get_quarter(test_start_date)\n",
        "\n",
        "    # Extract year and month\n",
        "    train_start = f\"{train_start_date.year}-{train_start_date.month:02}\"\n",
        "    val_start = f\"{val_start_date.year}\"\n",
        "    test_start = f\"{test_start_date.year}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start} | {val_start} {val_quarter} | {test_start} {test_quarter}\"\n",
        "\n",
        "    return dataset_name\n",
        "\n",
        "def get_yearly_dataset_name(prefix, train_start, test_start, test_end):\n",
        "    # Extract year and month\n",
        "    train_start_str = f\"{train_start.year}-{train_start.month:02}\"\n",
        "    test_start_str = f\"{test_start.year}-{test_start.month:02}\"\n",
        "    test_end_str = f\"{test_end.year}-{test_end.month:02}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start_str} | {test_start_str} | {test_end_str}\"\n",
        "    return dataset_name\n"
      ],
      "metadata": {
        "id": "GQ6BIJxbwuVh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOfz3JlX-oG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title add_dataset\n",
        "\n",
        "def add_dataset(stock_index_name, train_df, test_df):\n",
        "    if 'datasets' not in globals():\n",
        "        global datasets\n",
        "        datasets = {}\n",
        "\n",
        "    # Ensure datetime format\n",
        "    if 'date' in train_df.columns:\n",
        "        train_df.set_index('date', inplace=True)\n",
        "    train_df.index = pd.to_datetime(train_df.index)\n",
        "\n",
        "    if 'date' in test_df.columns:\n",
        "        test_df.set_index('date', inplace=True)\n",
        "    test_df.index = pd.to_datetime(test_df.index)\n",
        "\n",
        "    train_start_date = train_df.index[0]\n",
        "    test_start_date = test_df.index[0]\n",
        "    test_end_date = test_df.index[-1]\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        stock_index_name,\n",
        "        train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    train_df.reset_index(inplace=True)\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    ticker_list = train_df.tic.unique().tolist()\n",
        "\n",
        "    datasets[dataset_name] = {\n",
        "        'train': train_df,\n",
        "        'test': test_df,\n",
        "        'metadata': dict(\n",
        "            stock_index_name = stock_index_name,\n",
        "            train_start_date = train_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "            num_tickers = len(ticker_list),\n",
        "            ticker_list = ticker_list,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUph5lzrTUS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA: DOW-30 (quarterly train/val/test)"
      ],
      "metadata": {
        "id": "LHIfVohUTAsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_date = '2015-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2016-10-01'"
      ],
      "metadata": {
        "id": "dTOtt7iQg7TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate_quarterly_date_ranges\n",
        "\n",
        "def generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=False,\n",
        "    finetune_previous_val=False\n",
        "):\n",
        "    is_quarter_start = lambda date: date.month in [1, 4, 7, 10] and date.day == 1\n",
        "\n",
        "    min_test_start_date = pd.Timestamp(min_test_start_date)\n",
        "    train_start_date = pd.Timestamp(train_start_date)\n",
        "    max_test_end_date = pd.Timestamp(max_test_end_date)\n",
        "\n",
        "    assert is_quarter_start(train_start_date), f\"train_start_date {train_start_date} is not a quarter start date.\"\n",
        "    assert is_quarter_start(min_test_start_date), f\"min_test_start_date {min_test_start_date} is not a quarter start date.\"\n",
        "\n",
        "    test_start_date = min_test_start_date\n",
        "    date_ranges = []\n",
        "    full_train_start_date = train_start_date\n",
        "\n",
        "    while True:\n",
        "        val_start_date = test_start_date - pd.DateOffset(months=3)\n",
        "        test_end_date = test_start_date + pd.DateOffset(months=3)\n",
        "\n",
        "        if test_end_date > max_test_end_date:\n",
        "            break\n",
        "\n",
        "        if len(date_ranges) == 0:\n",
        "            # The first date_range contains the full training period\n",
        "            train_start_date = full_train_start_date\n",
        "        elif finetune_previous_val:\n",
        "            # Use the previous validation range as the training range\n",
        "            train_start_date = date_ranges[-1]['val_start_date']\n",
        "\n",
        "        date_range = dict(\n",
        "            train_start_date=train_start_date,\n",
        "            val_start_date=val_start_date,\n",
        "            test_start_date=test_start_date,\n",
        "            test_end_date=test_end_date,\n",
        "        )\n",
        "\n",
        "        if return_strings:\n",
        "            date_range = {k: str(v) for k, v in date_range.items()}\n",
        "\n",
        "        date_ranges.append(date_range)\n",
        "\n",
        "        test_start_date = test_end_date\n",
        "\n",
        "    return date_ranges\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "# print(*date_ranges[:2], sep='\\n')\n",
        "print(*date_ranges, sep='\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DGMvKo0wxwYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12798cd4-6064-4feb-94aa-5a2d9c14ad51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_start_date': Timestamp('2015-01-01 00:00:00'), 'val_start_date': Timestamp('2015-10-01 00:00:00'), 'test_start_date': Timestamp('2016-01-01 00:00:00'), 'test_end_date': Timestamp('2016-04-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2015-10-01 00:00:00'), 'val_start_date': Timestamp('2016-01-01 00:00:00'), 'test_start_date': Timestamp('2016-04-01 00:00:00'), 'test_end_date': Timestamp('2016-07-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2016-01-01 00:00:00'), 'val_start_date': Timestamp('2016-04-01 00:00:00'), 'test_start_date': Timestamp('2016-07-01 00:00:00'), 'test_end_date': Timestamp('2016-10-01 00:00:00')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8GdHjZWTcHu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title split_data\n",
        "\n",
        "def split_data(data_df, date_range):\n",
        "    def subset_date_range(df, start_date, end_date):\n",
        "        df = df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "        df = fix_daily_index(df)\n",
        "        return df\n",
        "\n",
        "    return {\n",
        "        'train': subset_date_range(data_df, date_range['train_start_date'], date_range['val_start_date']),\n",
        "        'val': subset_date_range(data_df, date_range['val_start_date'], date_range['test_start_date']),\n",
        "        'test': subset_date_range(data_df, date_range['test_start_date'], date_range['test_end_date']),\n",
        "    }\n",
        "\n",
        "# data_splits = split_data(preproc_df, date_ranges[0])\n",
        "# data_splits['train'].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prepare_data (for np env)\n",
        "import hashlib\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "import os\n",
        "\n",
        "CACHE_DIR = './cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def stable_hash(data):\n",
        "    return hashlib.sha256(str(data).encode()).hexdigest()\n",
        "\n",
        "def get_env_config(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    if_train,\n",
        "    ticker_list=DOW_30_TICKER,\n",
        "    technical_indicator_list=INDICATORS,\n",
        "    time_interval='1d',\n",
        "    if_vix=True,\n",
        "    **kwargs\n",
        "):\n",
        "    data_hash = stable_hash(tuple(sorted(ticker_list) + sorted(technical_indicator_list)))\n",
        "    file_path = Path(CACHE_DIR) / f\"{start_date}_{end_date}_{time_interval}_{data_hash}.csv\"\n",
        "    dp = DataProcessor(data_source='yahoofinance', tech_indicator=technical_indicator_list, vix=if_vix, **kwargs)\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Using cached data: {file_path}\")\n",
        "        data = pd.read_csv(file_path, index_col=0)\n",
        "    else:\n",
        "        print(\"Creating new data.\")\n",
        "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "        data = dp.clean_data(data)\n",
        "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "        if if_vix:\n",
        "            data = dp.add_vix(data)\n",
        "        data.to_csv(file_path)\n",
        "\n",
        "    price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n",
        "\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"if_train\": if_train\n",
        "    }\n",
        "\n",
        "    return env_config\n",
        "\n",
        "# date_range=date_ranges[0]\n",
        "# train_env_config = get_env_config(\n",
        "#     start_date=date_range['val_start_date'],\n",
        "#     end_date=date_range['test_start_date'],\n",
        "#     if_train=True\n",
        "# )\n",
        "# val_env_config = get_env_config(\n",
        "#     start_date=date_range['test_start_date'],\n",
        "#     end_date=date_range['test_end_date'],\n",
        "#     if_train=False\n",
        "# )"
      ],
      "metadata": {
        "id": "E34A0Yyhy4Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NYSbz9QQ7pS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb artifacts"
      ],
      "metadata": {
        "id": "KemUy0OKg-wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_artifact\n",
        "\n",
        "def update_artifact(folder_path, name_prefix, type):\n",
        "    \"\"\"\n",
        "    Create or update a W&B artifact consisting of a folder.\n",
        "\n",
        "    Args:\n",
        "        run: The current W&B run.\n",
        "        folder_path (str): Path to the folder to upload.\n",
        "        artifact_name (str): Name of the artifact.\n",
        "        artifact_type (str): Type of the artifact.\n",
        "    \"\"\"\n",
        "    run = wandb.run\n",
        "    artifact_name = f'{name_prefix}-{wandb.run.id}'\n",
        "\n",
        "    # Create a new artifact\n",
        "    artifact = wandb.Artifact(name=artifact_name, type=type)\n",
        "\n",
        "    # Add the folder to the artifact\n",
        "    artifact.add_dir(folder_path)\n",
        "\n",
        "    # Log the artifact to W&B\n",
        "    run.log_artifact(artifact)\n",
        "    print(f\"Artifact '{artifact_name}' has been updated and uploaded.\")"
      ],
      "metadata": {
        "id": "jF0Xbv9f631H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCmdRFmDh-CI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title update_model_artifacts\n",
        "\n",
        "def update_model_artifacts(log_results_folder=True):\n",
        "    if log_results_folder:\n",
        "        update_artifact(\n",
        "            folder_path = RESULTS_DIR,\n",
        "            name_prefix = 'results',\n",
        "            type = 'results'\n",
        "        )\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = TRAINED_MODEL_DIR,\n",
        "        name_prefix = 'trained_models',\n",
        "        type = 'trained_models'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_dataset_artifact\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def update_dataset_artifact(config, train_df, val_df=None, test_df=None):\n",
        "    DATASET_DIR = Path('./dataset')\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(DATASET_DIR / 'train_data.csv', index=False)\n",
        "\n",
        "    if test_df is not None:\n",
        "        test_df.to_csv(DATASET_DIR / 'test_data.csv', index=False)\n",
        "\n",
        "    if val_df is not None:\n",
        "        val_df.to_csv(DATASET_DIR / 'val_data.csv', index=False)\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = DATASET_DIR,\n",
        "        name_prefix = 'dataset',\n",
        "        type = 'dataset'\n",
        "    )"
      ],
      "metadata": {
        "id": "Prm8SfPo7CJY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_env_state_artifact\n",
        "\n",
        "def update_env_state_artifact(val_env_end_state):\n",
        "    file_path = 'val_env_end_state.csv'\n",
        "\n",
        "    df_last_state = pd.DataFrame({\"last_state\": val_env_end_state})\n",
        "    df_last_state.to_csv(\n",
        "        file_path, index=False\n",
        "    )\n",
        "\n",
        "    artifact = wandb.Artifact(name=f'val_env_end_state-{wandb.run.id}', type='env_state')\n",
        "    artifact.add_file(file_path)\n",
        "    wandb.run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "x66rQn0TGEkM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & helper funcs"
      ],
      "metadata": {
        "id": "_BZTsxX0tkDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_yearly_train_test\n",
        "def build_yearly_train_test(config):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        config['train_years_count'],\n",
        "        config['test_years_count'],\n",
        "        config['test_start_year']\n",
        "    )\n",
        "\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        train_start_date=train_start_date,\n",
        "        test_start_date=test_start_date,\n",
        "        test_end_date=test_end_date,\n",
        "        dataset_name=dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SmzfG0qaVP93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_quarterly_train_val_test\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "\n",
        "def build_quarterly_train_val_test(config):\n",
        "    date_range = {key: pd.Timestamp(date) for key, date in config['date_range'].items()}\n",
        "\n",
        "    train_start_date = date_range['train_start_date']\n",
        "    val_start_date = date_range['val_start_date']\n",
        "    test_start_date = date_range['test_start_date']\n",
        "    test_end_date = date_range['test_end_date']\n",
        "\n",
        "    train_env_config = get_env_config(\n",
        "        start_date=date_range['train_start_date'],\n",
        "        end_date=date_range['val_start_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    val_env_config = get_env_config(\n",
        "        start_date=date_range['val_start_date'],\n",
        "        end_date=date_range['test_start_date'],\n",
        "        if_train=True\n",
        "    )\n",
        "\n",
        "    test_env_config = get_env_config(\n",
        "        start_date=date_range['test_start_date'],\n",
        "        end_date=date_range['test_end_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    dataset_name = get_quarterly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, val_start_date, test_start_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        dataset_name = dataset_name\n",
        "    ))\n",
        "\n",
        "    return train_env_config, val_env_config, test_env_config\n",
        "\n",
        "# train_env_config, val_env_config, test_env_config = build_quarterly_train_val_test(config)"
      ],
      "metadata": {
        "id": "N2JCHB8ibDce",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init StockTradingEnv (numpy)\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.config import INDICATORS\n",
        "# from finrl.config import CACHE_DIR\n",
        "\n",
        "def init_env(\n",
        "    np_env_config,\n",
        "    run_config,\n",
        "    mode,\n",
        "    turbulence_threshold=99,\n",
        "):\n",
        "    assert mode in ['train', 'val', 'test']\n",
        "\n",
        "    env = StockTradingEnv(\n",
        "        config=np_env_config,\n",
        "        initial_capital=run_config['initial_amount'],\n",
        "        buy_cost_pct=run_config['cost_pct'],\n",
        "        sell_cost_pct=run_config['cost_pct'],\n",
        "        turbulence_thresh=turbulence_threshold\n",
        "    )\n",
        "\n",
        "    return env\n",
        "\n",
        "# env = init_env(\n",
        "#     train_np_env_config,\n",
        "#     run_config,\n",
        "#     'train'\n",
        "# )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Mp0H06urpp53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0-3fjeCG_GJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frdIBaq0pqe2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title compute metrics\n",
        "import wandb\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(account_values: List[pd.DataFrame, pd.Series, np.array]):\n",
        "    \"\"\"\n",
        "    If DataFrame then should contain two columns - 'date' and name of algo, e.g. 'a2c'.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(account_values, pd.DataFrame):\n",
        "        assert isinstance(account_values, pd.DataFrame)\n",
        "        if 'date' not in account_values.columns:\n",
        "            if account_values.index.name == 'date':\n",
        "                account_values.reset_index(inplace=True)\n",
        "            else:\n",
        "                raise ValueError(\"should contain 'date' column or index\")\n",
        "        account_values = account_values.dropna().set_index('date').iloc[:, 0]\n",
        "    elif isinstance(account_values, np.ndarray):\n",
        "        account_values = pd.Series(account_values)\n",
        "\n",
        "    sharpe = calculate_sharpe_ratio(account_values)\n",
        "    mdd = calculate_mdd(account_values)\n",
        "    cum_ret = (account_values.iloc[-1] - account_values.iloc[0]) / account_values.iloc[0] * 100\n",
        "    # num_days = (account_values.index.max() - account_values.index.min()).days\n",
        "    num_days = len(account_values)\n",
        "    ann_ret = ((1 + cum_ret / 100) ** (365 / num_days) - 1) * 100\n",
        "\n",
        "    return {\n",
        "            f'sharpe_ratio': sharpe,\n",
        "            f'mdd': mdd,\n",
        "            f'ann_return': ann_ret,\n",
        "            f'cum_return': cum_ret,\n",
        "        }\n",
        "\n",
        "def get_env_metrics(env):\n",
        "    end_total_asset = env.state[0] + sum(\n",
        "        np.array(env.state[1 : (env.stock_dim + 1)])\n",
        "        * np.array(env.state[(env.stock_dim + 1) : (env.stock_dim * 2 + 1)])\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'begin_total_asset': env.asset_memory[0],\n",
        "        'end_total_asset': end_total_asset,\n",
        "        'total_cost': env.cost,\n",
        "        'total_trades': env.trades,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG9DR-fz9e-s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_metrics\n",
        "\n",
        "def log_metrics(metrics, model_name, split_label, step=None):\n",
        "    print(f'log_metrics for {model_name}')\n",
        "\n",
        "    rename_metrics = lambda model_name: {\n",
        "        f\"{key}/{model_name}\": value for key, value in metrics.items()\n",
        "    }\n",
        "\n",
        "    renamed_metrics = rename_metrics(model_name)\n",
        "    wandb.log({split_label: renamed_metrics}, step=step)\n",
        "    # wandb.run.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_best_model_metrics\n",
        "\n",
        "def update_best_model_metrics(metrics, model_name, split_label):\n",
        "    if 'sharpe_ratios' not in wandb.run.config:\n",
        "        wandb.run.config['sharpe_ratios'] = {}\n",
        "\n",
        "    if split_label not in wandb.run.config['sharpe_ratios']:\n",
        "        wandb.run.config['sharpe_ratios'][split_label] = {}\n",
        "\n",
        "    sharpe_ratios = wandb.run.config['sharpe_ratios'][split_label]\n",
        "\n",
        "    print(f\"DEBUG ({split_label}): run.id = {wandb.run.id}\")\n",
        "    print(f\"DEBUG ({split_label}): sharpe_ratios = {sharpe_ratios}\")\n",
        "    print(f\"DEBUG ({split_label}): updating best model based on sharpe_ratios: {sharpe_ratios}\")\n",
        "    if len(sharpe_ratios) > 0:\n",
        "        best_model_name = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "        if metrics['sharpe_ratio'] > sharpe_ratios[best_model_name]:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" > {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                f\". New best model: {model_name}.\"\n",
        "            )\n",
        "            log_metrics(metrics, 'best_model', split_label)\n",
        "            wandb.log({split_label: {'best_model_name': model_name}})\n",
        "        else:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" <= {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                \". Not updating best model.\"\n",
        "            )\n",
        "    else:\n",
        "        print(f\"DEBUG ({split_label}): no models logged yet, new best model is current one: {model_name}\")\n",
        "        print(f\"DEBUG ({split_label}): wandb.run.config['sharpe_ratios'] = {wandb.run.config['sharpe_ratios']}\")\n",
        "        log_metrics(metrics, 'best_model', split_label)\n",
        "\n",
        "    wandb.run.config['sharpe_ratios'][split_label][model_name] = metrics['sharpe_ratio']"
      ],
      "metadata": {
        "cellView": "form",
        "id": "78Xhl6wmmY9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "znlXqpfQZkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title init\n",
        "parameters_dict = {}\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'test.sharpe_ratio/best_model'\n",
        "    },\n",
        "    'parameters': parameters_dict\n",
        "}"
      ],
      "metadata": {
        "id": "kF6LboMHB2xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - yearly_train_test\n",
        "\n",
        "min_test_start_year = 2020\n",
        "max_test_start_year = 2025\n",
        "\n",
        "########################################################\n",
        "\n",
        "yearly_dataset_params = dict(\n",
        "    # dataset_period = {'value': 'year'},\n",
        "    # dataset_splits = {'parameters': {\n",
        "    #     'train': {'value': True },\n",
        "    #     'val': {'value': False },\n",
        "    #     'test': {'value': True },\n",
        "    # }},\n",
        "\n",
        "    dataset_type = {'value':'yearly_train_test'},\n",
        "\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_years_count = {'value': 10},\n",
        "    test_years_count = {'value': 1},\n",
        "    test_start_year = {\n",
        "        'values': list(range(min_test_start_year, max_test_start_year))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7RzkkJNiSkP7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - quarterly_train_test\n",
        "\n",
        "# Full date range (18 backtests)\n",
        "train_start_date = '2009-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2020-08-05'\n",
        "\n",
        "#################################################################\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date, min_test_start_date, max_test_end_date, return_strings=True\n",
        ")\n",
        "\n",
        "quarterly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'quarterly_train_val_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "    train_start_date = {'value': train_start_date},\n",
        "    min_test_start_date = {'value': min_test_start_date},\n",
        "    max_test_end_date = {'value': max_test_end_date},\n",
        "    date_range = {\n",
        "        'values': date_ranges\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "l-hEkHxAfYVv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: choose dataset\n",
        "parameters_dict.update(\n",
        "    # yearly_dataset_params,\n",
        "    quarterly_dataset_params\n",
        ")"
      ],
      "metadata": {
        "id": "GJDvneoY00zo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: env params\n",
        "parameters_dict.update(dict(\n",
        "    cost_pct = {'value': 1e-3},\n",
        "    initial_amount = {'value': 50_000},\n",
        "    turbulence_threshold = {\n",
        "        'value': 99,\n",
        "        # 'values': [30, 40, 50, 60, 70]\n",
        "    },\n",
        "    if_vix = {'value': True}\n",
        "))"
      ],
      "metadata": {
        "id": "ILF379nrW4YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: models_used\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': True},\n",
        "    'if_using_ppo': {'value': True},\n",
        "    'if_using_td3': {'value': True},\n",
        "    'if_using_sac': {'value': True}\n",
        "})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PzWRRS6Prorh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: model and training params\n",
        "model_params = {\n",
        "    'parameters': {\n",
        "        'ppo': {'value': dict(\n",
        "            train_batch_size=2048,\n",
        "            num_epochs=10,\n",
        "            minibatch_size=128,\n",
        "            lr=5e-5,\n",
        "            gamma=0.99\n",
        "        )}\n",
        "    }\n",
        "}\n",
        "\n",
        "train_params = {\n",
        "    'parameters': {\n",
        "        'ppo': {\n",
        "            'parameters': {\n",
        "                'steps': {'value': 2_048}\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "env_runners_params = {\n",
        "    'parameters': dict(\n",
        "        num_envs_per_env_runner = {'value': 1},\n",
        "        num_env_runners = {'value': 0},\n",
        "    )\n",
        "}\n",
        "\n",
        "parameters_dict.update({'env_runners_params': env_runners_params})\n",
        "parameters_dict.update({'train_params': train_params})\n",
        "parameters_dict.update({'model_params': model_params})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XaCu08TIKegP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & eval funcs"
      ],
      "metadata": {
        "id": "oqc-9r54tuXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kRSGp8MduO9_"
      },
      "outputs": [],
      "source": [
        "#@title MetricsLoggerCallback (class)\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from typing import Optional, Sequence\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "class MetricsLoggerCallback(DefaultCallbacks):\n",
        "    def __init__(self, model_name, ema_coeff=0.2, ma_window=20, log_to_wandb=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.ema_coeff = ema_coeff\n",
        "        self.ma_window = ma_window\n",
        "        self.metric_names = set()\n",
        "        self.log_to_wandb = log_to_wandb\n",
        "\n",
        "    def unwrap_env(self, env):\n",
        "        env = env.unwrapped\n",
        "        # print(type(env))\n",
        "        # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.vector.sync_vector_env.SyncVectorEnv'>\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            return env\n",
        "        else:\n",
        "            env = env.envs[0]\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.OrderEnforcing'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.PassiveEnvChecker'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'finrl.meta.env_stock_trading.env_stocktrading.StockTradingEnv'>\n",
        "        return env\n",
        "\n",
        "    def on_episode_step(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        env = self.unwrap_env(env)\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            asset_values = env.get_attr('asset_memory')\n",
        "            asset_values = pd.concat([pd.Series(av) for av in asset_values], axis=1).mean(axis=1)\n",
        "        else:\n",
        "            asset_values = env.asset_memory\n",
        "\n",
        "        metrics = get_account_value_metrics(asset_values)\n",
        "\n",
        "        # mode = env.mode\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            # metric_name = f\"{mode}/{metric_name}\" if mode != \"\" else metric_name\n",
        "            episode.add_temporary_timestep_data(metric_name, metric_value)\n",
        "            self.metric_names.update([metric_name])\n",
        "\n",
        "    def on_episode_end(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        for metric_name in self.metric_names:\n",
        "            metric_values = episode.get_temporary_timestep_data(metric_name)\n",
        "            metric_value = np.nanmean(np.array(metric_values))\n",
        "\n",
        "            # metrics_logger.log_value(\n",
        "            #     metric_name,\n",
        "            #     metric_value,\n",
        "            #     reduce='mean',\n",
        "            # )\n",
        "\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_EMA_{self.ema_coeff}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                ema_coeff=self.ema_coeff\n",
        "            )\n",
        "\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_ma_{self.ma_window}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                window=self.ma_window\n",
        "            )\n",
        "\n",
        "            if self.log_to_wandb:\n",
        "                mode = 'val' if env_runner.config.in_evaluation else 'train'\n",
        "                wandb.log({\n",
        "                    f\"{mode}.{metric_name}/{self.model_name}\": metric_value,\n",
        "                }) # TODO: log on every episode step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title print_result\n",
        "\n",
        "RESULT_KEYS_TO_INCLUDE = [\n",
        "    'sharpe_ratio_MA',\n",
        "    'ann_return_MA',\n",
        "    'mdd_MA',\n",
        "\n",
        "    'sharpe_ratio_EMA',\n",
        "    'ann_return_EMA',\n",
        "    'mdd_EMA',\n",
        "]\n",
        "\n",
        "def print_result(result):\n",
        "    print()\n",
        "    for key in result['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"train/{key}: {round(result['env_runners'][key], 2)}\")\n",
        "                break\n",
        "\n",
        "    for key in result['evaluation']['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"val/{key}: {round(result['evaluation']['env_runners'][key], 2)}\")\n",
        "                break\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gg8DIuPJ9FwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0iyYcwTOZYP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title benchmark_exec_time\n",
        "import pandas as pd\n",
        "from time import perf_counter\n",
        "from functools import wraps\n",
        "\n",
        "def benchmark_exec_time(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "\n",
        "        start = perf_counter()\n",
        "        output = func(*args, **kwargs)\n",
        "        end = perf_counter()\n",
        "\n",
        "        exec_time_sec = end - start\n",
        "\n",
        "        data = {\n",
        "            \"func_name\": func.__name__,\n",
        "            \"exec_time_sec\": exec_time_sec,\n",
        "        }\n",
        "        print(f'\\nBenchmark results: {data}')\n",
        "        return output, exec_time_sec\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train_eval_models\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_models = {}\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            print(f\"Training {model_name.upper()} agent\")\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "            algo = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = pretrained_models.get(model_name, None)\n",
        "            )\n",
        "\n",
        "            print(f\"Evaluating {model_name.upper()} agent\")\n",
        "            val_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = val_np_env_config,\n",
        "                mode='val',\n",
        "            )\n",
        "            fig = plot_results(**val_result)\n",
        "            log_plot_as_artifact(fig, \"asset_value_comparison_validation\", artifact_type=\"plot\")\n",
        "\n",
        "            test_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = test_np_env_config,\n",
        "                mode='test',\n",
        "            )\n",
        "            fig = plot_results(**test_result)\n",
        "            log_plot_as_artifact(fig, \"test_asset_value_comparison_test\", artifact_type=\"plot\")\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")"
      ],
      "metadata": {
        "id": "TLDeH1lN2XAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15pANCivoIru"
      },
      "outputs": [],
      "source": [
        "#@title Train RLlib model\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from ray.tune.registry import register_env\n",
        "from time import perf_counter\n",
        "from math import ceil\n",
        "import ray\n",
        "\n",
        "def train_rllib_model(\n",
        "    run_config,\n",
        "    model_name,\n",
        "    model_config,\n",
        "    train_np_env_config,\n",
        "    val_np_env_config,\n",
        "    pretrained_algo=None\n",
        "):\n",
        "    num_envs_per_env_runner = run_config['env_runners_params']['num_envs_per_env_runner']\n",
        "    num_env_runners = run_config['env_runners_params']['num_env_runners']\n",
        "\n",
        "    if pretrained_algo:\n",
        "        algo = pretrained_algo\n",
        "    else:\n",
        "        run_config = (\n",
        "            model_config()\n",
        "            .environment(\n",
        "                env=\"stock_trading_env\",\n",
        "                env_config={\n",
        "                    # \"df\": train,\n",
        "                    \"np_env_config\": train_np_env_config,\n",
        "                    \"run_config\": run_config,\n",
        "                    \"mode\": 'train'\n",
        "                },\n",
        "            )\n",
        "            .env_runners(\n",
        "                num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "                num_env_runners=num_env_runners,\n",
        "                num_cpus_per_env_runner= (2/num_env_runners) if num_env_runners > 2 else None,\n",
        "\n",
        "                # gym_env_vectorize_mode=gym.envs.registration.VectorizeMode.ASYNC,\n",
        "            )\n",
        "            .training(\n",
        "                train_batch_size=2048,\n",
        "                num_epochs=10,\n",
        "                minibatch_size=128,\n",
        "            )\n",
        "            .evaluation(\n",
        "                # Set up the validation environment\n",
        "                evaluation_interval=1,  # Specify evaluation frequency (1=after each training step)\n",
        "                evaluation_config={\n",
        "                    \"env\": \"stock_trading_env\",\n",
        "                    \"env_config\": {\n",
        "                        # \"df\": val,\n",
        "                        \"np_env_config\": val_np_env_config,\n",
        "                        \"run_config\": run_config,\n",
        "                        \"mode\": 'val'\n",
        "                    },\n",
        "                },\n",
        "            )\n",
        "            # .callbacks(MetricsLoggerCallback)\n",
        "            .callbacks(partial(MetricsLoggerCallback, model_name='ppo', log_to_wandb=True))\n",
        "            .resources(\n",
        "                num_gpus=1 if torch.cuda.is_available() else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 2. build the algorithm ..\n",
        "        if run_config.num_env_runners > 0:\n",
        "            ray.shutdown()\n",
        "            ray.init()\n",
        "\n",
        "            register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "        algo = run_config.build()\n",
        "\n",
        "    print(f\"Envs: {algo.config.num_envs_per_env_runner}, Runners: {algo.config.num_env_runners}\")\n",
        "\n",
        "    # 3. .. train it ..\n",
        "    @benchmark_exec_time\n",
        "    def _train_rllib(total_timesteps):\n",
        "        results = []\n",
        "        total_batches = ceil(total_timesteps / algo.config.train_batch_size)\n",
        "        print(f\"total_batches: {total_batches}\")\n",
        "        print(f\"total_timesteps: {total_timesteps}\")\n",
        "        for _ in range(total_batches):\n",
        "            result = algo.train()\n",
        "            results.append(result)\n",
        "\n",
        "        print_result(result)\n",
        "        return results\n",
        "\n",
        "    results, exec_time_sec = _train_rllib(algo.config['train_params'])\n",
        "\n",
        "    # Log train duration\n",
        "    duration_minutes = round(exec_time_sec / 60, 1)\n",
        "    wandb.run.summary[f\"train.duration_minutes/{model_name}\"] = duration_minutes\n",
        "    update_model_artifacts(log_results_folder=False)\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = (Path(TRAINED_MODEL_DIR) / model_name).resolve()\n",
        "    algo.save(ckpt_path)\n",
        "\n",
        "    return algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk0_pf5m9s_M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate RLlib model\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
        "from ray.rllib.models.torch.torch_distributions import TorchDiagGaussian\n",
        "\n",
        "import torch\n",
        "\n",
        "# Create the testing environment\n",
        "def evaluate_model(algo, model_name, mode, np_env_config, run_config, turbulence_thresh=None):\n",
        "    if turbulence_thresh is None:\n",
        "        turbulence_thresh = run_config.get('turbulence_thresh', 99)\n",
        "\n",
        "    turbulence_name = 'turbulence' if not run_config.get('if_vix', None) else '^VIX'\n",
        "\n",
        "    print(f\"Evaluating using `{turbulence_name}`: {turbulence_thresh}\")\n",
        "\n",
        "    env_config = {\n",
        "        # \"df\": val,\n",
        "        \"np_env_config\": np_env_config,\n",
        "        \"run_config\": run_config,\n",
        "        \"mode\": 'val',\n",
        "        'turbulence_threshold': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    eval_env = create_stock_trading_env(env_config)\n",
        "    state, info = eval_env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Perform inference using the trained RLlib agent\n",
        "    rl_module = algo.env_runner.module\n",
        "    while not done:\n",
        "        # Compute action using the RLlib trained agent\n",
        "        input_dict = {Columns.OBS: torch.Tensor(state).unsqueeze(0)}\n",
        "        rl_module_out = rl_module.forward_inference(input_dict)\n",
        "        logits = rl_module_out[Columns.ACTION_DIST_INPUTS]\n",
        "\n",
        "        # Take mean of multivariate Gaussian distribution\n",
        "        mean, log_std = logits.chunk(2, dim=-1)\n",
        "\n",
        "        # action_distribution = TorchDiagGaussian.from_logits(logits)\n",
        "        # action_distribution = action_distribution.to_deterministic()\n",
        "        # assert np.allclose(mean, action_distribution.loc)\n",
        "        # assert np.allclose(log_std.exp(), action_distribution._dist.scale)\n",
        "        # action = action_distribution.sample()\n",
        "\n",
        "        action = mean.detach().numpy().squeeze()\n",
        "\n",
        "        # Clip the action to ensure it's within the action space bounds\n",
        "        action = np.clip(action, eval_env.action_space.low, eval_env.action_space.high)\n",
        "\n",
        "        # Perform action\n",
        "        state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "    df_account_value = eval_env.save_asset_memory()\n",
        "    metrics = compute_metrics(df_account_value)\n",
        "\n",
        "    turb_name = 'ti' if not run_config.get('if_vix', None) else 'vix'\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        metric_name = f\"{mode}.{turb_name}_{turbulence_thresh}.{metric_name}/{model_name}\"\n",
        "        wandb.run.summary[metric_name] = metric_value\n",
        "\n",
        "    turbulence_series = pd.Series(\n",
        "        np_env_config['turbulence_array'][:len(df_account_value)],\n",
        "        index=df_account_value['date'],\n",
        "        name=turbulence_name\n",
        "    )\n",
        "\n",
        "    eval_result = {\n",
        "        'account_value': df_account_value.rename(columns={'account_value': model_name}),\n",
        "        'turbulence_series': turbulence_series,\n",
        "        'turbulence_thresh': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    return eval_result\n",
        "\n",
        "# val_result = evaluate_model(\n",
        "#     algo,\n",
        "#     model_name,\n",
        "#     run_config=run_config,\n",
        "#     np_env_config = val_np_env_config,\n",
        "#     mode='val',\n",
        "# )\n",
        "\n",
        "# test_result = evaluate_model(\n",
        "#     algo,\n",
        "#     model_name,\n",
        "#     run_config=run_config,\n",
        "#     np_env_config = test_np_env_config,\n",
        "#     mode='test',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbLgPDKNPQX3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title plot_results (w/separate turbulence plot)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_results(\n",
        "        account_value,\n",
        "        turbulence_series,\n",
        "        turbulence_thresh,\n",
        "    ):\n",
        "\n",
        "    assert turbulence_series.name in ['turbulence', '^VIX']\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "    # Main plot\n",
        "    method_styles = {\n",
        "        'A2C': {'color': '#8c564b', 'linestyle': '--'},\n",
        "        'DDPG': {'color': '#e377c2', 'linestyle': '-'},\n",
        "        'PPO': {'color': '#7f7f7f', 'linestyle': '-'},\n",
        "        'TD3': {'color': '#bcbd22', 'linestyle': '--'},\n",
        "        'SAC': {'color': '#17becf', 'linestyle': '-'},\n",
        "        'DJIA': {'color': '#000000', 'linestyle': '-'},\n",
        "    }\n",
        "\n",
        "    if 'DJIA' in account_value:\n",
        "        ax1.plot(account_value.index, account_value['DJIA'], label=\"Dow Jones Index\",\n",
        "                linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "    if 'date' in account_value.columns:\n",
        "        account_value.set_index('date', inplace=True)\n",
        "\n",
        "    account_value.rename(columns={col: col.upper() for col in account_value.columns}, inplace=True)\n",
        "\n",
        "    for model_name in account_value.columns:\n",
        "        style = method_styles[model_name]\n",
        "        ax1.plot(account_value.index, account_value[model_name], label=model_name, **style)\n",
        "\n",
        "    # Customize main plot\n",
        "    ax1.set_title(\"Performance Comparison of DRL Agents\", fontsize=20, fontweight='bold')\n",
        "    ax1.set_ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Turbulence plot\n",
        "    if turbulence_series.name == 'turbulence':\n",
        "        turbulence_label = \"Turbulence Index\"\n",
        "    else:\n",
        "        turbulence_label = \"VIX Coefficient\"\n",
        "    ax2.plot(turbulence_series.index, turbulence_series, label=\"Turbulence Index\", color='red', linestyle='--', linewidth=2)\n",
        "    ax2.axhline(y=turbulence_thresh, color='red', linestyle=':', label='Turbulence Threshold')\n",
        "    ax2.set_ylabel(turbulence_label, fontsize=16, fontweight='bold')\n",
        "    ax2.legend(loc='lower left')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "    max_turbulence = max(turbulence_series.max(), turbulence_thresh)\n",
        "    ax2.set_ylim(0, max_turbulence + 10)\n",
        "    ax2.set_yticks(list(ax2.get_yticks()) + [turbulence_thresh])\n",
        "\n",
        "    # Shared x-axis label\n",
        "    ax2.set_xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# fig = plot_results(**val_result)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02OzYWBGMVNE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title log_plot_as_artifact\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "def log_plot_as_artifact(fig, artifact_name_prefix, artifact_type=\"plot\"):\n",
        "    print('log_plot_as_artifact')\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure without clipping and log it as a W&B artifact.\n",
        "\n",
        "    Parameters:\n",
        "        fig (matplotlib.figure.Figure): The Matplotlib figure to save and log.\n",
        "        artifact_name (str): The name of the W&B artifact.\n",
        "        artifact_type (str): The type of the artifact (default is \"plot\").\n",
        "        filename (str): The filename to save the plot as (default is \"plot.png\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get full artifact name\n",
        "        artifact_name = f'{artifact_name_prefix}-{wandb.run.id}'\n",
        "        filename = artifact_name + '.png'\n",
        "\n",
        "        # Save the figure with tight layout and proper padding\n",
        "        fig.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "        plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "        # Create and log the W&B artifact\n",
        "        artifact = wandb.Artifact(artifact_name, type=artifact_type)\n",
        "        artifact.add_file(filename, skip_cache=True, overwrite=True)\n",
        "        wandb.log_artifact(artifact)\n",
        "    finally:\n",
        "        # Ensure the file is deleted after use\n",
        "        if os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "\n",
        "# log_plot_as_artifact(fig, \"performance_comparison_DRL_agents\", artifact_type=\"plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run sweep"
      ],
      "metadata": {
        "id": "fXntnmmhuSsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSosu3u-YIIA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sweep Runner\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "def set_run_name(prefix, n=5):\n",
        "    run_name = f\"{prefix} | {wandb.run.id}\"\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "class SweepRunner:\n",
        "    def __init__(self, sweep_id):\n",
        "        self.sweep_id = sweep_id\n",
        "        self.pretrained_models = {}\n",
        "\n",
        "    def main(self, run_config=None):\n",
        "        run_timer_start = perf_counter()\n",
        "        with wandb.init(config=run_config):\n",
        "            run_config = wandb.config\n",
        "\n",
        "            if run_config['dataset_type'] == 'yearly_train_test':\n",
        "                raise NotImplementedError\n",
        "            elif run_config['dataset_type'] == 'quarterly_train_val_test':\n",
        "                train_np_env_config, val_np_env_config, test_np_env_config = build_quarterly_train_val_test(run_config)\n",
        "                set_run_name(run_config['dataset_name'])\n",
        "\n",
        "                if self.pretrained_models:\n",
        "                    run_config.update({'finetune': True})\n",
        "                else:\n",
        "                    run_config.update({'finetune': False})\n",
        "\n",
        "                pretrained_models = train_eval_rllib_models(\n",
        "                    run_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config,\n",
        "                    pretrained_models=self.pretrained_models\n",
        "                )\n",
        "\n",
        "                self.pretrained_models = pretrained_models\n",
        "\n",
        "            run_timer_end = perf_counter()\n",
        "            run_duration_minutes = round( (run_timer_end - run_timer_start) / 60, 1)\n",
        "            wandb.run.summary[f\"run.duration_minutes\"] = run_duration_minutes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN SWEEP\n",
        "def run_sweep(n_runs, sweep_config=None, sweep_id=None):\n",
        "    if sweep_id is None:\n",
        "        assert sweep_config is not None\n",
        "        sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "    else:\n",
        "        assert sweep_config is None\n",
        "\n",
        "    !rm -rf {TRAINED_MODEL_DIR}/*\n",
        "\n",
        "    sweep_runner = SweepRunner(sweep_id)\n",
        "    wandb.agent(sweep_id, sweep_runner.main, project=PROJECT, count=n_runs)\n",
        "\n",
        "run_sweep(\n",
        "    # sweep_id='50a5ela4',\n",
        "    sweep_config=sweep_config,\n",
        "\n",
        "    # n_runs=None\n",
        "    n_runs=1,\n",
        ")"
      ],
      "metadata": {
        "id": "_jqFSQXXM4Z0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SIU-vXqDRW3L",
        "oWn4ZCwkvtN3",
        "hWUph5lzrTUS",
        "KemUy0OKg-wX",
        "_BZTsxX0tkDZ",
        "znlXqpfQZkzU",
        "oqc-9r54tuXG"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOS86hazBMM9l3BZJbADcnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}