{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_sweep_rllib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-J5mD_PTar9"
      },
      "source": [
        "#Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRswNd6LH7lw",
        "outputId": "eb55e729-f2c5-4ce6-823d-fdb55e6d93a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3==2.6.0a0 in /usr/local/lib/python3.11/dist-packages (2.6.0a0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3==2.6.0a0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3==2.6.0a0) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3==2.6.0a0) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3==2.6.0a0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.6.0a0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3==2.6.0a0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.6.0a0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3==2.6.0a0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3=='2.6.0a0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RfYDJoTXo6-J"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zw9eNgAjLqsV"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# git clone https://github.com/tony-pitchblack/FinRL.git\n",
        "# cd ./FinRL\n",
        "# git checkout benchmarking\n",
        "# pip install -r FinRL/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6nvjUhJQPQw",
        "outputId": "774658f6-22d4-400f-b1d0-6194a6354b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tony-pitchblack/FinRL.git@benchmarking\n",
            "  Cloning https://github.com/tony-pitchblack/FinRL.git (to revision benchmarking) to /tmp/pip-req-build-2z4p4kj9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tony-pitchblack/FinRL.git /tmp/pip-req-build-2z4p4kj9\n",
            "  Running command git checkout -b benchmarking --track origin/benchmarking\n",
            "  Switched to a new branch 'benchmarking'\n",
            "  Branch 'benchmarking' set up to track remote branch 'benchmarking' from 'origin'.\n",
            "  Resolved https://github.com/tony-pitchblack/FinRL.git to commit f7dab0a4cda4ae68bfe3b45c3be038bf01327878\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 88.5 ms, sys: 14 ms, total: 103 ms\n",
            "Wall time: 12.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install git+https://github.com/tony-pitchblack/FinRL.git@benchmarking --no-deps \\\n",
        "    # --force-reinstall --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S3eA6pMHTxn",
        "outputId": "1e0d9434-6cad-47c8-e780-beb620c668fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-13 20:01:00--  https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 764 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     764  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-13 20:01:00 (39.9 MB/s) - ‘requirements.txt’ saved [764/764]\n",
            "\n",
            "Requirement already satisfied: alpaca_trade_api>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: alpaca-py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.38.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.28.1)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: ccxt>=1.66.32 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (4.4.58)\n",
            "Requirement already satisfied: exchange_calendars==3.6.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.6.3)\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.13.0)\n",
            "Requirement already satisfied: jqdatasdk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.9.7)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (4.4.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (4.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (8.3.4)\n",
            "Requirement already satisfied: recommonmark in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (0.7.1)\n",
            "Requirement already satisfied: dm_tree in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.1.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=65.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (75.1.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (8.1.3)\n",
            "Requirement already satisfied: sphinx_rtd_theme in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.0.2)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (2.0.37)\n",
            "Requirement already satisfied: stockstats>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (0.6.4)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (4.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (2.6.2.2)\n",
            "Requirement already satisfied: wheel>=0.33.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (0.45.1)\n",
            "Requirement already satisfied: wrds in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (3.3.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.2.52)\n",
            "Requirement already satisfied: ray[default] in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (2.42.1)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (2.6.0a0)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (2025.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.11/dist-packages (from exchange_calendars==3.6.3->-r requirements.txt (line 7)) (0.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata==4.13.0->-r requirements.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.10.11)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/dist-packages (from alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py->-r requirements.txt (line 2)) (2.10.6)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from alpaca-py->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (43.0.3)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt>=1.66.32->-r requirements.txt (line 5)) (1.18.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 9)) (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk->-r requirements.txt (line 11)) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (3.2.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->-r requirements.txt (line 17)) (2025.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit->-r requirements.txt (line 20)) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit->-r requirements.txt (line 20)) (2.6.7)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from pre-commit->-r requirements.txt (line 20)) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit->-r requirements.txt (line 20)) (20.29.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 25)) (1.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.5.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (7.1.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]->-r requirements.txt (line 26)) (1.70.0)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (2024.10.0)\n",
            "Requirement already satisfied: ormsgpack==1.7.0 in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (1.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ray[rllib]->-r requirements.txt (line 27)) (1.13.1)\n",
            "Requirement already satisfied: commonmark>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from recommonmark->-r requirements.txt (line 29)) (0.9.1)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.11/dist-packages (from recommonmark->-r requirements.txt (line 29)) (0.21.2)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (25.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm_tree->-r requirements.txt (line 30)) (1.17.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.0->-r requirements.txt (line 33)) (3.5.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (3.1.5)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->-r requirements.txt (line 40)) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.11/dist-packages (from sphinx_rtd_theme->-r requirements.txt (line 41)) (4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy->-r requirements.txt (line 44)) (3.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.5.1+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 45)) (0.10.1)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/dist-packages (from wrds->-r requirements.txt (line 51)) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 54)) (1.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from aiodns>=1.1.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements.txt (line 54)) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 54)) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx->-r requirements.txt (line 40)) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py->-r requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca_trade_api>=2.1.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.0.11)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk->-r requirements.txt (line 11)) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]->-r requirements.txt (line 45)) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->-r requirements.txt (line 3)) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 3)) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 20)) (0.3.9)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl>=1.7.2->ccxt>=1.66.32->-r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]->-r requirements.txt (line 26)) (0.22.3)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]->-r requirements.txt (line 26)) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]->-r requirements.txt (line 26)) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]->-r requirements.txt (line 45)) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->-r requirements.txt (line 5)) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (2.27.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]->-r requirements.txt (line 45)) (0.1.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r requirements.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]->-r requirements.txt (line 26)) (0.6.1)\n",
            "CPU times: user 101 ms, sys: 14.5 ms, total: 115 ms\n",
            "Wall time: 9.81 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!wget https://raw.githubusercontent.com/tony-pitchblack/FinRL/benchmarking/requirements.txt -O requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU-vXqDRW3L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "US_vB7hNSdeu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HJsl_3tVre6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWn4ZCwkvtN3"
      },
      "source": [
        "# General funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "gbd4N4QLPXlL"
      },
      "outputs": [],
      "source": [
        "#@title YahooDownloader\n",
        "\n",
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
        "            )\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "\n",
        "        try:\n",
        "            # Convert wide to long format\n",
        "            # print(f\"DATA COLS: {data_df.columns}\")\n",
        "            data_df = data_df.sort_index(axis=1).set_index(['Date']).drop(columns=['tic']).stack(level='Ticker', future_stack=True)\n",
        "            data_df.reset_index(inplace=True)\n",
        "            data_df.columns.name = ''\n",
        "\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(columns={'Ticker': 'Tic', 'Adj Close': 'Adjcp'}, inplace=True)\n",
        "            data_df.rename(columns={col: col.lower() for col in data_df.columns}, inplace=True)\n",
        "\n",
        "            columns = [\n",
        "                \"date\",\n",
        "                \"tic\",\n",
        "                \"open\",\n",
        "                \"high\",\n",
        "                \"low\",\n",
        "                \"close\",\n",
        "                \"adjcp\",\n",
        "                \"volume\",\n",
        "            ]\n",
        "\n",
        "            data_df = data_df[columns]\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "YhIyXmfQ8EAS"
      },
      "outputs": [],
      "source": [
        "#@title fix_daily_index\n",
        "\n",
        "def make_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    # Get unique dates and create a mapping to daily indices\n",
        "    total_dates = data_df[date_column].unique()\n",
        "    date_to_index = {date: idx for idx, date in enumerate(sorted(total_dates))}\n",
        "    return data_df[date_column].map(date_to_index)\n",
        "\n",
        "def set_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    \"\"\"\n",
        "    Constructs a daily index from unique dates in the specified column.\n",
        "\n",
        "    Parameters:\n",
        "        data_df (pd.DataFrame): The input DataFrame.\n",
        "        date_column (str): The name of the column containing dates.\n",
        "        new_index_name (str): The name for the new index.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a daily index.\n",
        "    \"\"\"\n",
        "\n",
        "    # Map dates to daily indices and set as index\n",
        "    data_df[new_index_name] = make_daily_index(data_df, date_column='date', new_index_name='date_index')\n",
        "\n",
        "    data_df.set_index(new_index_name, inplace=True)\n",
        "    data_df.index.name = ''  # Remove the index name for simplicity\n",
        "\n",
        "    return data_df\n",
        "\n",
        "def fix_daily_index(df):\n",
        "    if df.index.name == 'date':\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    daily_index = make_daily_index(df, date_column='date', new_index_name='date_index')\n",
        "    if (df.index.values != daily_index.values).any():\n",
        "\n",
        "        df.index = daily_index\n",
        "        df.index.name = ''\n",
        "\n",
        "    return df\n",
        "\n",
        "# trade = fix_daily_index(trade)\n",
        "# trade.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "GQ6BIJxbwuVh"
      },
      "outputs": [],
      "source": [
        "#@title get dataset name\n",
        "\n",
        "def get_quarterly_dataset_name(prefix, train_start_date, val_start_date, test_start_date):\n",
        "    get_quarter = lambda date: f'Q{(date.month - 1) // 3 + 1}'\n",
        "\n",
        "    val_quarter = get_quarter(val_start_date)\n",
        "    test_quarter = get_quarter(test_start_date)\n",
        "\n",
        "    # Extract year and month\n",
        "    train_start = f\"{train_start_date.year}-{train_start_date.month:02}\"\n",
        "    val_start = f\"{val_start_date.year}\"\n",
        "    test_start = f\"{test_start_date.year}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start} | {val_start} {val_quarter} | {test_start} {test_quarter}\"\n",
        "\n",
        "    return dataset_name\n",
        "\n",
        "def get_yearly_dataset_name(prefix, train_start, test_start, test_end):\n",
        "    # Extract year and month\n",
        "    train_start_str = f\"{train_start.year}-{train_start.month:02}\"\n",
        "    test_start_str = f\"{test_start.year}-{test_start.month:02}\"\n",
        "    test_end_str = f\"{test_end.year}-{test_end.month:02}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start_str} | {test_start_str} | {test_end_str}\"\n",
        "    return dataset_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "TOfz3JlX-oG5"
      },
      "outputs": [],
      "source": [
        "#@title add_dataset\n",
        "\n",
        "def add_dataset(stock_index_name, train_df, test_df):\n",
        "    if 'datasets' not in globals():\n",
        "        global datasets\n",
        "        datasets = {}\n",
        "\n",
        "    # Ensure datetime format\n",
        "    if 'date' in train_df.columns:\n",
        "        train_df.set_index('date', inplace=True)\n",
        "    train_df.index = pd.to_datetime(train_df.index)\n",
        "\n",
        "    if 'date' in test_df.columns:\n",
        "        test_df.set_index('date', inplace=True)\n",
        "    test_df.index = pd.to_datetime(test_df.index)\n",
        "\n",
        "    train_start_date = train_df.index[0]\n",
        "    test_start_date = test_df.index[0]\n",
        "    test_end_date = test_df.index[-1]\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        stock_index_name,\n",
        "        train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    train_df.reset_index(inplace=True)\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    ticker_list = train_df.tic.unique().tolist()\n",
        "\n",
        "    datasets[dataset_name] = {\n",
        "        'train': train_df,\n",
        "        'test': test_df,\n",
        "        'metadata': dict(\n",
        "            stock_index_name = stock_index_name,\n",
        "            train_start_date = train_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "            num_tickers = len(ticker_list),\n",
        "            ticker_list = ticker_list,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUph5lzrTUS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHIfVohUTAsG"
      },
      "source": [
        "## DATA: DOW-30 (quarterly train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dTOtt7iQg7TU"
      },
      "outputs": [],
      "source": [
        "train_start_date = '2015-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2016-10-01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGMvKo0wxwYh",
        "outputId": "9dba0032-065f-4b34-8d55-cb8f3a345262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_start_date': Timestamp('2015-01-01 00:00:00'), 'val_start_date': Timestamp('2015-10-01 00:00:00'), 'test_start_date': Timestamp('2016-01-01 00:00:00'), 'test_end_date': Timestamp('2016-04-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2015-10-01 00:00:00'), 'val_start_date': Timestamp('2016-01-01 00:00:00'), 'test_start_date': Timestamp('2016-04-01 00:00:00'), 'test_end_date': Timestamp('2016-07-01 00:00:00')}\n",
            "{'train_start_date': Timestamp('2016-01-01 00:00:00'), 'val_start_date': Timestamp('2016-04-01 00:00:00'), 'test_start_date': Timestamp('2016-07-01 00:00:00'), 'test_end_date': Timestamp('2016-10-01 00:00:00')}\n"
          ]
        }
      ],
      "source": [
        "#@title generate_quarterly_date_ranges\n",
        "\n",
        "def generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=False,\n",
        "    finetune_previous_val=False\n",
        "):\n",
        "    is_quarter_start = lambda date: date.month in [1, 4, 7, 10] and date.day == 1\n",
        "\n",
        "    min_test_start_date = pd.Timestamp(min_test_start_date)\n",
        "    train_start_date = pd.Timestamp(train_start_date)\n",
        "    max_test_end_date = pd.Timestamp(max_test_end_date)\n",
        "\n",
        "    assert is_quarter_start(train_start_date), f\"train_start_date {train_start_date} is not a quarter start date.\"\n",
        "    assert is_quarter_start(min_test_start_date), f\"min_test_start_date {min_test_start_date} is not a quarter start date.\"\n",
        "\n",
        "    test_start_date = min_test_start_date\n",
        "    date_ranges = []\n",
        "    full_train_start_date = train_start_date\n",
        "\n",
        "    while True:\n",
        "        val_start_date = test_start_date - pd.DateOffset(months=3)\n",
        "        test_end_date = test_start_date + pd.DateOffset(months=3)\n",
        "\n",
        "        if test_end_date > max_test_end_date:\n",
        "            break\n",
        "\n",
        "        if len(date_ranges) == 0:\n",
        "            # The first date_range contains the full training period\n",
        "            train_start_date = full_train_start_date\n",
        "        elif finetune_previous_val:\n",
        "            # Use the previous validation range as the training range\n",
        "            train_start_date = date_ranges[-1]['val_start_date']\n",
        "\n",
        "        date_range = dict(\n",
        "            train_start_date=train_start_date,\n",
        "            val_start_date=val_start_date,\n",
        "            test_start_date=test_start_date,\n",
        "            test_end_date=test_end_date,\n",
        "        )\n",
        "\n",
        "        if return_strings:\n",
        "            date_range = {k: str(v) for k, v in date_range.items()}\n",
        "\n",
        "        date_ranges.append(date_range)\n",
        "\n",
        "        test_start_date = test_end_date\n",
        "\n",
        "    return date_ranges\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "# print(*date_ranges[:2], sep='\\n')\n",
        "print(*date_ranges, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "S8GdHjZWTcHu"
      },
      "outputs": [],
      "source": [
        "#@title split_data\n",
        "\n",
        "def split_data(data_df, date_range):\n",
        "    def subset_date_range(df, start_date, end_date):\n",
        "        df = df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "        df = fix_daily_index(df)\n",
        "        return df\n",
        "\n",
        "    return {\n",
        "        'train': subset_date_range(data_df, date_range['train_start_date'], date_range['val_start_date']),\n",
        "        'val': subset_date_range(data_df, date_range['val_start_date'], date_range['test_start_date']),\n",
        "        'test': subset_date_range(data_df, date_range['test_start_date'], date_range['test_end_date']),\n",
        "    }\n",
        "\n",
        "# data_splits = split_data(preproc_df, date_ranges[0])\n",
        "# data_splits['train'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "E34A0Yyhy4Sy"
      },
      "outputs": [],
      "source": [
        "#@title prepare_data (for np env)\n",
        "import hashlib\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "import os\n",
        "\n",
        "CACHE_DIR = './cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def stable_hash(data):\n",
        "    return hashlib.sha256(str(data).encode()).hexdigest()\n",
        "\n",
        "def get_env_config(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    if_train,\n",
        "    ticker_list=DOW_30_TICKER,\n",
        "    technical_indicator_list=INDICATORS,\n",
        "    time_interval='1d',\n",
        "    if_vix=True,\n",
        "    **kwargs\n",
        "):\n",
        "    print(f\"Loading {'train' if if_train else 'eval'} data from {start_date} to {end_date}.\")\n",
        "\n",
        "    data_hash = stable_hash(tuple(sorted(ticker_list) + sorted(technical_indicator_list)))\n",
        "    file_path = Path(CACHE_DIR) / f\"{start_date}_{end_date}_{time_interval}_{data_hash}.csv\"\n",
        "    dp = DataProcessor(data_source='yahoofinance', tech_indicator=technical_indicator_list, vix=if_vix, **kwargs)\n",
        "    if os.path.isfile(file_path):\n",
        "        print(f\"Using cached data: {file_path}\")\n",
        "        data = pd.read_csv(file_path, index_col=0)\n",
        "    else:\n",
        "        print(\"Creating new data.\")\n",
        "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "        data = dp.clean_data(data)\n",
        "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "        if if_vix:\n",
        "            data = dp.add_vix(data)\n",
        "        data.to_csv(file_path)\n",
        "\n",
        "    (\n",
        "        price_array,\n",
        "        tech_array,\n",
        "        turbulence_array,\n",
        "        timestamp_array,\n",
        "    ) = dp.df_to_array(\n",
        "        data,\n",
        "        if_vix,\n",
        "        return_timestamps=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"timestamp_array\": timestamp_array,\n",
        "        \"if_train\": if_train\n",
        "    }\n",
        "\n",
        "    return env_config\n",
        "\n",
        "# date_range=date_ranges[0]\n",
        "# train_env_config = get_env_config(\n",
        "#     start_date=date_range['val_start_date'],\n",
        "#     end_date=date_range['test_start_date'],\n",
        "#     if_train=True\n",
        "# )\n",
        "# val_env_config = get_env_config(\n",
        "#     start_date=date_range['test_start_date'],\n",
        "#     end_date=date_range['test_end_date'],\n",
        "#     if_train=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSbz9QQ7pS8"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemUy0OKg-wX"
      },
      "source": [
        "## Wandb artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "jF0Xbv9f631H"
      },
      "outputs": [],
      "source": [
        "#@title update_artifact\n",
        "\n",
        "def update_artifact(folder_path, name_prefix, type):\n",
        "    \"\"\"\n",
        "    Create or update a W&B artifact consisting of a folder.\n",
        "\n",
        "    Args:\n",
        "        run: The current W&B run.\n",
        "        folder_path (str): Path to the folder to upload.\n",
        "        artifact_name (str): Name of the artifact.\n",
        "        artifact_type (str): Type of the artifact.\n",
        "    \"\"\"\n",
        "    run = wandb.run\n",
        "    artifact_name = f'{name_prefix}-{wandb.run.id}'\n",
        "\n",
        "    # Create a new artifact\n",
        "    artifact = wandb.Artifact(name=artifact_name, type=type)\n",
        "\n",
        "    # Add the folder to the artifact\n",
        "    artifact.add_dir(folder_path)\n",
        "\n",
        "    # Log the artifact to W&B\n",
        "    run.log_artifact(artifact)\n",
        "    print(f\"Artifact '{artifact_name}' has been updated and uploaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "bCmdRFmDh-CI"
      },
      "outputs": [],
      "source": [
        "#@title update_model_artifacts\n",
        "\n",
        "def update_model_artifacts(log_results_folder=True):\n",
        "    if log_results_folder:\n",
        "        update_artifact(\n",
        "            folder_path = RESULTS_DIR,\n",
        "            name_prefix = 'results',\n",
        "            type = 'results'\n",
        "        )\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = TRAINED_MODEL_DIR,\n",
        "        name_prefix = 'trained_models',\n",
        "        type = 'trained_models'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "Prm8SfPo7CJY"
      },
      "outputs": [],
      "source": [
        "#@title update_dataset_artifact\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def update_dataset_artifact(config, train_df, val_df=None, test_df=None):\n",
        "    DATASET_DIR = Path('./dataset')\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(DATASET_DIR / 'train_data.csv', index=False)\n",
        "\n",
        "    if test_df is not None:\n",
        "        test_df.to_csv(DATASET_DIR / 'test_data.csv', index=False)\n",
        "\n",
        "    if val_df is not None:\n",
        "        val_df.to_csv(DATASET_DIR / 'val_data.csv', index=False)\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = DATASET_DIR,\n",
        "        name_prefix = 'dataset',\n",
        "        type = 'dataset'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "x66rQn0TGEkM"
      },
      "outputs": [],
      "source": [
        "#@title update_env_state_artifact\n",
        "\n",
        "def update_env_state_artifact(val_env_end_state):\n",
        "    file_path = 'val_env_end_state.csv'\n",
        "\n",
        "    df_last_state = pd.DataFrame({\"last_state\": val_env_end_state})\n",
        "    df_last_state.to_csv(\n",
        "        file_path, index=False\n",
        "    )\n",
        "\n",
        "    artifact = wandb.Artifact(name=f'val_env_end_state-{wandb.run.id}', type='env_state')\n",
        "    artifact.add_file(file_path)\n",
        "    wandb.run.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BZTsxX0tkDZ"
      },
      "source": [
        "## Build & helper funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "SmzfG0qaVP93"
      },
      "outputs": [],
      "source": [
        "#@title build_yearly_train_test\n",
        "def build_yearly_train_test(config):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        config['train_years_count'],\n",
        "        config['test_years_count'],\n",
        "        config['test_start_year']\n",
        "    )\n",
        "\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    train_df = set_daily_index(train_df)\n",
        "    test_df = set_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        train_start_date=train_start_date,\n",
        "        test_start_date=test_start_date,\n",
        "        test_end_date=test_end_date,\n",
        "        dataset_name=dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N2JCHB8ibDce"
      },
      "outputs": [],
      "source": [
        "#@title build_quarterly_train_val_test\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "\n",
        "def build_quarterly_train_val_test(config):\n",
        "    date_range = {key: pd.Timestamp(date) for key, date in config['date_range'].items()}\n",
        "\n",
        "    train_start_date = date_range['train_start_date']\n",
        "    val_start_date = date_range['val_start_date']\n",
        "    test_start_date = date_range['test_start_date']\n",
        "    test_end_date = date_range['test_end_date']\n",
        "\n",
        "    train_env_config = get_env_config(\n",
        "        start_date=date_range['train_start_date'],\n",
        "        end_date=date_range['val_start_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    val_env_config = get_env_config(\n",
        "        start_date=date_range['val_start_date'],\n",
        "        end_date=date_range['test_start_date'],\n",
        "        if_train=True\n",
        "    )\n",
        "\n",
        "    test_env_config = get_env_config(\n",
        "        start_date=date_range['test_start_date'],\n",
        "        end_date=date_range['test_end_date'],\n",
        "        if_train=False\n",
        "    )\n",
        "\n",
        "    dataset_name = get_quarterly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, val_start_date, test_start_date\n",
        "    )\n",
        "\n",
        "    config.update({\n",
        "        \"dataset_name\": dataset_name,\n",
        "        \"train.num_datapoints\": len(train_env_config['price_array']),\n",
        "        \"val.num_datapoints\": len(val_env_config['price_array']),\n",
        "        \"test.num_datapoints\": len(test_env_config['price_array']),\n",
        "    })\n",
        "\n",
        "    return train_env_config, val_env_config, test_env_config\n",
        "\n",
        "# train_env_config, val_env_config, test_env_config = build_quarterly_train_val_test(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "Mp0H06urpp53"
      },
      "outputs": [],
      "source": [
        "#@title Init StockTradingEnv (numpy)\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.config import INDICATORS\n",
        "# from finrl.config import CACHE_DIR\n",
        "\n",
        "def init_env(\n",
        "    np_env_config,\n",
        "\n",
        "    # run_config,\n",
        "    initial_amount,\n",
        "    cost_pct,\n",
        "\n",
        "    mode,\n",
        "    turbulence_threshold=99,\n",
        "):\n",
        "    assert mode in ['train', 'val', 'test']\n",
        "\n",
        "    print('Initializing env...', end=' ')\n",
        "    env = StockTradingEnv(\n",
        "        config=np_env_config,\n",
        "        initial_capital=initial_amount,\n",
        "        buy_cost_pct=cost_pct,\n",
        "        sell_cost_pct=cost_pct,\n",
        "        turbulence_thresh=turbulence_threshold\n",
        "    )\n",
        "    print('Done.')\n",
        "\n",
        "    return env\n",
        "\n",
        "# env = init_env(\n",
        "#     train_np_env_config,\n",
        "#     run_config,\n",
        "#     'train'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cellView": "form",
        "id": "t0-3fjeCG_GJ"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "S4pwu5RcQz_F"
      },
      "outputs": [],
      "source": [
        "#@title compute metrics\n",
        "import wandb\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(account_values: List[pd.DataFrame, pd.Series, np.array], use_round=True):\n",
        "    \"\"\"\n",
        "    If DataFrame then should contain two columns - 'date' and name of algo, e.g. 'a2c'.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(account_values, pd.DataFrame):\n",
        "        assert isinstance(account_values, pd.DataFrame)\n",
        "        if 'date' not in account_values.columns:\n",
        "            if account_values.index.name == 'date':\n",
        "                account_values.reset_index(inplace=True)\n",
        "            else:\n",
        "                raise ValueError(\"should contain 'date' column or index\")\n",
        "        account_values = account_values.dropna().set_index('date').iloc[:, 0]\n",
        "    elif isinstance(account_values, np.ndarray):\n",
        "        account_values = pd.Series(account_values)\n",
        "\n",
        "    sharpe = calculate_sharpe_ratio(account_values)\n",
        "    mdd = calculate_mdd(account_values)\n",
        "    cum_ret = (account_values.iloc[-1] - account_values.iloc[0]) / account_values.iloc[0] * 100\n",
        "    # num_days = (account_values.index.max() - account_values.index.min()).days\n",
        "    num_days = len(account_values)\n",
        "    ann_ret = ((1 + cum_ret / 100) ** (365 / num_days) - 1) * 100\n",
        "\n",
        "    metrics = {\n",
        "            f'sharpe_ratio': sharpe,\n",
        "            f'mdd': mdd,\n",
        "            f'ann_return': ann_ret,\n",
        "            f'cum_return': cum_ret,\n",
        "        }\n",
        "\n",
        "    if use_round:\n",
        "        metrics = {k: round(v, 2) for k, v in metrics.items()}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_env_metrics(env):\n",
        "    end_total_asset = env.state[0] + sum(\n",
        "        np.array(env.state[1 : (env.stock_dim + 1)])\n",
        "        * np.array(env.state[(env.stock_dim + 1) : (env.stock_dim * 2 + 1)])\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'begin_total_asset': env.asset_memory[0],\n",
        "        'end_total_asset': end_total_asset,\n",
        "        'total_cost': env.cost,\n",
        "        'total_trades': env.trades,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "id": "SG9DR-fz9e-s"
      },
      "outputs": [],
      "source": [
        "#@title log_metrics\n",
        "\n",
        "def log_metrics(metrics, model_name, split_label, step=None):\n",
        "    print(f'log_metrics for {model_name}')\n",
        "\n",
        "    rename_metrics = lambda model_name: {\n",
        "        f\"{key}/{model_name}\": value for key, value in metrics.items()\n",
        "    }\n",
        "\n",
        "    renamed_metrics = rename_metrics(model_name)\n",
        "    wandb.log({split_label: renamed_metrics}, step=step)\n",
        "    # wandb.run.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "id": "78Xhl6wmmY9m"
      },
      "outputs": [],
      "source": [
        "#@title update_best_model_metrics\n",
        "\n",
        "def update_best_model_metrics(metrics, model_name, split_label):\n",
        "    if 'sharpe_ratios' not in wandb.run.config:\n",
        "        wandb.run.config['sharpe_ratios'] = {}\n",
        "\n",
        "    if split_label not in wandb.run.config['sharpe_ratios']:\n",
        "        wandb.run.config['sharpe_ratios'][split_label] = {}\n",
        "\n",
        "    sharpe_ratios = wandb.run.config['sharpe_ratios'][split_label]\n",
        "\n",
        "    print(f\"DEBUG ({split_label}): run.id = {wandb.run.id}\")\n",
        "    print(f\"DEBUG ({split_label}): sharpe_ratios = {sharpe_ratios}\")\n",
        "    print(f\"DEBUG ({split_label}): updating best model based on sharpe_ratios: {sharpe_ratios}\")\n",
        "    if len(sharpe_ratios) > 0:\n",
        "        best_model_name = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "        if metrics['sharpe_ratio'] > sharpe_ratios[best_model_name]:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" > {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                f\". New best model: {model_name}.\"\n",
        "            )\n",
        "            log_metrics(metrics, 'best_model', split_label)\n",
        "            wandb.log({split_label: {'best_model_name': model_name}})\n",
        "        else:\n",
        "            print(\n",
        "                f\"DEBUG ({split_label}): {round(metrics['sharpe_ratio'], 2)} ({model_name})\"\n",
        "                f\" <= {round(sharpe_ratios[best_model_name], 2)} ({best_model_name})\"\n",
        "                \". Not updating best model.\"\n",
        "            )\n",
        "    else:\n",
        "        print(f\"DEBUG ({split_label}): no models logged yet, new best model is current one: {model_name}\")\n",
        "        print(f\"DEBUG ({split_label}): wandb.run.config['sharpe_ratios'] = {wandb.run.config['sharpe_ratios']}\")\n",
        "        log_metrics(metrics, 'best_model', split_label)\n",
        "\n",
        "    wandb.run.config['sharpe_ratios'][split_label][model_name] = metrics['sharpe_ratio']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlXqpfQZkzU"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "kF6LboMHB2xZ"
      },
      "outputs": [],
      "source": [
        "#@title init config\n",
        "parameters_dict = {}\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'test.sharpe_ratio/best_model'\n",
        "    },\n",
        "    'parameters': parameters_dict\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "id": "7RzkkJNiSkP7"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: create dataset - yearly_train_test\n",
        "\n",
        "min_test_start_year = 2020\n",
        "max_test_start_year = 2025\n",
        "\n",
        "########################################################\n",
        "\n",
        "yearly_dataset_params = dict(\n",
        "    # dataset_period = {'value': 'year'},\n",
        "    # dataset_splits = {'parameters': {\n",
        "    #     'train': {'value': True },\n",
        "    #     'val': {'value': False },\n",
        "    #     'test': {'value': True },\n",
        "    # }},\n",
        "\n",
        "    dataset_type = {'value':'yearly_train_test'},\n",
        "\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_years_count = {'value': 10},\n",
        "    test_years_count = {'value': 1},\n",
        "    test_start_year = {\n",
        "        'values': list(range(min_test_start_year, max_test_start_year))\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "l-hEkHxAfYVv"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: create dataset - quarterly_train_test\n",
        "\n",
        "# Full date range (18 backtests)\n",
        "train_start_date = '2009-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2020-08-05'\n",
        "\n",
        "# NUM_DATE_RANGES = None\n",
        "NUM_DATE_RANGES = 2\n",
        "\n",
        "# NUM_RUNS_PER_DATERANGE = 1\n",
        "NUM_RUNS_PER_DATERANGE = 1\n",
        "\n",
        "#################################################################\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date,\n",
        "    min_test_start_date,\n",
        "    max_test_end_date,\n",
        "    return_strings=True,\n",
        "    finetune_previous_val=True\n",
        ")\n",
        "\n",
        "truncated_date_ranges = date_ranges[:NUM_DATE_RANGES]\n",
        "copied_or_truncated_date_ranges = [\n",
        "    date_range\n",
        "    for date_range in truncated_date_ranges\n",
        "    for _ in range(NUM_RUNS_PER_DATERANGE)\n",
        "]\n",
        "\n",
        "quarterly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'quarterly_train_val_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "    train_start_date = {'value': train_start_date},\n",
        "    min_test_start_date = {'value': min_test_start_date},\n",
        "    max_test_end_date = {'value': max_test_end_date},\n",
        "    date_range = {\n",
        "        # 'values': copied_or_truncated_date_ranges\n",
        "        'values': truncated_date_ranges\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "GJDvneoY00zo"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: choose dataset\n",
        "parameters_dict.update(\n",
        "    # yearly_dataset_params,\n",
        "    quarterly_dataset_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "id": "EipLEV8h4L5Q"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: number of seeds\n",
        "\n",
        "NUM_SEEDS = 2\n",
        "\n",
        "parameters_dict.update({\n",
        "    'seed': {'values': (np.random.randn(NUM_SEEDS) * 1e8).astype(int).tolist()}\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "cellView": "form",
        "id": "ILF379nrW4YK"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: env params\n",
        "parameters_dict.update(dict(\n",
        "    cost_pct = {'value': 1e-3},\n",
        "    initial_amount = {'value': 50_000},\n",
        "    turbulence_threshold = {\n",
        "        'value': 99,\n",
        "        # 'values': [30, 40, 50, 60, 70]\n",
        "    },\n",
        "    eval_turbulence_thresh = {'value': 25},\n",
        "    if_vix = {'value': True}\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PzWRRS6Prorh"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: models_used\n",
        "\n",
        "MODELS_USED = [\n",
        "    'ppo'\n",
        "]\n",
        "\n",
        "parameters_dict.update({\n",
        "    'models_used': {'value': MODELS_USED}\n",
        "})\n",
        "\n",
        "# TODO: remove legacy config used for backward compat\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': 'ppo' in MODELS_USED},\n",
        "    'if_using_ddpg': {'value': 'ddpg' in MODELS_USED},\n",
        "    'if_using_ppo': {'value': 'ppo' in MODELS_USED},\n",
        "    'if_using_td3': {'value': 'td3' in MODELS_USED},\n",
        "    'if_using_sac': {'value': 'sac' in MODELS_USED}\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "cellView": "form",
        "id": "XaCu08TIKegP"
      },
      "outputs": [],
      "source": [
        "#@title CONFIG: model and training params\n",
        "\n",
        "training_params = {\n",
        "    \"parameters\": {\n",
        "        \"ppo\": {\n",
        "            \"parameters\": dict(\n",
        "                steps={\"value\": 2_048},\n",
        "                # steps={\"values\": [2_048, 4_096]}, # gridsearch steps demo\n",
        "                # steps={\"values\": [2**i for i in range(11, 18)]}, # [2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
        "\n",
        "                train_batch_size={\"value\": 2048},\n",
        "                num_epochs={\"value\": 10},\n",
        "                minibatch_size={\"value\": 128},\n",
        "                lr={\"value\": 5e-5},\n",
        "                gamma={\"value\": 0.99},\n",
        "            )\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "env_runners_params = {\n",
        "    'parameters': dict(\n",
        "        num_envs_per_env_runner = {'value': 1},\n",
        "        num_env_runners = {'value': 0},\n",
        "    )\n",
        "}\n",
        "\n",
        "parameters_dict.update({'env_runners_params': env_runners_params})\n",
        "parameters_dict.update({'training_params': training_params})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqc-9r54tuXG"
      },
      "source": [
        "# Train & eval funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kRSGp8MduO9_"
      },
      "outputs": [],
      "source": [
        "#@title MetricsLoggerCallback (class)\n",
        "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
        "from typing import Optional, Sequence\n",
        "import gymnasium as gym\n",
        "from gymnasium.vector import AsyncVectorEnv\n",
        "\n",
        "class MetricsLoggerCallback(DefaultCallbacks):\n",
        "    def __init__(self, model_name, ema_coeff=0.2, ma_window=20, log_to_wandb=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.ema_coeff = ema_coeff\n",
        "        self.ma_window = ma_window\n",
        "        self.metric_names = set()\n",
        "        self.log_to_wandb = log_to_wandb\n",
        "\n",
        "    def unwrap_env(self, env):\n",
        "        env = env.unwrapped\n",
        "        # print(type(env))\n",
        "        # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.vector.sync_vector_env.SyncVectorEnv'>\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            return env\n",
        "        else:\n",
        "            env = env.envs[0]\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.OrderEnforcing'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'gymnasium.wrappers.common.PassiveEnvChecker'>\n",
        "\n",
        "            env = env.env\n",
        "            # print(type(env))\n",
        "            # (SingleAgentEnvRunner pid=127688) <class 'finrl.meta.env_stock_trading.env_stocktrading.StockTradingEnv'>\n",
        "        return env\n",
        "\n",
        "    def on_episode_step(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        env = self.unwrap_env(env)\n",
        "\n",
        "        if isinstance(env, AsyncVectorEnv):\n",
        "            asset_values = env.get_attr('asset_memory')\n",
        "            asset_values = pd.concat([pd.Series(av) for av in asset_values], axis=1).mean(axis=1)\n",
        "\n",
        "            # TODO: save_asset_memory\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            # asset_values = env.asset_memory\n",
        "            asset_values = env.save_asset_memory()\n",
        "\n",
        "        metrics = compute_metrics(asset_values)\n",
        "\n",
        "        # mode = env.mode\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            # metric_name = f\"{mode}/{metric_name}\" if mode != \"\" else metric_name\n",
        "            episode.add_temporary_timestep_data(metric_name, metric_value)\n",
        "            self.metric_names.update([metric_name])\n",
        "\n",
        "    def on_episode_end(\n",
        "            self,\n",
        "            *,\n",
        "            episode,\n",
        "            env_runner,\n",
        "            metrics_logger,\n",
        "            env,\n",
        "            env_index,\n",
        "            rl_module,\n",
        "            **kwargs,\n",
        "        ) -> None:\n",
        "\n",
        "        for metric_name in self.metric_names:\n",
        "            metric_values = episode.get_temporary_timestep_data(metric_name)\n",
        "            metric_value = np.nanmean(np.array(metric_values))\n",
        "\n",
        "            # metrics_logger.log_value(\n",
        "            #     metric_name,\n",
        "            #     metric_value,\n",
        "            #     reduce='mean',\n",
        "            # )\n",
        "\n",
        "            # Log EMA metrics locally\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_EMA_{self.ema_coeff}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                ema_coeff=self.ema_coeff\n",
        "            )\n",
        "\n",
        "            # Log MA metrics locally\n",
        "            metrics_logger.log_value(\n",
        "                f\"{metric_name}_ma_{self.ma_window}\",\n",
        "                metric_value,\n",
        "                reduce='mean',\n",
        "                window=self.ma_window\n",
        "            )\n",
        "\n",
        "            # Log unsmoothed metrics to wandb\n",
        "            if self.log_to_wandb:\n",
        "                mode = 'val' if env_runner.config.in_evaluation else 'train'\n",
        "                wandb.log({\n",
        "                    f\"{mode}.{metric_name}/{self.model_name}\": metric_value,\n",
        "                }) # TODO: log on every episode step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gg8DIuPJ9FwH"
      },
      "outputs": [],
      "source": [
        "#@title print_result\n",
        "\n",
        "RESULT_KEYS_TO_INCLUDE = [\n",
        "    'sharpe_ratio_MA',\n",
        "    'ann_return_MA',\n",
        "    'mdd_MA',\n",
        "\n",
        "    'sharpe_ratio_EMA',\n",
        "    'ann_return_EMA',\n",
        "    'mdd_EMA',\n",
        "]\n",
        "\n",
        "def print_result(result):\n",
        "    print()\n",
        "    for key in result['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"train/{key}: {round(result['env_runners'][key], 2)}\")\n",
        "                break\n",
        "\n",
        "    for key in result['evaluation']['env_runners'].keys():\n",
        "        for include_key in RESULT_KEYS_TO_INCLUDE:\n",
        "            if key.startswith(include_key):\n",
        "                print(f\"val/{key}: {round(result['evaluation']['env_runners'][key], 2)}\")\n",
        "                break\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V0iyYcwTOZYP"
      },
      "outputs": [],
      "source": [
        "#@title benchmark_exec_time\n",
        "import pandas as pd\n",
        "from time import perf_counter\n",
        "from functools import wraps\n",
        "\n",
        "def benchmark_exec_time(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "\n",
        "        start = perf_counter()\n",
        "        output = func(*args, **kwargs)\n",
        "        end = perf_counter()\n",
        "\n",
        "        exec_time_sec = end - start\n",
        "\n",
        "        data = {\n",
        "            \"func_name\": func.__name__,\n",
        "            \"exec_time_sec\": exec_time_sec,\n",
        "        }\n",
        "        # print(f'\\nBenchmark results: {data}')\n",
        "        return output, exec_time_sec\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TLDeH1lN2XAS"
      },
      "outputs": [],
      "source": [
        "#@title Train_eval_models\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_models = {} # pretrained on previous train set (not validation set)\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            print(f\"Training {model_name.upper()} agent\")\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "            algo = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = pretrained_models.get(model_name, None)\n",
        "            )\n",
        "\n",
        "            print(f\"Evaluating {model_name.upper()} agent\")\n",
        "            val_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = val_np_env_config,\n",
        "                mode='val',\n",
        "            )\n",
        "            fig = plot_results(**val_result)\n",
        "            log_plot_as_artifact(fig, \"val_cumulative_return\", artifact_type=\"plot\")\n",
        "\n",
        "            test_result = evaluate_model(\n",
        "                algo,\n",
        "                model_name,\n",
        "                run_config=run_config,\n",
        "                np_env_config = test_np_env_config,\n",
        "                mode='test',\n",
        "            )\n",
        "            fig = plot_results(**test_result)\n",
        "            log_plot_as_artifact(fig, \"test_cumulative_return\", artifact_type=\"plot\")\n",
        "            pretrained_models[model_name] = algo\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")\n",
        "\n",
        "    return pretrained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qH-OEXfqEaFR"
      },
      "outputs": [],
      "source": [
        "#@title Train_eval_models (w/ threshold gridsearch)\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "AVAILABLE_MODELS_CONFIGS = {\n",
        "    'ppo': PPOConfig\n",
        "}\n",
        "\n",
        "def create_stock_trading_env(env_config):\n",
        "    return init_env(**env_config)\n",
        "\n",
        "def train_eval_rllib_models(\n",
        "        run_config,\n",
        "        train_np_env_config,\n",
        "        val_np_env_config,\n",
        "        test_np_env_config,\n",
        "        model_list = ['ppo'], # TODO: discard in favor of 'if_using_{model_name}'\n",
        "        pretrained_val_models = {},\n",
        "    ):\n",
        "\n",
        "    assert set(model_list).issubset(AVAILABLE_MODELS_CONFIGS)\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "    for model_name in model_list:\n",
        "        if run_config[f\"if_using_{model_name}\"]:\n",
        "            model_config = AVAILABLE_MODELS_CONFIGS[model_name]\n",
        "\n",
        "            # Skip training if a model pretrained on previous validation set is passed.\n",
        "            # The training should happen only for first run,\n",
        "            # further runs should reuse pretrained val models.\n",
        "            if model_name in pretrained_val_models:\n",
        "                algo_train = pretrained_val_models[model_name]\n",
        "            else:\n",
        "                algo_train = train_rllib_model(\n",
        "                    run_config,\n",
        "                    model_name,\n",
        "                    model_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    pretrained_algo = None # Train from scratch\n",
        "                )\n",
        "\n",
        "            wandb.run.summary['val.num_pretrain_iters'] = algo_train.training_iteration\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            # using model trained on current train set (previous val set)\n",
        "            val_best_th = evaluate_threshold_grid(\n",
        "                algo_train,\n",
        "                model_name,\n",
        "                run_config,\n",
        "                val_np_env_config,\n",
        "                split_label='val',\n",
        "            )\n",
        "\n",
        "            # Finetune on current validation set (previous test set)\n",
        "            algo_val = train_rllib_model(\n",
        "                run_config,\n",
        "                model_name,\n",
        "                model_config,\n",
        "                train_np_env_config,\n",
        "                val_np_env_config,\n",
        "                pretrained_algo = algo_train\n",
        "            )\n",
        "\n",
        "            # Evaluate on test set\n",
        "            # using model finetuned on current validation set (previous test set)\n",
        "            _ = evaluate_threshold_grid(\n",
        "                algo_val,\n",
        "                model_name,\n",
        "                run_config,\n",
        "                test_np_env_config,\n",
        "                split_label='test',\n",
        "                chosen_th=val_best_th # chosen before fine-tuning\n",
        "            )\n",
        "\n",
        "            pretrained_val_models[model_name] = algo_val\n",
        "        else:\n",
        "            print(f\"Skipping {model_name.upper()} agent\")\n",
        "\n",
        "    return pretrained_val_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "15pANCivoIru"
      },
      "outputs": [],
      "source": [
        "#@title Train RLlib model\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from ray.tune.registry import register_env\n",
        "from time import perf_counter\n",
        "from math import ceil\n",
        "import ray\n",
        "\n",
        "def train_rllib_model(\n",
        "    run_config,\n",
        "    algo_name,\n",
        "    algo_cls_config,\n",
        "    train_np_env_config,\n",
        "    val_np_env_config,\n",
        "    pretrained_algo=None\n",
        "):\n",
        "\n",
        "    num_envs_per_env_runner = run_config['env_runners_params']['num_envs_per_env_runner']\n",
        "    num_env_runners = run_config['env_runners_params']['num_env_runners']\n",
        "\n",
        "    if pretrained_algo:\n",
        "        print(f\"Finetuning {algo_name.upper()} agent.\")\n",
        "        algo = pretrained_algo\n",
        "        print(f'Using agent with {algo.training_iteration} pretrain iterations.')\n",
        "    else:\n",
        "        print(f\"Training {algo_name.upper()} agent from scratch.\")\n",
        "        algo_config = (\n",
        "            algo_cls_config()\n",
        "            .environment(\n",
        "                env=\"stock_trading_env\",\n",
        "                env_config={\n",
        "                    # \"df\": train,\n",
        "                    \"np_env_config\": train_np_env_config,\n",
        "\n",
        "                    # \"run_config\": run_config,\n",
        "                    \"initial_amount\": run_config['initial_amount'],\n",
        "                    \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                    \"mode\": 'train'\n",
        "                },\n",
        "            )\n",
        "            .env_runners(\n",
        "                num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "                num_env_runners=num_env_runners,\n",
        "                num_cpus_per_env_runner= (2/num_env_runners) if num_env_runners > 2 else None,\n",
        "\n",
        "                # gym_env_vectorize_mode=gym.envs.registration.VectorizeMode.ASYNC,\n",
        "            )\n",
        "            .training(\n",
        "                train_batch_size=2048,\n",
        "                num_epochs=10,\n",
        "                minibatch_size=128,\n",
        "            )\n",
        "            .evaluation(\n",
        "                # Set up the validation environment\n",
        "                evaluation_interval=1,  # Specify evaluation frequency (1=after each training step)\n",
        "                evaluation_config={\n",
        "                    \"env\": \"stock_trading_env\",\n",
        "                    \"env_config\": {\n",
        "                        # \"df\": val,\n",
        "                        \"np_env_config\": val_np_env_config,\n",
        "\n",
        "                        # \"run_config\": run_config,\n",
        "                        \"initial_amount\": run_config['initial_amount'],\n",
        "                        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "                        \"mode\": 'val'\n",
        "                    },\n",
        "                },\n",
        "            )\n",
        "            # .callbacks(MetricsLoggerCallback)\n",
        "            .callbacks(partial(MetricsLoggerCallback, model_name='ppo', log_to_wandb=True))\n",
        "            .resources(\n",
        "                num_gpus=1 if torch.cuda.is_available() else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 2. build the algorithm ..\n",
        "        if algo_config.num_env_runners > 0:\n",
        "            ray.shutdown()\n",
        "            ray.init()\n",
        "\n",
        "            register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "\n",
        "        algo = algo_config.build()\n",
        "        print(f'Created new {algo_name} agent.')\n",
        "\n",
        "    print(f\"Envs: {algo.config.num_envs_per_env_runner}, Runners: {algo.config.num_env_runners}\")\n",
        "\n",
        "    # 3. .. train it ..\n",
        "    @benchmark_exec_time\n",
        "    def _train_rllib(total_timesteps):\n",
        "        print('Started training.')\n",
        "        results = []\n",
        "        total_batches = ceil(total_timesteps / algo.config.train_batch_size)\n",
        "        print(f\"total_batches: {total_batches}\")\n",
        "        print(f\"total_timesteps: {total_timesteps}\")\n",
        "        for _ in range(total_batches):\n",
        "            result = algo.train()\n",
        "            results.append(result)\n",
        "\n",
        "        print_result(result)\n",
        "        print('Training complete.')\n",
        "        return results\n",
        "\n",
        "    results, exec_time_sec = _train_rllib(run_config['training_params'][algo_name]['steps'])\n",
        "    print(f\"TRAINING DURATION: {exec_time_sec}\")\n",
        "\n",
        "    # Log train duration\n",
        "    duration_minutes = round(exec_time_sec / 60, 1)\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "    wandb.run.summary[f\"train.duration_minutes/{algo_name}\"] = duration_minutes\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = (Path(TRAINED_MODEL_DIR) / algo_name).resolve()\n",
        "    algo.save(ckpt_path)\n",
        "    update_model_artifacts(log_results_folder=False)\n",
        "\n",
        "    return algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pk0_pf5m9s_M"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate RLlib model\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
        "from ray.rllib.models.torch.torch_distributions import TorchDiagGaussian\n",
        "from ray.rllib.core.rl_module.rl_module import RLModule\n",
        "import torch\n",
        "\n",
        "# Create the testing environment\n",
        "def evaluate_model(algo_or_rl_module, model_name, split_label, np_env_config, run_config, turbulence_thresh=None,\n",
        "                   log_to_wandb=False, return_metrics=False):\n",
        "\n",
        "    if turbulence_thresh is None:\n",
        "        turbulence_thresh = run_config.get('turbulence_thresh', 99)\n",
        "    turbulence_name = 'turbulence' if not run_config.get('if_vix', None) else '^VIX'\n",
        "    print(f\"Evaluating for `{split_label}` using `{turbulence_name}`: {turbulence_thresh}\")\n",
        "\n",
        "    env_config = {\n",
        "        # \"df\": val,\n",
        "        \"np_env_config\": np_env_config,\n",
        "\n",
        "        # \"run_config\": run_config,\n",
        "        \"initial_amount\": run_config['initial_amount'],\n",
        "        \"cost_pct\": run_config['cost_pct'],\n",
        "\n",
        "        \"mode\": split_label,\n",
        "        'turbulence_threshold': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    eval_env = create_stock_trading_env(env_config)\n",
        "    state, info = eval_env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Perform inference using the trained RLlib agent\n",
        "    if isinstance(algo_or_rl_module, Algorithm):\n",
        "        rl_module = algo_or_rl_module.env_runner.module\n",
        "    elif isinstance(algo_or_rl_module, RLModule):\n",
        "        rl_module = algo_or_rl_module\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    while not done:\n",
        "        # Compute action using the RLlib trained agent\n",
        "        input_dict = {Columns.OBS: torch.Tensor(state).unsqueeze(0)}\n",
        "        rl_module_out = rl_module.forward_inference(input_dict)\n",
        "        logits = rl_module_out[Columns.ACTION_DIST_INPUTS]\n",
        "\n",
        "        # Take mean of multivariate Gaussian distribution\n",
        "        mean, log_std = logits.chunk(2, dim=-1)\n",
        "\n",
        "        # action_distribution = TorchDiagGaussian.from_logits(logits)\n",
        "        # action_distribution = action_distribution.to_deterministic()\n",
        "        # assert np.allclose(mean, action_distribution.loc)\n",
        "        # assert np.allclose(log_std.exp(), action_distribution._dist.scale)\n",
        "        # action = action_distribution.sample()\n",
        "\n",
        "        action = mean.detach().numpy().squeeze()\n",
        "\n",
        "        # Clip the action to ensure it's within the action space bounds\n",
        "        action = np.clip(action, eval_env.action_space.low, eval_env.action_space.high)\n",
        "\n",
        "        # Perform action\n",
        "        state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "    df_account_value = eval_env.save_asset_memory()\n",
        "    metrics = compute_metrics(df_account_value)\n",
        "    print(metrics)\n",
        "\n",
        "    if log_to_wandb:\n",
        "        turbulence_log_name = 'ti' if not run_config.get('if_vix', None) else 'vix'\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            metric_name = f\"{split_label}.{turbulence_log_name}_{turbulence_thresh}.{metric_name}/{model_name}\"\n",
        "            wandb.run.summary[metric_name] = round(metric_value, 2)\n",
        "\n",
        "    turbulence_series = pd.Series(\n",
        "        np_env_config['turbulence_array'][:len(df_account_value)],\n",
        "        index=df_account_value['date'],\n",
        "        name=turbulence_name\n",
        "    )\n",
        "\n",
        "    eval_result = {\n",
        "        'account_value': df_account_value.rename(columns={'account_value': model_name}),\n",
        "        'turbulence_series': turbulence_series,\n",
        "        'turbulence_thresh': turbulence_thresh\n",
        "    }\n",
        "\n",
        "    if return_metrics:\n",
        "        return eval_result, metrics\n",
        "    else:\n",
        "        return eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oiBoqBiFVYiA"
      },
      "outputs": [],
      "source": [
        "#@title plot_results (enhanced)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "def plot_results(\n",
        "        account_value,\n",
        "        turbulence_series,\n",
        "        turbulence_thresh,\n",
        "        figsize='small',\n",
        "        split_label=None,\n",
        "        metrics=None,\n",
        "        ylim_bottom = None,\n",
        "        ylim_top = None\n",
        "    ):\n",
        "    assert split_label in ['val', 'test']\n",
        "    assert turbulence_series.name in ['turbulence', '^VIX']\n",
        "    assert figsize in ['small', 'medium']\n",
        "\n",
        "    figsizes = {\n",
        "        'medium': (14, 10),\n",
        "        'small': (8.3, 8)\n",
        "    }\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsizes[figsize], sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "    # Method styles\n",
        "    method_styles = {\n",
        "        'A2C': {'color': '#8c564b', 'linestyle': '--'},\n",
        "        'DDPG': {'color': '#e377c2', 'linestyle': '-'},\n",
        "        'PPO': {'color': '#7f7f7f', 'linestyle': '-'},\n",
        "        'TD3': {'color': '#bcbd22', 'linestyle': '--'},\n",
        "        'SAC': {'color': '#17becf', 'linestyle': '-'},\n",
        "        'DJIA': {'color': '#000000', 'linestyle': '-'},\n",
        "    }\n",
        "\n",
        "    # Plot DJIA if present\n",
        "    if 'DJIA' in account_value:\n",
        "        ax1.plot(account_value.index, account_value['DJIA'], label=\"Dow Jones Index\",\n",
        "                linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "    # Ensure date is index\n",
        "    if 'date' in account_value.columns:\n",
        "        account_value.set_index('date', inplace=True)\n",
        "\n",
        "    account_value.rename(columns={col: col.upper() for col in account_value.columns}, inplace=True)\n",
        "\n",
        "    # Plot account values\n",
        "    for model_name in account_value.columns:\n",
        "        style = method_styles.get(model_name, {'color': 'blue', 'linestyle': '-'})  # Default style fallback\n",
        "        ax1.plot(account_value.index, account_value[model_name], label=model_name, **style)\n",
        "\n",
        "    # Construct subtitle text\n",
        "    turbulence_label = \"Turbulence Index\" if turbulence_series.name == 'turbulence' else \"VIX Coefficient\"\n",
        "    split_label_name = ('validation' if split_label == 'val' else split_label).capitalize()\n",
        "    title = f\"{split_label_name} split | {turbulence_label} threshold: {turbulence_thresh}\"\n",
        "    fig.suptitle(title, fontsize=20, fontweight='bold')\n",
        "\n",
        "   # Define the mapping for metric names\n",
        "    full_names = {\n",
        "        'mdd': 'MDD',\n",
        "        'ann_return': 'Annualized Return',\n",
        "        'cum_return': 'Cumulative Return',\n",
        "        'sharpe_ratio': 'Sharpe Ratio'\n",
        "    }\n",
        "\n",
        "    # Add subtitle, properly positioned and centered\n",
        "    if metrics:\n",
        "        metric_text = \", \".join(\n",
        "            f\"{full_names.get(name, name.replace('_', ' ').capitalize())}: {value:.2f}\"\n",
        "            for name, value in metrics.items()\n",
        "        )\n",
        "        ax1.set_title(metric_text, fontsize=12, color='gray', ha='center')\n",
        "\n",
        "\n",
        "    # **Prettify y-axis numbers**\n",
        "    ax1.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f\"{x:,.0f}\"))\n",
        "\n",
        "    # **Horizontal Line at Initial Asset Value**\n",
        "    initial_asset_value = account_value.iloc[0].mean()  # Assuming initial value from mean of first row\n",
        "    ax1.axhline(y=initial_asset_value, color='gray', linestyle='-.', linewidth=1.5, label=\"Initial Asset Value\")\n",
        "\n",
        "    # **Main Plot Customization**\n",
        "    ax1.set_ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "    if ylim_bottom is not None:\n",
        "        ax1.set_ylim(bottom=ylim_bottom)\n",
        "    if ylim_top is not None:\n",
        "        ax1.set_ylim(top=ylim_top)\n",
        "\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # **Turbulence Plot**\n",
        "    ax2.plot(turbulence_series.index, turbulence_series, label=turbulence_label, color='red', linestyle='--', linewidth=2)\n",
        "    ax2.axhline(y=turbulence_thresh, color='red', linestyle=':', label=f'Threshold = {turbulence_thresh}')\n",
        "\n",
        "    ax2.set_ylabel(turbulence_label, fontsize=16, fontweight='bold')\n",
        "    ax2.legend(loc='upper left')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    max_turbulence = max(turbulence_series.max(), turbulence_thresh)\n",
        "    ax2.set_ylim(0, max_turbulence + 10)\n",
        "\n",
        "    # **Shared x-axis label**\n",
        "    ax2.set_xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "02OzYWBGMVNE"
      },
      "outputs": [],
      "source": [
        "#@title log_plot_as_artifact\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "def log_plot_as_artifact(fig, artifact_name_prefix, artifact_type=\"plot\"):\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure without clipping and log it as a W&B artifact.\n",
        "\n",
        "    Parameters:\n",
        "        fig (matplotlib.figure.Figure): The Matplotlib figure to save and log.\n",
        "        artifact_name (str): The name of the W&B artifact.\n",
        "        artifact_type (str): The type of the artifact (default is \"plot\").\n",
        "        filename (str): The filename to save the plot as (default is \"plot.png\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get full artifact name\n",
        "        artifact_name = f'{artifact_name_prefix}-{wandb.run.id}'\n",
        "        filename = artifact_name + '.png'\n",
        "\n",
        "        # Save the figure with tight layout and proper padding\n",
        "        fig.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "        plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "        # Create and log the W&B artifact\n",
        "        artifact = wandb.Artifact(artifact_name, type=artifact_type)\n",
        "        artifact.add_file(filename, skip_cache=True, overwrite=True)\n",
        "        wandb.log_artifact(artifact)\n",
        "    finally:\n",
        "        # Ensure the file is deleted after use\n",
        "        if os.path.exists(filename):\n",
        "            os.remove(filename)\n",
        "\n",
        "# log_plot_as_artifact(fig, \"performance_comparison_DRL_agents\", artifact_type=\"plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_dxl6IJ0FlMO"
      },
      "outputs": [],
      "source": [
        "#@title batch_log_plots_as_artifact\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def batch_log_plots_as_artifact(\n",
        "        figs, fig_names, artifact_name_prefix, artifact_type=\"plot\"\n",
        "    ):\n",
        "\n",
        "    \"\"\"\n",
        "    Save a list of Matplotlib figures to a folder, log the folder as a W&B artifact,\n",
        "    and delete the folder after logging.\n",
        "\n",
        "    Parameters:\n",
        "        figs (list): List of Matplotlib figure objects.\n",
        "        folder_name (str): Name of the folder to store plots.\n",
        "        artifact_name_prefix (str): Prefix for the artifact name.\n",
        "        artifact_type (str): The type of the artifact (default is \"plot\").\n",
        "    \"\"\"\n",
        "    assert wandb.run.id\n",
        "\n",
        "    # Ensure the folder exists\n",
        "    os.makedirs(artifact_name_prefix, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Save all figures in the folder\n",
        "        for i, (fig, fig_name) in enumerate(zip(figs, fig_names)):\n",
        "            filename = os.path.join(artifact_name_prefix, f\"{fig_name}.png\")\n",
        "            fig.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "            plt.close(fig)  # Close the figure to free up memory\n",
        "\n",
        "        # Create and log the W&B artifact\n",
        "        artifact_name = f\"{artifact_name_prefix}-{wandb.run.id}\"\n",
        "        artifact = wandb.Artifact(artifact_name, type=artifact_type)\n",
        "        artifact.add_dir(artifact_name_prefix, skip_cache=True)\n",
        "        wandb.log_artifact(artifact)\n",
        "    finally:\n",
        "        # Ensure the folder is deleted after use\n",
        "        if os.path.exists(artifact_name_prefix):\n",
        "            shutil.rmtree(artifact_name_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H-l9U3K9TnlB"
      },
      "outputs": [],
      "source": [
        "#@title log_eval_results\n",
        "\n",
        "def log_eval_results(\n",
        "        model_name,\n",
        "        metrics,\n",
        "        split_label,\n",
        "        turbulence_log_name,\n",
        "        turbulence_thresh=None,\n",
        "        postfix=None\n",
        "    ):\n",
        "\n",
        "    if turbulence_thresh is None:\n",
        "        assert postfix in ['best', 'chosen']\n",
        "\n",
        "    if postfix is None:\n",
        "        assert turbulence_thresh is not None\n",
        "        postfix = turbulence_thresh\n",
        "\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        formatted_name = f\"{split_label}.{turbulence_log_name}_{postfix}.{metric_name}/{model_name}\"\n",
        "        wandb.run.summary[formatted_name] = round(metric_value, 2)  # Use formatted_name instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lyYbDansvhI8"
      },
      "outputs": [],
      "source": [
        "#@title evaluate_threshold_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "def evaluate_threshold_grid(\n",
        "    algo_or_rl_module,\n",
        "    model_name,\n",
        "    run_config,\n",
        "    np_env_config,\n",
        "    num_grid_points=10,\n",
        "    split_label='val',\n",
        "    chosen_th=None,\n",
        "    plot_padding=500\n",
        "):\n",
        "\n",
        "    if run_config.get('if_vix', True):\n",
        "        turbulence_log_name = 'vix'\n",
        "    else:\n",
        "        turbulence_log_name = 'ti'\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Calculate threshold grid\n",
        "    turb_ary = np_env_config['turbulence_array']\n",
        "    threshold_grid = np.linspace(turb_ary.min(), turb_ary.max(), num_grid_points)\n",
        "    threshold_grid = np.ceil(threshold_grid).astype(int).tolist()\n",
        "    threshold_grid.append(max(threshold_grid) + 1)\n",
        "    if chosen_th is not None:\n",
        "        threshold_grid.append(chosen_th)\n",
        "\n",
        "    th_metrics = []  # List to store metrics for all thresholds\n",
        "    th_results = {}  # Dictionary to store results per threshold\n",
        "\n",
        "    for th in threshold_grid:\n",
        "        result, metrics = evaluate_model(\n",
        "            turbulence_thresh=th,\n",
        "            model_name=model_name,\n",
        "            algo_or_rl_module=algo_or_rl_module,\n",
        "            run_config=run_config,\n",
        "            np_env_config=np_env_config,\n",
        "            split_label=split_label,\n",
        "            log_to_wandb=True,\n",
        "            return_metrics=True\n",
        "        )\n",
        "\n",
        "        metrics = {turbulence_log_name: th, **metrics}\n",
        "        th_metrics.append(metrics)\n",
        "        th_results[th] = result\n",
        "\n",
        "    # Convert metrics to a DataFrame and log to WandB\n",
        "    df = pd.DataFrame(th_metrics).astype({turbulence_log_name: int})\n",
        "    metric_cols = df.drop(columns=[turbulence_log_name]).columns # take only true metric columns\n",
        "\n",
        "    # Identify the protected row where turbulence_log_name == chosen_th\n",
        "    protected_row = df[df[turbulence_log_name] == chosen_th].tail(1)  # Keep only the last occurrence\n",
        "\n",
        "    # Drop duplicate rows (except the protected one)\n",
        "    df = df.drop_duplicates(metric_cols, keep=\"last\")\n",
        "\n",
        "    # Drop rows where all metric columns are 0, excluding the protected row\n",
        "    df = df[~((df[metric_cols] == 0).all(axis=1) & ~df.index.isin(protected_row.index))]\n",
        "\n",
        "\n",
        "    wandb_table = wandb.Table(dataframe=df)\n",
        "    wandb.log({f\"threshold_grid_metrics-{split_label}\": wandb_table})\n",
        "\n",
        "    # Log metrics for best threshold\n",
        "    best_idx = df['sharpe_ratio'].idxmax()  # Use idxmax() instead of argmax()\n",
        "    best_metrics = df.loc[best_idx].to_dict()\n",
        "    best_metrics[turbulence_log_name] = int(best_metrics[turbulence_log_name])\n",
        "    log_eval_results(model_name, best_metrics, split_label, turbulence_log_name, 'best')\n",
        "\n",
        "    # Log metrics for chosen threshold (if any)\n",
        "    if chosen_th is not None:\n",
        "        chosen_metrics = df[df[turbulence_log_name] == chosen_th].iloc[0].to_dict()\n",
        "        log_eval_results(model_name, chosen_metrics, split_label, turbulence_log_name, 'chosen')\n",
        "\n",
        "    # Compute min_account_value and max_account_value\n",
        "    min_account_value = float('+inf')\n",
        "    max_account_value = float('-inf')\n",
        "    for th_value, result in th_results.items():\n",
        "        min_account_value = min(min_account_value, result['account_value'][model_name].min())\n",
        "        max_account_value = max(max_account_value, result['account_value'][model_name].max())\n",
        "\n",
        "    min_account_value -= plot_padding\n",
        "    max_account_value += plot_padding\n",
        "\n",
        "    # Plot returns\n",
        "    figs = []\n",
        "    fig_names = []\n",
        "    fig_collection_name = f\"cum_return-{split_label}-{turbulence_log_name}\"\n",
        "    best_th_value = best_metrics[turbulence_log_name]\n",
        "\n",
        "    for th_value in df[turbulence_log_name].values:\n",
        "        result = th_results[th_value]\n",
        "        metrics = df.drop(columns=[turbulence_log_name])[\n",
        "            df[turbulence_log_name] == th_value].iloc[0].to_dict()\n",
        "\n",
        "        fig = plot_results(\n",
        "            **result,\n",
        "            figsize='small',\n",
        "            split_label=split_label,\n",
        "            metrics=metrics,\n",
        "            ylim_bottom=min_account_value,\n",
        "            ylim_top=max_account_value\n",
        "        )\n",
        "        fig_name = f\"{fig_collection_name}_{th_value}\"\n",
        "        if th_value == best_th_value:\n",
        "            fig_name += \"_best\"\n",
        "        if chosen_th is not None and th_value == chosen_th:\n",
        "            fig_name += \"_chosen\"\n",
        "\n",
        "        figs.append(fig)\n",
        "        fig_names.append(fig_name)\n",
        "\n",
        "    batch_log_plots_as_artifact(\n",
        "        figs,\n",
        "        fig_names,\n",
        "        artifact_name_prefix=fig_collection_name\n",
        "    )\n",
        "\n",
        "    best_th = best_metrics[turbulence_log_name]\n",
        "    return best_th"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXntnmmhuSsR"
      },
      "source": [
        "# Sweep Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "cellView": "form",
        "id": "8CXECEemca6u"
      },
      "outputs": [],
      "source": [
        "#@title load_model\n",
        "\n",
        "import ray\n",
        "from ray.rllib.algorithms.ppo import PPO\n",
        "\n",
        "AVAILABLE_MODELS_CLASSES = {\n",
        "    'ppo': PPO\n",
        "}\n",
        "\n",
        "def load_model(model_name, trained_model_dir = TRAINED_MODEL_DIR):\n",
        "    model_class = AVAILABLE_MODELS_CLASSES[model_name]\n",
        "    checkpoint_path = os.path.abspath(f\"{trained_model_dir}/{model_name}\")\n",
        "    algo = model_class.from_checkpoint(checkpoint_path)\n",
        "    return algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "cellView": "form",
        "id": "CMnFUx9gfaGH"
      },
      "outputs": [],
      "source": [
        "#@title Download artifacts\n",
        "\n",
        "def download_artifacts(run_id, artifact_types):\n",
        "    # Initialize the W&B API\n",
        "    api = wandb.Api()\n",
        "\n",
        "    # Retrieve the run\n",
        "    run = api.run(f\"{ENTITY}/{PROJECT}/{run_id}\")\n",
        "\n",
        "    # Iterate over the artifacts used or logged by the run\n",
        "    for artifact in run.logged_artifacts():\n",
        "        if artifact.type in artifact_types:\n",
        "            artifact_folder = f'./{artifact.type}'\n",
        "            !rm -rf artifact_folder\n",
        "            artifact.download(artifact_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "2LOgre2vnj_6"
      },
      "outputs": [],
      "source": [
        "#@title find_previous_run (sweep)\n",
        "\n",
        "def find_prev_run_id(\n",
        "        sweep_id,\n",
        "        curr_run_id,\n",
        "        curr_config_hash # HACK: pass along with curr_run_id since uploading new config_hash to wandb might be slow\n",
        "    ):\n",
        "\n",
        "    api = wandb.Api()\n",
        "    sweep_runs = api.sweep(f\"{ENTITY}/{PROJECT}/{sweep_id}\").runs\n",
        "    curr_run = api.run(f\"{ENTITY}/{PROJECT}/{curr_run_id}\")\n",
        "\n",
        "    prev_run_id = None\n",
        "    for run in sweep_runs:\n",
        "        # find run with same `hash_config`\n",
        "        # and current previous `test_start_date == current `val_start_date`\n",
        "        if run.id != curr_run.id \\\n",
        "        and run.summary['config_hash'] == curr_config_hash \\\n",
        "        and run.config['date_range']['test_start_date'] == curr_run.config['date_range']['val_start_date']:\n",
        "            prev_run_id = run.id\n",
        "            break\n",
        "\n",
        "    return prev_run_id\n",
        "\n",
        "# SWEEP_ID = '9otblgq2'\n",
        "# CURR_RUN_ID = 'kxjtk9qg'\n",
        "# prev_run_id = find_prev_run_id(SWEEP_ID, CURR_RUN_ID)\n",
        "# prev_run_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "cellView": "form",
        "id": "uSosu3u-YIIA"
      },
      "outputs": [],
      "source": [
        "#@title Sweep Runner\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "import random\n",
        "import string\n",
        "import json\n",
        "\n",
        "def config_to_canonical_string(cfg_dict):\n",
        "    return json.dumps(cfg_dict, sort_keys=True)\n",
        "\n",
        "def set_run_name(prefix, n=5):\n",
        "    run_name = f\"{prefix} | {wandb.run.id}\"\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "class SweepRunner:\n",
        "    def __init__(self, sweep_id):\n",
        "        self.sweep_id = sweep_id\n",
        "        self.pretrained_val_models = {}\n",
        "\n",
        "    def main(self, run_config=None):\n",
        "        run_timer_start = perf_counter()\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"START TIME:\", current_time)\n",
        "\n",
        "        with wandb.init(config=run_config):\n",
        "            run_config = wandb.config\n",
        "            # print('Debug copy run_config...', end=' ')\n",
        "            # _ = deepcopy(run_config)\n",
        "            # print(\"Done.\")\n",
        "\n",
        "            if run_config['dataset_type'] == 'yearly_train_test':\n",
        "                raise NotImplementedError\n",
        "            elif run_config['dataset_type'] == 'quarterly_train_val_test':\n",
        "                (\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config\n",
        "                ) = build_quarterly_train_val_test(run_config)\n",
        "                set_run_name(run_config['dataset_name'])\n",
        "\n",
        "                # Extract pretrained model from previous run\n",
        "                # for a given unique hyperparameter combination (e.g. (seed, steps) pair).\n",
        "                # Ignore `date_range` parameter since pretrained models should persist\n",
        "                # across the whole date range\n",
        "                config_seed = run_config['seed']\n",
        "                config_training_params = run_config['training_params']\n",
        "                # config_hash = hash(str(config_seed) + str(config_training_params))\n",
        "                config_hash = hash(config_to_canonical_string({\n",
        "                    'seed': config_seed,\n",
        "                    'training_params': config_training_params\n",
        "                }))\n",
        "\n",
        "                # run_config_copy = run_config._as_dict().copy()\n",
        "                # run_config_copy.pop('date_range')\n",
        "                # config_hash = hash(str(run_config_copy))\n",
        "\n",
        "\n",
        "                # extract pretrained model from previous run for a given seed\n",
        "                pretrained_val_models = self.pretrained_val_models.get(config_hash, {})\n",
        "                wandb.run.summary['config_hash'] = config_hash\n",
        "\n",
        "                if pretrained_val_models:\n",
        "                    run_config.update({'finetune': True})\n",
        "                else:\n",
        "                    run_config.update({'finetune': False})\n",
        "\n",
        "                pretrained_val_models = train_eval_rllib_models(\n",
        "                    run_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config,\n",
        "                    pretrained_val_models=pretrained_val_models\n",
        "                )\n",
        "\n",
        "                self.pretrained_val_models[config_hash] = pretrained_val_models\n",
        "\n",
        "            ray.shutdown()\n",
        "\n",
        "            run_timer_end = perf_counter()\n",
        "            run_duration_minutes = round( (run_timer_end - run_timer_start) / 60, 1)\n",
        "            wandb.run.summary[f\"run.duration_minutes\"] = run_duration_minutes\n",
        "            print(f\"RUN DURATION: {run_duration_minutes}\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"END TIME:\", current_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eszjIpg-TfkR"
      },
      "outputs": [],
      "source": [
        "#@title Sweep Runner (load model weights + continue = async agent)\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "from hashlib import sha256\n",
        "import random\n",
        "import string\n",
        "\n",
        "def config_to_canonical_string(cfg_dict):\n",
        "    return json.dumps(cfg_dict, sort_keys=True)\n",
        "\n",
        "def set_run_name(prefix, n=5):\n",
        "    run_name = f\"{prefix} | {wandb.run.id}\"\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "class SweepRunner:\n",
        "    def __init__(self, sweep_id):\n",
        "        self.sweep_id = sweep_id\n",
        "        self.pretrained_val_models = {}\n",
        "\n",
        "    def main(self, run_config=None):\n",
        "        run_timer_start = perf_counter()\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"START TIME:\", current_time)\n",
        "\n",
        "        with wandb.init(config=run_config):\n",
        "            run_config = wandb.config\n",
        "            # print('Debug copy run_config...', end=' ')\n",
        "            # _ = deepcopy(run_config)\n",
        "            # print(\"Done.\")\n",
        "\n",
        "            if run_config['dataset_type'] == 'yearly_train_test':\n",
        "                raise NotImplementedError\n",
        "            elif run_config['dataset_type'] == 'quarterly_train_val_test':\n",
        "                (\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config\n",
        "                ) = build_quarterly_train_val_test(run_config)\n",
        "                set_run_name(run_config['dataset_name'])\n",
        "\n",
        "                # Extract pretrained model from previous run\n",
        "                # for a given unique hyperparameter combination (e.g. (seed, steps) pair).\n",
        "                # Ignore `date_range` parameter since pretrained models should persist\n",
        "                # across the whole date range\n",
        "                config_seed = run_config['seed']\n",
        "                config_training_params = run_config['training_params']\n",
        "                config_canonical_str = config_to_canonical_string({\n",
        "                    'seed': config_seed,\n",
        "                    'training_params': config_training_params\n",
        "                })\n",
        "                config_hash = sha256(config_canonical_str.encode()).hexdigest()\n",
        "\n",
        "                # extract pretrained model from previous run for a given seed\n",
        "                pretrained_val_models = self.pretrained_val_models.get(config_hash, {})\n",
        "                wandb.run.summary['config_hash'] = config_hash\n",
        "\n",
        "                if not pretrained_val_models:\n",
        "                    print(\"Looking for models from previous run for this date range...\")\n",
        "                    prev_run_id = find_prev_run_id(self.sweep_id, wandb.run.id, config_hash)\n",
        "                    if prev_run_id is not None:\n",
        "                        download_artifacts(prev_run_id, artifact_types=['trained_models'])\n",
        "\n",
        "                        register_env(\"stock_trading_env\", create_stock_trading_env)\n",
        "                        pretrained_val_models = {\n",
        "                            model_name: load_model(model_name)\n",
        "                            for model_name in run_config['models_used']\n",
        "                        }\n",
        "                        print(\"Models downloaded.\")\n",
        "                    else:\n",
        "                        print(\"No models found.\")\n",
        "\n",
        "                if pretrained_val_models:\n",
        "                    run_config.update({'finetune': True})\n",
        "                else:\n",
        "                    run_config.update({'finetune': False})\n",
        "\n",
        "                pretrained_val_models = train_eval_rllib_models(\n",
        "                    run_config,\n",
        "                    train_np_env_config,\n",
        "                    val_np_env_config,\n",
        "                    test_np_env_config,\n",
        "                    pretrained_val_models=pretrained_val_models\n",
        "                )\n",
        "\n",
        "                self.pretrained_val_models[config_hash] = pretrained_val_models\n",
        "\n",
        "            ray.shutdown()\n",
        "\n",
        "            run_timer_end = perf_counter()\n",
        "            run_duration_minutes = round( (run_timer_end - run_timer_start) / 60, 1)\n",
        "            wandb.run.summary[f\"run.duration_minutes\"] = run_duration_minutes\n",
        "            print(f\"RUN DURATION: {run_duration_minutes}\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        print(\"END TIME:\", current_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4_Tz9YgisuK"
      },
      "source": [
        "# Run sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jqFSQXXM4Z0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title RUN SWEEP\n",
        "def run_sweep(n_runs, sweep_config=None, sweep_id=None):\n",
        "    wandb.finish()\n",
        "    if sweep_id is None:\n",
        "        assert sweep_config is not None\n",
        "        sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "    else:\n",
        "        assert sweep_config is None\n",
        "\n",
        "    !rm -rf {TRAINED_MODEL_DIR}/*\n",
        "\n",
        "    sweep_runner = SweepRunner(sweep_id)\n",
        "    wandb.agent(sweep_id, sweep_runner.main, project=PROJECT, count=n_runs)\n",
        "\n",
        "run_sweep(\n",
        "    sweep_id='jrebsx44',\n",
        "    # sweep_config=sweep_config,\n",
        "\n",
        "    # n_runs=None,\n",
        "    n_runs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8phhMNxzDhym"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y-J5mD_PTar9",
        "SIU-vXqDRW3L",
        "oWn4ZCwkvtN3",
        "hWUph5lzrTUS",
        "LHIfVohUTAsG",
        "NYSbz9QQ7pS8",
        "znlXqpfQZkzU",
        "oqc-9r54tuXG",
        "fXntnmmhuSsR"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyP/QK8+ndohOAoE+SEAlG1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}