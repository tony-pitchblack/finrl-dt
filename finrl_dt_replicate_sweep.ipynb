{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "Y-J5mD_PTar9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yfinance==0.2.50"
      ],
      "metadata": {
        "id": "Sg45avPpswui"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FjEBRlmyPp2a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3\n",
        "!pip install finrl\n",
        "!pip install alpaca_trade_api\n",
        "!pip install exchange_calendars\n",
        "!pip install stockstats\n",
        "!pip install wrds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "if np.__version__ != '1.26.4':\n",
        "    !pip install -q numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "id": "_RTmg2pwpR2W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import pandas as pd\n",
        "\n",
        "if pd.__version__ != '2.2.2':\n",
        "    !pip install -q pandas==2.2.2 --force-reinstall"
      ],
      "metadata": {
        "id": "KjhXAcAws0Rx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "RfYDJoTXo6-J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU-vXqDRW3L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "US_vB7hNSdeu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HJsl_3tVre6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ],
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General funcs"
      ],
      "metadata": {
        "id": "oWn4ZCwkvtN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title YahooDownloader\n",
        "\n",
        "\"\"\"Contains methods and classes to collect data from\n",
        "Yahoo Finance API\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "class YahooDownloader:\n",
        "    \"\"\"Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from neofinrl_config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from neofinrl_config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from neofinrl_config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
        "        \"\"\"Fetches data from Yahoo API\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        `pd.DataFrame`\n",
        "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
        "            for the specified stock ticker\n",
        "        \"\"\"\n",
        "        # Download and save the data in a pandas DataFrame:\n",
        "        data_df = pd.DataFrame()\n",
        "        num_failures = 0\n",
        "        for tic in self.ticker_list:\n",
        "            temp_df = yf.download(\n",
        "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
        "            )\n",
        "            temp_df[\"tic\"] = tic\n",
        "            if len(temp_df) > 0:\n",
        "                # data_df = data_df.append(temp_df)\n",
        "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "        if num_failures == len(self.ticker_list):\n",
        "            raise ValueError(\"no data is fetched.\")\n",
        "        # reset the index, we want to use numbers as index instead of dates\n",
        "        data_df = data_df.reset_index()\n",
        "\n",
        "        try:\n",
        "            # Convert wide to long format\n",
        "            # print(f\"DATA COLS: {data_df.columns}\")\n",
        "            data_df = data_df.sort_index(axis=1).set_index(['Date']).drop(columns=['tic']).stack(level='Ticker', future_stack=True)\n",
        "            data_df.reset_index(inplace=True)\n",
        "            data_df.columns.name = ''\n",
        "\n",
        "            # convert the column names to standardized names\n",
        "            data_df.rename(columns={'Ticker': 'Tic', 'Adj Close': 'Adjcp'}, inplace=True)\n",
        "            data_df.rename(columns={col: col.lower() for col in data_df.columns}, inplace=True)\n",
        "\n",
        "            columns = [\n",
        "                \"date\",\n",
        "                \"tic\",\n",
        "                \"open\",\n",
        "                \"high\",\n",
        "                \"low\",\n",
        "                \"close\",\n",
        "                \"adjcp\",\n",
        "                \"volume\",\n",
        "            ]\n",
        "\n",
        "            data_df = data_df[columns]\n",
        "            # use adjusted close price instead of close price\n",
        "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
        "            # drop the adjusted close price column\n",
        "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
        "\n",
        "        except NotImplementedError:\n",
        "            print(\"the features are not supported currently\")\n",
        "\n",
        "        # create day of the week column (monday = 0)\n",
        "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
        "        # convert date to standard string format, easy to filter\n",
        "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
        "        # drop missing data\n",
        "        data_df = data_df.dropna()\n",
        "        data_df = data_df.reset_index(drop=True)\n",
        "        print(\"Shape of DataFrame: \", data_df.shape)\n",
        "        # print(\"Display DataFrame: \", data_df.head())\n",
        "\n",
        "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
        "\n",
        "        return data_df\n",
        "\n",
        "    def select_equal_rows_stock(self, df):\n",
        "        df_check = df.tic.value_counts()\n",
        "        df_check = pd.DataFrame(df_check).reset_index()\n",
        "        df_check.columns = [\"tic\", \"counts\"]\n",
        "        mean_df = df_check.counts.mean()\n",
        "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
        "        names = df.tic.value_counts().index\n",
        "        select_stocks_list = list(names[equal_list])\n",
        "        df = df[df.tic.isin(select_stocks_list)]\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "gbd4N4QLPXlL",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BRInsCaFgH63",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title construct_daily_index\n",
        "def construct_daily_index(data_df, date_column='date', new_index_name='date_index'):\n",
        "    \"\"\"\n",
        "    Constructs a daily index from unique dates in the specified column.\n",
        "\n",
        "    Parameters:\n",
        "        data_df (pd.DataFrame): The input DataFrame.\n",
        "        date_column (str): The name of the column containing dates.\n",
        "        new_index_name (str): The name for the new index.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a daily index.\n",
        "    \"\"\"\n",
        "    # Get unique dates and create a mapping to daily indices\n",
        "    total_dates = data_df[date_column].unique()\n",
        "    date_to_index = {date: idx for idx, date in enumerate(sorted(total_dates))}\n",
        "\n",
        "    # Map dates to daily indices and set as index\n",
        "    data_df[new_index_name] = data_df[date_column].map(date_to_index)\n",
        "    data_df.set_index(new_index_name, inplace=True)\n",
        "    data_df.index.name = ''  # Remove the index name for simplicity\n",
        "\n",
        "    return data_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get dataset name\n",
        "\n",
        "def get_quarterly_dataset_name(prefix, train_start_date, val_start_date, test_start_date):\n",
        "    get_quarter = lambda date: f'Q{(date.month - 1) // 3 + 1}'\n",
        "\n",
        "    val_quarter = get_quarter(val_start_date)\n",
        "    test_quarter = get_quarter(test_start_date)\n",
        "\n",
        "    # Extract year and month\n",
        "    train_start = f\"{train_start_date.year}-{train_start_date.month:02}\"\n",
        "    val_start = f\"{val_start_date.year}\"\n",
        "    test_start = f\"{test_start_date.year}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start} | {val_start} {val_quarter} | {test_start} {test_quarter}\"\n",
        "\n",
        "    return dataset_name\n",
        "\n",
        "def get_yearly_dataset_name(prefix, train_start, test_start, test_end):\n",
        "    # Extract year and month\n",
        "    train_start_str = f\"{train_start.year}-{train_start.month:02}\"\n",
        "    test_start_str = f\"{test_start.year}-{test_start.month:02}\"\n",
        "    test_end_str = f\"{test_end.year}-{test_end.month:02}\"\n",
        "\n",
        "    # Construct the dataset name\n",
        "    dataset_name = f\"{prefix} | {train_start_str} | {test_start_str} | {test_end_str}\"\n",
        "    return dataset_name\n"
      ],
      "metadata": {
        "id": "GQ6BIJxbwuVh"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TOfz3JlX-oG5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title add_dataset\n",
        "\n",
        "def add_dataset(stock_index_name, train_df, test_df):\n",
        "    if 'datasets' not in globals():\n",
        "        global datasets\n",
        "        datasets = {}\n",
        "\n",
        "    # Ensure datetime format\n",
        "    if 'date' in train_df.columns:\n",
        "        train_df.set_index('date', inplace=True)\n",
        "    train_df.index = pd.to_datetime(train_df.index)\n",
        "\n",
        "    if 'date' in test_df.columns:\n",
        "        test_df.set_index('date', inplace=True)\n",
        "    test_df.index = pd.to_datetime(test_df.index)\n",
        "\n",
        "    train_start_date = train_df.index[0]\n",
        "    test_start_date = test_df.index[0]\n",
        "    test_end_date = test_df.index[-1]\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        stock_index_name,\n",
        "        train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    train_df.reset_index(inplace=True)\n",
        "    test_df.reset_index(inplace=True)\n",
        "\n",
        "    train_df = construct_daily_index(train_df)\n",
        "    test_df = construct_daily_index(test_df)\n",
        "\n",
        "    ticker_list = train_df.tic.unique().tolist()\n",
        "\n",
        "    datasets[dataset_name] = {\n",
        "        'train': train_df,\n",
        "        'test': test_df,\n",
        "        'metadata': dict(\n",
        "            stock_index_name = stock_index_name,\n",
        "            train_start_date = train_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "            num_tickers = len(ticker_list),\n",
        "            ticker_list = ticker_list,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUph5lzrTUS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA: DOW-30 (quarterly train/val/test)"
      ],
      "metadata": {
        "id": "LHIfVohUTAsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download\n",
        "%%capture\n",
        "\n",
        "train_start_date = '2009-01-01'\n",
        "max_test_end_date = '2020-08-01'\n",
        "\n",
        "########################\n",
        "\n",
        "data_df = YahooDownloader(\n",
        "    start_date= pd.Timestamp(train_start_date),\n",
        "    end_date= pd.Timestamp(max_test_end_date),\n",
        "    ticker_list=config_tickers.DOW_30_TICKER\n",
        ").fetch_data()\n",
        "\n",
        "data_df['date'] = pd.to_datetime(data_df['date'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y3m7Yc3rTILs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title add features\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "\n",
        "fe = FeatureEngineer(use_turbulence=True, use_vix=True)\n",
        "preproc_df = fe.preprocess_data(data_df.astype({'date': str}))\n",
        "preproc_df['date'] = pd.to_datetime(preproc_df['date'])\n",
        "# preproc_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89022b5-0947-4bf1-8fbb-f61af8b85cd3",
        "cellView": "form",
        "id": "wQ9IFLWeTILs"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully added technical indicators\n",
            "Shape of DataFrame:  (2916, 8)\n",
            "Successfully added vix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully added turbulence index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate_quarterly_date_ranges\n",
        "from calendar import monthrange\n",
        "\n",
        "min_test_start_date = '2016-01-01'\n",
        "\n",
        "def generate_quarterly_date_ranges(train_start_date, min_test_start_date, max_test_end_date, preproc_df, return_strings=False):\n",
        "    is_quarter_start = lambda date: date.month in [1, 4, 7, 10] and date.day == 1\n",
        "\n",
        "    min_test_start_date = pd.Timestamp(min_test_start_date)\n",
        "    train_start_date = pd.Timestamp(train_start_date)\n",
        "    max_test_end_date = pd.Timestamp(max_test_end_date)\n",
        "\n",
        "    assert is_quarter_start(train_start_date), f\"train_start_date {train_start_date} is not a quarter start date.\"\n",
        "    assert is_quarter_start(min_test_start_date), f\"min_test_start_date {min_test_start_date} is not a quarter start date.\"\n",
        "\n",
        "    assert max_test_end_date + pd.DateOffset(month=3) <= preproc_df['date'].max()\n",
        "    assert train_start_date + pd.DateOffset(days=1) >= preproc_df['date'].min()\n",
        "\n",
        "    test_start_date = min_test_start_date\n",
        "    date_ranges = []\n",
        "    while True:\n",
        "        val_start_date = test_start_date - pd.DateOffset(months=3)\n",
        "        test_end_date = test_start_date + pd.DateOffset(months=3)\n",
        "\n",
        "        if test_end_date > max_test_end_date:\n",
        "            break\n",
        "\n",
        "        date_range = (dict(\n",
        "            train_start_date = train_start_date,\n",
        "            val_start_date = val_start_date,\n",
        "            test_start_date = test_start_date,\n",
        "            test_end_date = test_end_date,\n",
        "        ))\n",
        "\n",
        "        if return_strings:\n",
        "            date_range = {k: str(v) for k, v in date_range.items()}\n",
        "\n",
        "        date_ranges.append(date_range)\n",
        "\n",
        "        test_start_date = test_end_date\n",
        "\n",
        "\n",
        "    return date_ranges\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(train_start_date, min_test_start_date, max_test_end_date, preproc_df)\n",
        "date_ranges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SSetOu0ejt2w",
        "outputId": "484d252f-3a38-47ad-8223-22b86aaa5054"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2015-10-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2016-01-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2016-04-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2016-01-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2016-04-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2016-07-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2016-04-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2016-07-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2016-10-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2016-07-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2016-10-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2017-01-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2016-10-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2017-01-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2017-04-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2017-01-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2017-04-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2017-07-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2017-04-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2017-07-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2017-10-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2017-07-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2017-10-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2018-01-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2017-10-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2018-01-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2018-04-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2018-01-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2018-04-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2018-07-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2018-04-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2018-07-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2018-10-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2018-07-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2018-10-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2019-01-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2018-10-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2019-01-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2019-04-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2019-01-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2019-04-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2019-07-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2019-04-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2019-07-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2019-10-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2019-07-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2019-10-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2020-01-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2019-10-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2020-01-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2020-04-01 00:00:00')},\n",
              " {'train_start_date': Timestamp('2009-01-01 00:00:00'),\n",
              "  'val_start_date': Timestamp('2020-01-01 00:00:00'),\n",
              "  'test_start_date': Timestamp('2020-04-01 00:00:00'),\n",
              "  'test_end_date': Timestamp('2020-07-01 00:00:00')}]"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_date_range(df, start_date, end_date):\n",
        "    return df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "\n",
        "for date_range in date_ranges:\n",
        "    train_df, val_df, test_df = (\n",
        "        subset_date_range(preproc_df, date_range['train_start_date'], date_range['val_start_date']),\n",
        "        subset_date_range(preproc_df, date_range['val_start_date'], date_range['test_start_date']),\n",
        "        subset_date_range(preproc_df, date_range['test_start_date'], date_range['test_end_date']),\n",
        "    )"
      ],
      "metadata": {
        "id": "S8GdHjZWTcHu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA: DOW-30 (yearly train/test)"
      ],
      "metadata": {
        "id": "Op7oS8Jw1pgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate date range (yearly train/test)\n",
        "\n",
        "min_test_start_year = 2020\n",
        "max_test_start_year = 2025\n",
        "\n",
        "train_years_count = 10\n",
        "val_years_count = 0.25\n",
        "test_years_count = 1.5\n",
        "\n",
        "train_start_date = \\\n",
        "    pd.Timestamp(year=min_test_start_year, month=1, day=1) - \\\n",
        "    pd.Timedelta(days=int(train_years_count * 365.2425))\n",
        "\n",
        "max_test_end_date = \\\n",
        "    pd.Timestamp(year=max_test_start_year, month=1, day=1) + \\\n",
        "    pd.Timedelta(days=int(test_years_count * 365.2425))\n",
        "\n",
        "train_start_date, max_test_end_date"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L9_FOjg-qLA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4cfc4f49-d6a0-45e8-ca57-990efbd69b1d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Timestamp('2010-01-01 00:00:00'), Timestamp('2026-07-02 00:00:00'))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download data\n",
        "\n",
        "data_df = YahooDownloader(\n",
        "    start_date=train_start_date,\n",
        "    end_date=max_test_end_date,\n",
        "    ticker_list=config_tickers.DOW_30_TICKER\n",
        ").fetch_data()\n",
        "\n",
        "data_df['date'] = pd.to_datetime(data_df['date'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_NfZX4cWP8h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title add features\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "\n",
        "fe = FeatureEngineer(use_turbulence=True, use_vix=True)\n",
        "preproc_df = fe.preprocess_data(data_df.astype({'date': str}))\n",
        "preproc_df['date'] = pd.to_datetime(preproc_df['date'])\n",
        "# preprocessed_data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKKGaOBO6XWM",
        "outputId": "29c65dfd-e985-4de8-8637-131fa8b04728",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully added technical indicators\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of DataFrame:  (2916, 8)\n",
            "Successfully added vix\n",
            "Successfully added turbulence index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate splits (yearly train / test)\n",
        "def generate_yearly_train_test_dates(train_years_count, test_years_count, test_start_year):\n",
        "    test_start_date = pd.Timestamp(year=test_start_year, month=1, day=1)\n",
        "\n",
        "    train_start_date = \\\n",
        "        test_start_date - \\\n",
        "        pd.Timedelta(days=int(train_years_count * 365.2425))\n",
        "\n",
        "    test_end_date = \\\n",
        "        test_start_date + \\\n",
        "        pd.Timedelta(days=int(test_years_count * 365.2425))\n",
        "\n",
        "    return train_start_date, test_start_date, test_end_date\n",
        "\n",
        "# clip max year w.r.t. to available data\n",
        "max_data_date = data_df['date'].max()\n",
        "max_test_start_year = min(max_test_start_year, max_data_date.year)\n",
        "\n",
        "for test_start_year in range(min_test_start_year, max_test_start_year + 1):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        train_years_count, test_years_count, test_start_year\n",
        "    )\n",
        "\n",
        "    # Filter using the 'date' column\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    # add_dataset('DOW_30', train_df, test_df)\n",
        "\n",
        "    print(f\"Train start: {train_df['date'].min()}, Train end: {train_df['date'].max()}\")\n",
        "    print(f\"Test start: {test_df['date'].min()}, Test end: {test_df['date'].max()}\")\n",
        "    print()\n",
        "\n",
        "# print(*list(datasets.keys()), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBiw3TS56LfN",
        "outputId": "3832598f-800b-446a-deb5-254def5ebde2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train start: 2010-01-04 00:00:00, Train end: 2019-12-31 00:00:00\n",
            "Test start: 2020-01-02 00:00:00, Test end: 2021-06-30 00:00:00\n",
            "\n",
            "Train start: 2011-01-03 00:00:00, Train end: 2020-12-31 00:00:00\n",
            "Test start: 2021-01-04 00:00:00, Test end: 2022-07-01 00:00:00\n",
            "\n",
            "Train start: 2012-01-03 00:00:00, Train end: 2021-12-31 00:00:00\n",
            "Test start: 2022-01-03 00:00:00, Test end: 2023-06-30 00:00:00\n",
            "\n",
            "Train start: 2013-01-02 00:00:00, Train end: 2022-12-30 00:00:00\n",
            "Test start: 2023-01-03 00:00:00, Test end: 2024-06-28 00:00:00\n",
            "\n",
            "Train start: 2014-01-02 00:00:00, Train end: 2023-12-29 00:00:00\n",
            "Test start: 2024-01-02 00:00:00, Test end: 2024-12-20 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NYSbz9QQ7pS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb artifacts"
      ],
      "metadata": {
        "id": "KemUy0OKg-wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_artifact\n",
        "\n",
        "def update_artifact(folder_path, name_prefix, type):\n",
        "    \"\"\"\n",
        "    Create or update a W&B artifact consisting of a folder.\n",
        "\n",
        "    Args:\n",
        "        run: The current W&B run.\n",
        "        folder_path (str): Path to the folder to upload.\n",
        "        artifact_name (str): Name of the artifact.\n",
        "        artifact_type (str): Type of the artifact.\n",
        "    \"\"\"\n",
        "    run = wandb.run\n",
        "    artifact_name = f'{name_prefix}-{wandb.run.id}'\n",
        "\n",
        "    # Create a new artifact\n",
        "    artifact = wandb.Artifact(name=artifact_name, type=type)\n",
        "\n",
        "    # Add the folder to the artifact\n",
        "    artifact.add_dir(folder_path)\n",
        "\n",
        "    # Log the artifact to W&B\n",
        "    run.log_artifact(artifact)\n",
        "    print(f\"Artifact '{artifact_name}' has been updated and uploaded.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jF0Xbv9f631H"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "B-a7jHoxrb3Q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title update_model_artifacts\n",
        "\n",
        "def update_model_artifacts():\n",
        "    update_artifact(\n",
        "        folder_path = RESULTS_DIR,\n",
        "        name_prefix = 'results',\n",
        "        type = 'results'\n",
        "    )\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = TRAINED_MODEL_DIR,\n",
        "        name_prefix = 'trained_models',\n",
        "        type = 'trained_models'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_dataset_artifact\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def update_dataset_artifact(config, train_df, val_df=None, test_df=None):\n",
        "    DATASET_DIR = Path('./dataset')\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(DATASET_DIR / 'train_data.csv')\n",
        "\n",
        "    if test_df is not None:\n",
        "        test_df.to_csv(DATASET_DIR / 'test_data.csv')\n",
        "\n",
        "    if val_df is not None:\n",
        "        val_df.to_csv(DATASET_DIR / 'val_data.csv')\n",
        "\n",
        "    update_artifact(\n",
        "        folder_path = DATASET_DIR,\n",
        "        name_prefix = 'dataset',\n",
        "        type = 'dataset'\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Prm8SfPo7CJY"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & helper funcs"
      ],
      "metadata": {
        "id": "_BZTsxX0tkDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_quarterly_train_val_test\n",
        "def build_quarterly_train_val_test(config):\n",
        "    date_range = {key: pd.Timestamp(date) for key, date in config['date_range'].items()}\n",
        "\n",
        "    train_start_date = date_range['train_start_date']\n",
        "    val_start_date = date_range['val_start_date']\n",
        "    test_start_date = date_range['test_start_date']\n",
        "    test_end_date = date_range['test_end_date']\n",
        "\n",
        "    def extract_date_range(df, start_date, end_date):\n",
        "        return df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
        "\n",
        "    train_df, val_df, test_df = (\n",
        "        extract_date_range(preproc_df, train_start_date, val_start_date),\n",
        "        extract_date_range(preproc_df, val_start_date, test_start_date),\n",
        "        extract_date_range(preproc_df, test_start_date, test_end_date),\n",
        "    )\n",
        "\n",
        "    train_df = construct_daily_index(train_df)\n",
        "    val_df = construct_daily_index(val_df)\n",
        "    test_df = construct_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_quarterly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, val_start_date, test_start_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        dataset_name = dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "N2JCHB8ibDce"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build_yearly_train_test\n",
        "def build_yearly_train_test(config):\n",
        "    train_start_date, test_start_date, test_end_date = generate_yearly_train_test_dates(\n",
        "        config['train_years_count'],\n",
        "        config['test_years_count'],\n",
        "        config['test_start_year']\n",
        "    )\n",
        "\n",
        "    train_df = preproc_df[(preproc_df['date'] >= train_start_date) & (preproc_df['date'] < test_start_date)]\n",
        "    test_df = preproc_df[(preproc_df['date'] >= test_start_date) & (preproc_df['date'] < test_end_date)]\n",
        "\n",
        "    train_df = construct_daily_index(train_df)\n",
        "    test_df = construct_daily_index(test_df)\n",
        "\n",
        "    dataset_name = get_yearly_dataset_name(\n",
        "        config['stock_index_name'], train_start_date, test_start_date, test_end_date\n",
        "    )\n",
        "\n",
        "    config.update(dict(\n",
        "        train_start_date=train_start_date,\n",
        "        test_start_date=test_start_date,\n",
        "        test_end_date=test_end_date,\n",
        "        dataset_name=dataset_name\n",
        "    ))\n",
        "\n",
        "    update_dataset_artifact(\n",
        "        config,\n",
        "\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        test_df=test_df,\n",
        "    )\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SmzfG0qaVP93"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "cellView": "form",
        "id": "hE7nHKrYos_t",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Calculate fee percent based on average price for past N days\n",
        "\n",
        "def cost_pct_from_avg_price(df, cost_abs, price_avg_days, verbose=False):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    avg_price_dict = {}\n",
        "    for tic, _df in df.groupby('tic'):\n",
        "        last_date = _df['date'].max()\n",
        "        _df = _df[_df.date >= last_date - pd.Timedelta(days=price_avg_days)]\n",
        "        avg_price = ((_df.high + _df.low) / 2).mean()\n",
        "        avg_price_dict.update({tic: avg_price})\n",
        "\n",
        "    avg_price_df = pd.DataFrame(avg_price_dict, index=[f'cost_avg']).T\n",
        "    cost_pct_df = (cost_abs / avg_price_df).rename(columns={'cost_avg': 'cost_pct'})\n",
        "\n",
        "    if verbose:\n",
        "        display(avg_price_df.head())\n",
        "        print()\n",
        "        display(cost_pct_df.head())\n",
        "\n",
        "    return cost_pct_df.values.flatten().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title set_cost_pct\n",
        "\n",
        "def set_cost_pct(train, config):\n",
        "    # Calculate reference price interval\n",
        "    REFERENCE_PRICE_END_DATE = config['REFERENCE_PRICE_END_DATE']\n",
        "    REFERNCE_PRICE_WINDOW_DAYS = config['REFERNCE_PRICE_WINDOW_DAYS']\n",
        "\n",
        "    ref_price_start_date = \\\n",
        "        pd.Timestamp(REFERENCE_PRICE_END_DATE) \\\n",
        "        - pd.Timedelta(days=REFERNCE_PRICE_WINDOW_DAYS)\n",
        "\n",
        "    ref_price_df = YahooDownloader(\n",
        "            start_date=ref_price_start_date,\n",
        "            end_date=REFERENCE_PRICE_END_DATE,\n",
        "            ticker_list=train.tic.unique().tolist(),\n",
        "            # ticker_list=config_tickers.DOW_30_TICKER\n",
        "        ).fetch_data()\n",
        "\n",
        "    # Calculate cost\n",
        "    COST_PCT = cost_pct_from_avg_price(\n",
        "        df=ref_price_df,\n",
        "        cost_abs=config['cost_abs'],\n",
        "        price_avg_days=config['REFERNCE_PRICE_WINDOW_DAYS'],\n",
        "        # verbose=False\n",
        "    )\n",
        "\n",
        "    config.update({'cost_pct': COST_PCT})"
      ],
      "metadata": {
        "id": "BrmZizKHFZZE",
        "cellView": "form"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "_DFUBgSZT5-a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Init env\n",
        "def init_env(train, config):\n",
        "    stock_dimension = len(train.tic.unique())\n",
        "    state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "    cost_pct = config['cost_pct']\n",
        "    if isinstance(cost_pct, list):\n",
        "        assert len(cost_pct) == stock_dimension\n",
        "        buy_cost_pct = sell_cost_pct = cost_pct\n",
        "    elif isinstance(cost_pct, (int, float)):\n",
        "        buy_cost_pct = sell_cost_pct = [ config['COST_PCT'] ] * stock_dimension\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": config['initial_amount'],\n",
        "        \"num_stock_shares\": num_stock_shares,\n",
        "        \"buy_cost_pct\": buy_cost_pct,\n",
        "        \"sell_cost_pct\": sell_cost_pct,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4,\n",
        "\n",
        "        \"print_verbosity\": 1,\n",
        "        # \"make_plots\": True\n",
        "    }\n",
        "\n",
        "    e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
        "    return e_train_gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "t0-3fjeCG_GJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title WandbLoggerCallback\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class WandbLoggerCallback(BaseCallback):\n",
        "    def __init__(self, model_name, verbose=0):\n",
        "        self.model_name = model_name\n",
        "        super(WandbLoggerCallback, self).__init__(verbose)\n",
        "        self.sharpe_ratios = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # print(f\"LOGGING {self.model_name} sharpe ratio\")\n",
        "        # Access the environment\n",
        "        env = self.training_env.envs[0]\n",
        "\n",
        "        # Check if the episode is terminal\n",
        "        env.terminal = env.day >= len(env.df.index.unique()) - 1\n",
        "        if env.terminal:\n",
        "            # results_df = pd.DataFrame(env.asset_memory, columns=[self.model_name])\n",
        "            asset_values = pd.Series(env.asset_memory, index=env.date_memory).dropna()\n",
        "            # display(asset_values)\n",
        "\n",
        "            # Calculate the Sharpe ratio\n",
        "            sharpe = calculate_sharpe_ratio(asset_values)\n",
        "            self.sharpe_ratios.append(sharpe)\n",
        "\n",
        "            # Calculate MDD\n",
        "            mdd = calculate_mdd(asset_values)\n",
        "\n",
        "            end_total_asset = env.state[0] + sum(\n",
        "                np.array(env.state[1 : (env.stock_dim + 1)])\n",
        "                * np.array(env.state[(env.stock_dim + 1) : (env.stock_dim * 2 + 1)])\n",
        "            )\n",
        "\n",
        "            # Calculate annualized return\n",
        "            cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "            if np.isinf(cum_ret) or np.isnan(cum_ret):\n",
        "                cum_ret = np.nan\n",
        "            num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "            ann_ret = ( (1 + cum_ret / 100) ** (365 / num_days) - 1 ) * 100\n",
        "\n",
        "            wandb.log({\n",
        "                'train': {\n",
        "                    f'begin_total_asset/{self.model_name}': env.asset_memory[0],\n",
        "                    f'end_total_asset/{self.model_name}': end_total_asset,\n",
        "                    f'total_cost/{self.model_name}': env.cost,\n",
        "                    f'total_trades/{self.model_name}': env.trades,\n",
        "                    f'sharpe_ratio/{self.model_name}': sharpe,\n",
        "                    f'ann_return/{self.model_name}': ann_ret,\n",
        "                    f'cum_return/{self.model_name}': cum_ret,\n",
        "                    f'mdd/{self.model_name}': mdd,\n",
        "                }\n",
        "            }, step=env.episode)\n",
        "\n",
        "            # Add to config for instant acess\n",
        "            if \"sharpe_ratios\" not in wandb.config:\n",
        "                wandb.config.sharpe_ratios = {}\n",
        "\n",
        "            wandb.config.sharpe_ratios[self.model_name] = sharpe\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eZPK1476kLm_"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title BestModelWandbLoggerCallback\n",
        "\n",
        "import re\n",
        "import wandb\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class BestModelWandbLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(BestModelWandbLoggerCallback, self).__init__(verbose)\n",
        "\n",
        "    def _on_step(self) -> None:\n",
        "        # Access the environment\n",
        "        env = self.training_env.envs[0]\n",
        "\n",
        "        # Check if the episode is terminal\n",
        "        env.terminal = env.day >= len(env.df.index.unique()) - 1\n",
        "        if env.terminal:\n",
        "            sharpe_ratios = wandb.config.get(\"sharpe_ratios\", {})\n",
        "            max_sharpe_ratio_model = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "            max_sharpe_ratio = sharpe_ratios[max_sharpe_ratio_model]\n",
        "\n",
        "            # Log the max Sharpe ratio and the corresponding model name\n",
        "            wandb.log({\n",
        "                'train': {\n",
        "                    'sharpe_ratio/best_model': max_sharpe_ratio,\n",
        "                    'best_model_name': max_sharpe_ratio_model\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return True\n",
        "\n",
        "class BestModelWandbLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(BestModelWandbLoggerCallback, self).__init__(verbose)\n",
        "\n",
        "    def _on_step(self) -> None:\n",
        "        # Access the environment\n",
        "        env = self.training_env.envs[0]\n",
        "\n",
        "        # Check if the episode is terminal\n",
        "        env.terminal = env.day >= len(env.df.index.unique()) - 1\n",
        "        if env.terminal:\n",
        "            sharpe_ratios = wandb.config.get(\"sharpe_ratios\", {})\n",
        "            max_sharpe_ratio_model = max(sharpe_ratios, key=sharpe_ratios.get)\n",
        "            max_sharpe_ratio = sharpe_ratios[max_sharpe_ratio_model]\n",
        "\n",
        "            # Fetch metrics for the best model\n",
        "            asset_values = pd.Series(env.asset_memory, index=env.date_memory).dropna()\n",
        "            mdd = calculate_mdd(asset_values)\n",
        "            cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "            num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "            ann_ret = ((1 + cum_ret / 100) ** (365 / num_days) - 1) * 100\n",
        "\n",
        "            # Log metrics\n",
        "            wandb.log({\n",
        "                'train': {\n",
        "                    'sharpe_ratio/best_model': max_sharpe_ratio,\n",
        "                    'best_model_name': max_sharpe_ratio_model,\n",
        "                    'mdd/best_model': mdd,\n",
        "                    'ann_return/best_model': ann_ret,\n",
        "                    'cum_return/best_model': cum_ret,\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PMDX8FvG_ipa"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom DRLAgent (3 callbacks)\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent, TensorboardCallback\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "import wandb\n",
        "\n",
        "class DRLAgent(DRLAgent):\n",
        "    @staticmethod\n",
        "    def train_model(\n",
        "        model,\n",
        "        tb_log_name,\n",
        "        total_timesteps=5000,\n",
        "        callback=None,  # Allow custom callbacks to be passed\n",
        "    ):\n",
        "        # Ensure TensorboardCallback is always included\n",
        "        tensorboard_callback = TensorboardCallback()\n",
        "\n",
        "        # Initialize default callbacks\n",
        "        sharpe_ratio_callback = WandbLoggerCallback(model_name=tb_log_name, verbose=1)\n",
        "        max_sharpe_ratio_ratio_callback = BestModelWandbLoggerCallback(verbose=1)\n",
        "\n",
        "        # Combine all callbacks (always include Tensorboard, SharpeRatio, and MaxSharpeRatio by default)\n",
        "        callbacks_to_use = [\n",
        "            tensorboard_callback,\n",
        "            sharpe_ratio_callback,\n",
        "            max_sharpe_ratio_ratio_callback\n",
        "        ]\n",
        "\n",
        "        # Add any custom callback passed by the user\n",
        "        if callback is not None:\n",
        "            if isinstance(callback, BaseCallback):\n",
        "                callbacks_to_use.append(callback)\n",
        "            elif isinstance(callback, list):\n",
        "                callbacks_to_use.extend(callback)\n",
        "            else:\n",
        "                raise ValueError(\"callback must be None, a BaseCallback, or a list of BaseCallback instances.\")\n",
        "\n",
        "        # Wrap all callbacks into a CallbackList\n",
        "        combined_callback = CallbackList(callbacks_to_use)\n",
        "\n",
        "        # Train the model with the combined callbacks\n",
        "        model = model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            tb_log_name=tb_log_name,\n",
        "            callback=combined_callback,\n",
        "        )\n",
        "        return model\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n_ztcCc4AgTl"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "znlXqpfQZkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title init\n",
        "parameters_dict = {}\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'max_sharpe_ratio'\n",
        "    },\n",
        "    'parameters': parameters_dict\n",
        "}"
      ],
      "metadata": {
        "id": "kF6LboMHB2xZ",
        "cellView": "form"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - yearly_train_test\n",
        "\n",
        "yearly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'yearly_train_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_years_count = {'value': 10},\n",
        "    test_years_count = {'value': 1},\n",
        "    test_start_year = {\n",
        "        # 'value': 2020,\n",
        "        'values': list(range(2020, 2025))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7RzkkJNiSkP7",
        "cellView": "form"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: create dataset - quarterly_train_test\n",
        "\n",
        "train_start_date = '2009-01-01'\n",
        "min_test_start_date = '2016-01-01'\n",
        "max_test_end_date = '2020-08-05'\n",
        "\n",
        "date_ranges = generate_quarterly_date_ranges(\n",
        "    train_start_date, min_test_start_date, max_test_end_date, preproc_df,\n",
        "    return_strings=True\n",
        ")\n",
        "\n",
        "#################################################################\n",
        "\n",
        "quarterly_dataset_params = dict(\n",
        "    dataset_type = {'value': 'quarterly_train_val_test'},\n",
        "    stock_index_name = {'value': 'DOW-30'},\n",
        "\n",
        "    train_start_date = {'value': train_start_date},\n",
        "    min_test_start_date = {'value': min_test_start_date},\n",
        "    max_test_end_date = {'value': max_test_end_date},\n",
        "    date_range = {'values': date_ranges}\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l-hEkHxAfYVv"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: choose dataset\n",
        "parameters_dict.update(\n",
        "    # yearly_dataset_params,\n",
        "    quarterly_dataset_params\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GJDvneoY00zo"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title env params\n",
        "parameters_dict.update(dict(\n",
        "    cost_abs = {'value': 2.5},\n",
        "    initial_amount = {'value': 50_000},\n",
        "    REFERENCE_PRICE_END_DATE = {'value': '2024-12-21'},\n",
        "    REFERNCE_PRICE_WINDOW_DAYS = {'value': 30}\n",
        "))"
      ],
      "metadata": {
        "id": "ILF379nrW4YK",
        "cellView": "form"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: using_model & train_params\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': True},\n",
        "    'if_using_ppo': {'value': True},\n",
        "    'if_using_td3': {'value': True},\n",
        "    'if_using_sac': {'value': True}\n",
        "})\n",
        "parameters_dict.update({\n",
        "    'train_params': {\n",
        "        'parameters': {\n",
        "            'a2c': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 50_000}\n",
        "                }\n",
        "            },\n",
        "            'ddpg': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 50_000}\n",
        "                }\n",
        "            },\n",
        "            'td3': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 50_000}\n",
        "                }\n",
        "            },\n",
        "            'sac': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 70_000}\n",
        "                }\n",
        "            },\n",
        "            'ppo': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 200_000}\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PzWRRS6Prorh"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: using_model & train_params (medium smoke)\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': True},\n",
        "    'if_using_ppo': {'value': True},\n",
        "    'if_using_td3': {'value': True},\n",
        "    'if_using_sac': {'value': True}\n",
        "})\n",
        "parameters_dict.update({\n",
        "    'train_params': {\n",
        "        'parameters': {\n",
        "            'a2c': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 3_000}\n",
        "                }\n",
        "            },\n",
        "            'ddpg': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 3_000}\n",
        "                }\n",
        "            },\n",
        "            'td3': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 3_000}\n",
        "                }\n",
        "            },\n",
        "            'sac': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 5_000}\n",
        "                }\n",
        "            },\n",
        "            'ppo': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 2_500}\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "})"
      ],
      "metadata": {
        "id": "5ZR9ljUV1uS2",
        "cellView": "form"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CONFIG: using_model & train_params (smoke a2c)\n",
        "\n",
        "parameters_dict.update({\n",
        "    'if_using_a2c': {'value': True},\n",
        "    'if_using_ddpg': {'value': False},\n",
        "    'if_using_ppo': {'value': False},\n",
        "    'if_using_td3': {'value': False},\n",
        "    'if_using_sac': {'value': False}\n",
        "})\n",
        "parameters_dict.update({\n",
        "    'train_params': {\n",
        "        'parameters': {\n",
        "            'a2c': {\n",
        "                'parameters': {\n",
        "                    'steps': {'value': 3_000}\n",
        "                }\n",
        "            },\n",
        "            # 'ddpg': {\n",
        "            #     'parameters': {\n",
        "            #         'steps': {'value': 50_000}\n",
        "            #     }\n",
        "            # },\n",
        "            # 'td3': {\n",
        "            #     'parameters': {\n",
        "            #         'steps': {'value': 50_000}\n",
        "            #     }\n",
        "            # },\n",
        "            # 'sac': {\n",
        "            #     'parameters': {\n",
        "            #         'steps': {'value': 70_000}\n",
        "            #     }\n",
        "            # },\n",
        "            # 'ppo': {\n",
        "            #     'parameters': {\n",
        "            #         'steps': {'value': 200_000}\n",
        "            #     }\n",
        "            # },\n",
        "        }\n",
        "    }\n",
        "})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "beATEA1EtXog"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train models"
      ],
      "metadata": {
        "id": "oqc-9r54tuXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate run name\n",
        "import random\n",
        "import string\n",
        "\n",
        "def generate_run_name(prefix, n=5):\n",
        "    random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n",
        "    return f\"{prefix} | {random_str}\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "zv6hAaGVYKfc"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "cellView": "form",
        "id": "dyvEieUxQvSA"
      },
      "outputs": [],
      "source": [
        "#@title FUNC: train models\n",
        "\n",
        "def train_models(e_train_gym, config):\n",
        "    check_and_make_directories([TRAINED_MODEL_DIR])\n",
        "\n",
        "    env_train, _ = e_train_gym.get_sb_env()\n",
        "    # print(type(env_train))\n",
        "\n",
        "    # Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "\n",
        "    # Load variables from the config\n",
        "    if_using_a2c = config[\"if_using_a2c\"]\n",
        "    if_using_ddpg = config[\"if_using_ddpg\"]\n",
        "    if_using_ppo = config[\"if_using_ppo\"]\n",
        "    if_using_td3 = config[\"if_using_td3\"]\n",
        "    if_using_sac = config[\"if_using_sac\"]\n",
        "\n",
        "    if if_using_a2c:\n",
        "        print(\"training A2C agent\")\n",
        "        agent = DRLAgent(env = env_train)\n",
        "        model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "        # set up logger\n",
        "        tmp_path = RESULTS_DIR + '/a2c'\n",
        "        !rm -rf {tmp_path}/*\n",
        "        new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "        # Set new logger\n",
        "        model_a2c.set_logger(new_logger_a2c)\n",
        "\n",
        "        trained_a2c = agent.train_model(\n",
        "            model=model_a2c,\n",
        "            tb_log_name='a2c',\n",
        "            total_timesteps=config['train_params']['a2c']['steps']\n",
        "        ) if if_using_a2c else None\n",
        "\n",
        "        trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None\n",
        "        update_model_artifacts()\n",
        "\n",
        "    if if_using_ddpg:\n",
        "        print(\"training DDPG agent\")\n",
        "        agent = DRLAgent(env = env_train)\n",
        "        model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "        # set up logger\n",
        "        tmp_path = RESULTS_DIR + '/ddpg'\n",
        "        !rm -rf {tmp_path}/*\n",
        "        new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "        # Set new logger\n",
        "        model_ddpg.set_logger(new_logger_ddpg)\n",
        "\n",
        "        trained_ddpg = agent.train_model(\n",
        "            model=model_ddpg,\n",
        "            tb_log_name='ddpg',\n",
        "            total_timesteps=config['train_params']['ddpg']['steps']\n",
        "        ) if if_using_ddpg else None\n",
        "\n",
        "        trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None\n",
        "        update_model_artifacts()\n",
        "\n",
        "    if if_using_td3:\n",
        "        print(\"training TD3 agent\")\n",
        "        agent = DRLAgent(env = env_train)\n",
        "        TD3_PARAMS = {\"batch_size\": 100,\n",
        "                    \"buffer_size\": 1000000,\n",
        "                    \"learning_rate\": 0.001}\n",
        "\n",
        "        model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "        # set up logger\n",
        "        tmp_path = RESULTS_DIR + '/td3'\n",
        "        !rm -rf {tmp_path}/*\n",
        "        new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "        # Set new logger\n",
        "        model_td3.set_logger(new_logger_td3)\n",
        "\n",
        "        trained_td3 = agent.train_model(\n",
        "            model=model_td3,\n",
        "            tb_log_name='td3',\n",
        "            total_timesteps=config['train_params']['td3']['steps']\n",
        "        ) if if_using_td3 else None\n",
        "\n",
        "        trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None\n",
        "        update_model_artifacts()\n",
        "\n",
        "    if if_using_sac:\n",
        "        print(\"training SAC agent\")\n",
        "        agent = DRLAgent(env = env_train)\n",
        "        SAC_PARAMS = {\n",
        "            \"batch_size\": 128,\n",
        "            \"buffer_size\": 100000,\n",
        "            \"learning_rate\": 0.0001,\n",
        "            \"learning_starts\": 100,\n",
        "            \"ent_coef\": \"auto_0.1\",\n",
        "        }\n",
        "\n",
        "        model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "        # set up logger\n",
        "        tmp_path = RESULTS_DIR + '/sac'\n",
        "        !rm -rf {tmp_path}/*\n",
        "        new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "        # Set new logger\n",
        "        model_sac.set_logger(new_logger_sac)\n",
        "\n",
        "        trained_sac = agent.train_model(\n",
        "            model=model_sac,\n",
        "            tb_log_name='sac',\n",
        "            total_timesteps=config['train_params']['sac']['steps']\n",
        "        ) if if_using_sac else None\n",
        "        trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None\n",
        "        update_model_artifacts()\n",
        "\n",
        "    if if_using_ppo:\n",
        "        agent = DRLAgent(env = env_train)\n",
        "        PPO_PARAMS = {\n",
        "            \"n_steps\": 2048,\n",
        "            \"ent_coef\": 0.01,\n",
        "            \"learning_rate\": 0.00025,\n",
        "            \"batch_size\": 128,\n",
        "        }\n",
        "        model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "        # set up logger\n",
        "        tmp_path = RESULTS_DIR + '/ppo'\n",
        "        !rm -rf {tmp_path}/*\n",
        "        new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "        # Set new logger\n",
        "        model_ppo.set_logger(new_logger_ppo)\n",
        "\n",
        "        trained_ppo = agent.train_model(\n",
        "            model=model_ppo,\n",
        "            tb_log_name='ppo',\n",
        "            total_timesteps=config['train_params']['ppo']['steps']\n",
        "        ) if if_using_ppo else None\n",
        "\n",
        "        trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None\n",
        "        update_model_artifacts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title main\n",
        "import wandb\n",
        "\n",
        "def main(config=None):\n",
        "    # Initialize a new wandb run using the context manager\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        # Build the dataset\n",
        "        if config['dataset_type'] == 'yearly_train_test':\n",
        "            train_df, test_df = build_yearly_train_test(config)\n",
        "        elif config['dataset_type'] == 'quarterly_train_val_test':\n",
        "            train_df, val_df, test_df = build_quarterly_train_val_test(config)\n",
        "\n",
        "        wandb.run.name = generate_run_name(config['dataset_name'])\n",
        "        wandb.run.save()\n",
        "\n",
        "        # Set the cost percentage (or any other constants you need to set)\n",
        "        set_cost_pct(train_df, config)\n",
        "\n",
        "        e_train_gym = init_env(train_df, config)\n",
        "        train_models(e_train_gym, config)"
      ],
      "metadata": {
        "id": "nhQvANbwxN5r",
        "cellView": "form"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSosu3u-YIIA"
      },
      "outputs": [],
      "source": [
        "# N_RUNS = None\n",
        "N_RUNS = 1\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "wandb.agent(sweep_id, main, count=N_RUNS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y-J5mD_PTar9",
        "SIU-vXqDRW3L",
        "Op7oS8Jw1pgx"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNr21KRa66HE5XRZnmSrQfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}