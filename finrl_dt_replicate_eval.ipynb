{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony-pitchblack/finrl-dt/blob/custom-backtesting/finrl_dt_replicate_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjEBRlmyPp2a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stable-baselines3\n",
        "!pip install finrl\n",
        "!pip install alpaca_trade_api\n",
        "!pip install exchange_calendars\n",
        "!pip install stockstats\n",
        "!pip install wrds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "k_IxXsJXHbl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObUHWwfqnzyQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"aee284a72205e2d6787bd3ce266c5b9aefefa42c\"\n",
        "\n",
        "PROJECT = 'finrl-dt-replicate'\n",
        "ENTITY = \"overfit1010\""
      ],
      "metadata": {
        "id": "I9s6zvbUAsyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLe4hw1bxRp5"
      },
      "source": [
        "# Load models and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjRu97DCuXqR"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./*\n",
        "\n",
        "RUN_ID = '4resq86d'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download artifacts\n",
        "\n",
        "# Initialize the W&B API\n",
        "api = wandb.Api()\n",
        "\n",
        "# Retrieve the run\n",
        "run = api.run(f\"{ENTITY}/{PROJECT}/{RUN_ID}\")\n",
        "\n",
        "# Iterate over the artifacts used or logged by the run\n",
        "for artifact in run.logged_artifacts():\n",
        "    artifact.download(f'./{artifact.type}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YgyEOHM_06J",
        "outputId": "3c0da6af-14b1-4b8e-9a53-8857a7a613aa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   6 of 6 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   10 of 10 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4IS_7kYSEFo"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir ./results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXFSnGQgLFhT"
      },
      "source": [
        "# Eval models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title imports\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xBYg1cN8GS3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PuggN9OcFEo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load data\n",
        "\n",
        "train = pd.read_csv('./dataset/train_data.csv')\n",
        "trade = pd.read_csv('./dataset/test_data.csv')\n",
        "\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']\n",
        "trade = trade.set_index(trade.columns[0])\n",
        "trade.index.names = ['']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0-3fjeCG_GJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define metric functions\n",
        "\n",
        "def calculate_mdd(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the Maximum Drawdown (MDD) of a portfolio.\n",
        "    \"\"\"\n",
        "    running_max = asset_values.cummax()\n",
        "    drawdown = (asset_values - running_max) / running_max\n",
        "    mdd = drawdown.min() * 100  # Convert to percentage\n",
        "    return mdd\n",
        "\n",
        "def calculate_sharpe_ratio(asset_values, risk_free_rate=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe Ratio of a portfolio.\n",
        "    \"\"\"\n",
        "    # Calculate daily returns\n",
        "    returns = asset_values.pct_change().dropna()\n",
        "    excess_returns = returns - risk_free_rate / 252  # Assuming 252 trading days\n",
        "    if excess_returns.std() == 0:\n",
        "        return 0.0\n",
        "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252)  # Annualized\n",
        "    return sharpe_ratio\n",
        "\n",
        "def calculate_annualized_return(asset_values):\n",
        "    \"\"\"\n",
        "    Calculate the annualized return of a portfolio.\n",
        "    \"\"\"\n",
        "    # Assume `asset_values` is indexed by date or trading day\n",
        "    total_return = (asset_values.iloc[-1] / asset_values.iloc[0] - 1) * 100\n",
        "    num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "    annualized_return = (1 + total_return) ** (365 / num_days) - 1\n",
        "    return annualized_return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(entity=ENTITY, project=PROJECT, id=RUN_ID, resume='must')\n",
        "config = wandb.run.config"
      ],
      "metadata": {
        "id": "C6CzlEdpFvRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval default"
      ],
      "metadata": {
        "id": "M_55bSnxe_3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "phW2BVenGxds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d474a0da-2b29-4588-fdd4-4beeb82aefca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with A2C set to True\n",
            "Stock Dimension: 29, State Space: 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n",
            "\n",
            "=== Metrics Comparison for A2C ===\n",
            "Method Cumulative Return (%)      MDD (%) Sharpe Ratio\n",
            "   A2C            1.16 ± 0.0 -11.13 ± 0.0   0.23 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for A2C ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "A2C: 1.16%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "A2C: -11.13%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "A2C: 0.23\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_A2C_Expert.png'\n",
            "----------------------------------------\n",
            "Running with DDPG set to True\n",
            "Stock Dimension: 29, State Space: 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n",
            "\n",
            "=== Metrics Comparison for DDPG ===\n",
            "Method Cumulative Return (%)      MDD (%) Sharpe Ratio\n",
            "  DDPG            5.67 ± 0.0 -10.37 ± 0.0   0.81 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for DDPG ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "DDPG: 5.67%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "DDPG: -10.37%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "DDPG: 0.81\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_DDPG_Expert.png'\n",
            "----------------------------------------\n",
            "Running with PPO set to True\n",
            "Stock Dimension: 29, State Space: 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n",
            "\n",
            "=== Metrics Comparison for PPO ===\n",
            "Method Cumulative Return (%)     MDD (%) Sharpe Ratio\n",
            "   PPO           17.59 ± 0.0 -6.72 ± 0.0   2.81 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for PPO ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "PPO: 17.59%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "PPO: -6.72%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "PPO: 2.81\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_PPO_Expert.png'\n",
            "----------------------------------------\n",
            "Running with TD3 set to True\n",
            "Stock Dimension: 29, State Space: 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Metrics Comparison for TD3 ===\n",
            "Method Cumulative Return (%)     MDD (%) Sharpe Ratio\n",
            "   TD3            4.96 ± 0.0 -9.88 ± 0.0   0.75 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for TD3 ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "TD3: 4.96%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "TD3: -9.88%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "TD3: 0.75\n",
            "\n",
            "\n",
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_TD3_Expert.png'\n",
            "----------------------------------------\n",
            "Running with SAC set to True\n",
            "Stock Dimension: 29, State Space: 291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n",
            "\n",
            "=== Metrics Comparison for SAC ===\n",
            "Method Cumulative Return (%)     MDD (%) Sharpe Ratio\n",
            "   SAC           24.66 ± 0.0 -8.28 ± 0.0   2.36 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for SAC ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "SAC: 24.66%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "SAC: -8.28%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "SAC: 2.36\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_SAC_Expert.png'\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title EVAL default\n",
        "# comparison under expert agent\n",
        "\n",
        "os.makedirs('./checkpoints', exist_ok=True)\n",
        "\n",
        "algorithms = [\n",
        "    'a2c',\n",
        "    'ddpg',\n",
        "    'ppo',\n",
        "    'td3',\n",
        "    'sac'\n",
        "]\n",
        "\n",
        "# Define a mapping for better legend labels\n",
        "label_mapping = {\n",
        "    # 'DT_LoRA_GPT2': 'DT-LoRA-GPT2',\n",
        "    # 'DT_LoRA_Random_Weight_GPT2': 'DT-LoRA-Random-GPT2',\n",
        "    # 'CQL': 'Conservative Q-Learning',\n",
        "    # 'IQL': 'Implicit Q-Learning',\n",
        "    # 'BC': 'Behavior Cloning',\n",
        "    'A2C': 'A2C',\n",
        "    'DDPG': 'DDPG',\n",
        "    'PPO': 'PPO',\n",
        "    'TD3': 'TD3',\n",
        "    'SAC': 'SAC',\n",
        "    'DJIA': 'Dow Jones Index'\n",
        "}\n",
        "\n",
        "for current_algo in algorithms:\n",
        "    # Reset all algorithms to False\n",
        "    if_using_a2c = False\n",
        "    if_using_ddpg = False\n",
        "    if_using_ppo = False\n",
        "    if_using_td3 = False\n",
        "    if_using_sac = False\n",
        "\n",
        "    # Set the current algorithm to True\n",
        "    if current_algo == 'a2c':\n",
        "        if_using_a2c = True\n",
        "    elif current_algo == 'ddpg':\n",
        "        if_using_ddpg = True\n",
        "    elif current_algo == 'ppo':\n",
        "        if_using_ppo = True\n",
        "    elif current_algo == 'td3':\n",
        "        if_using_td3 = True\n",
        "    elif current_algo == 'sac':\n",
        "        if_using_sac = True\n",
        "\n",
        "    # Reset algos_included for each iteration\n",
        "    algos_included = ''\n",
        "\n",
        "    print(f\"Running with {current_algo.upper()} set to True\")\n",
        "\n",
        "    # Load trained models\n",
        "    trained_a2c = A2C.load(os.path.join(TRAINED_MODEL_DIR, \"agent_a2c\")) if if_using_a2c else None\n",
        "    trained_ddpg = DDPG.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ddpg\")) if if_using_ddpg else None\n",
        "    trained_ppo = PPO.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ppo\")) if if_using_ppo else None\n",
        "    trained_td3 = TD3.load(os.path.join(TRAINED_MODEL_DIR, \"agent_td3\")) if if_using_td3 else None\n",
        "    trained_sac = SAC.load(os.path.join(TRAINED_MODEL_DIR, \"agent_sac\")) if if_using_sac else None\n",
        "\n",
        "    # Define environment parameters\n",
        "    stock_dimension = len(trade.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
        "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "    num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": config['initial_amount'],\n",
        "        \"num_stock_shares\": num_stock_shares,\n",
        "        \"buy_cost_pct\": config['cost_pct'],\n",
        "        \"sell_cost_pct\": config['cost_pct'],\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4\n",
        "    }\n",
        "\n",
        "    # Initialize trading environment\n",
        "    e_trade_gym = StockTradingEnv(\n",
        "        df=trade,\n",
        "        turbulence_threshold=70, risk_indicator_col='vix',\n",
        "        **env_kwargs\n",
        "    )\n",
        "\n",
        "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
        "    # Predict using trained models\n",
        "    df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
        "        model=trained_a2c,\n",
        "        environment=e_trade_gym) if if_using_a2c else (None, None)\n",
        "\n",
        "    df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
        "        model=trained_ddpg,\n",
        "        environment=e_trade_gym) if if_using_ddpg else (None, None)\n",
        "\n",
        "    df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
        "        model=trained_td3,\n",
        "        environment=e_trade_gym) if if_using_td3 else (None, None)\n",
        "\n",
        "    df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
        "        model=trained_sac, environment=e_trade_gym) if if_using_sac else (None, None)\n",
        "\n",
        "    df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
        "        model=trained_ppo, environment=e_trade_gym) if if_using_ppo else (None, None)\n",
        "\n",
        "    # Set indices for result merging\n",
        "    df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0]) if if_using_a2c else None\n",
        "    df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0]) if if_using_ddpg else None\n",
        "    df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0]) if if_using_ppo else None\n",
        "    df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0]) if if_using_td3 else None\n",
        "    df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0]) if if_using_sac else None\n",
        "\n",
        "    # Merge results\n",
        "    result = pd.DataFrame()\n",
        "    if if_using_a2c:\n",
        "        algos_included += '_a2c'\n",
        "        df_result_a2c.columns = ['A2C_' + col for col in df_result_a2c.columns]\n",
        "        result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
        "    if if_using_ddpg:\n",
        "        algos_included += '_ddpg'\n",
        "        df_result_ddpg.columns = ['DDPG_' + col for col in df_result_ddpg.columns]\n",
        "        result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True)\n",
        "    if if_using_td3:\n",
        "        algos_included += '_td3'\n",
        "        df_result_td3.columns = ['TD3_' + col for col in df_result_td3.columns]\n",
        "        result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True)\n",
        "    if if_using_sac:\n",
        "        algos_included += '_sac'\n",
        "        df_result_sac.columns = ['SAC_' + col for col in df_result_sac.columns]\n",
        "        result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True)\n",
        "    if if_using_ppo:\n",
        "        algos_included += '_ppo'\n",
        "        df_result_ppo.columns = ['PPO_' + col for col in df_result_ppo.columns]\n",
        "        result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "    # Function to load pickle files\n",
        "    def load_pickle(file_path):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    # Get all directories under ./checkpoints/\n",
        "    checkpoint_dirs = [d for d in os.listdir('./checkpoints') if os.path.isdir(os.path.join('./checkpoints', d))]\n",
        "\n",
        "    # Process each checkpoint directory\n",
        "    for dir_name in checkpoint_dirs:\n",
        "        dir_path = os.path.join('./checkpoints', dir_name)\n",
        "        pkl_files = [f for f in os.listdir(dir_path) if f.startswith('total_asset_value_change_test') and f.endswith('.pkl')]\n",
        "\n",
        "        for pkl_file in pkl_files:\n",
        "            file_path = os.path.join(dir_path, pkl_file)\n",
        "            data = load_pickle(file_path)\n",
        "            data = data[:335]  # Limit data to first 335 points\n",
        "\n",
        "            # Validate data length\n",
        "            if len(data) != len(result.index):\n",
        "                print(f\"Warning: Data length mismatch for {pkl_file} in {dir_name}. Expected {len(result.index)}, got {len(data)}. Skipping this file.\")\n",
        "                continue\n",
        "\n",
        "            # Use directory name and pkl file name (without extension) as column name\n",
        "            column_name = f'{dir_name}_{os.path.splitext(pkl_file)[0]}'\n",
        "\n",
        "            df = pd.DataFrame(data, columns=[column_name])\n",
        "            df.index = result.index  # Assuming the dates align with the existing result dataframe\n",
        "            result = pd.merge(result, df, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "    # Create column names list with better formatting\n",
        "    col_name = []\n",
        "    if if_using_a2c: col_name.append('A2C')\n",
        "    if if_using_ddpg: col_name.append('DDPG')\n",
        "    if if_using_td3: col_name.append('TD3')\n",
        "    if if_using_sac: col_name.append('SAC')\n",
        "    if if_using_ppo: col_name.append('PPO')\n",
        "    col_name.extend(result.columns[len(col_name):])  # Add all other column names\n",
        "    result.columns = col_name\n",
        "\n",
        "    # Define test period\n",
        "    TEST_START_DATE = '2020-07-01'\n",
        "    TEST_END_DATE = '2021-10-29'\n",
        "\n",
        "    # Fetch DJIA data for the test period\n",
        "    df_dji = YahooDownloader(start_date=TEST_START_DATE,\n",
        "                             end_date=TEST_END_DATE,\n",
        "                             ticker_list=['dji']).fetch_data()\n",
        "\n",
        "    df_dji = df_dji[['date','close']]\n",
        "    fst_day = df_dji['close'].iloc[0]\n",
        "    dji = pd.DataFrame({\n",
        "        'DJIA': df_dji['close'].div(fst_day).mul(1000000).values\n",
        "    }, index=df_dji['date'])\n",
        "\n",
        "    # Merge DJIA data using inner join to ensure alignment\n",
        "    result = pd.merge(result, dji, how='inner', left_index=True, right_index=True).fillna(method='bfill')\n",
        "\n",
        "    # Control variables\n",
        "    include_ensemble = False  # Set to False to exclude ensemble experiments\n",
        "    exclude_algo_experiments = False  # Set to True to exclude individual algo experiments based on if_using_... flags\n",
        "\n",
        "    if include_ensemble:\n",
        "        algos_included += '_ensemble'\n",
        "\n",
        "    # Function to extract experiment name from directory name\n",
        "    def extract_experiment_name(dir_name):\n",
        "        # This regex matches everything up to the last underscore and number\n",
        "        match = re.match(r'(.+)_\\d+$', dir_name)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return dir_name\n",
        "\n",
        "    # Function to check if an experiment should be included\n",
        "    def should_include_experiment(exp_name):\n",
        "        if not include_ensemble and 'ensemble' in exp_name.lower():\n",
        "            return False\n",
        "        if exclude_algo_experiments:\n",
        "            algo_flags = {\n",
        "                'a2c': if_using_a2c,\n",
        "                'ddpg': if_using_ddpg,\n",
        "                'ppo': if_using_ppo,\n",
        "                'td3': if_using_td3,\n",
        "                'sac': if_using_sac\n",
        "            }\n",
        "            for algo, flag in algo_flags.items():\n",
        "                if algo in exp_name.lower() and not flag:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    # Group similar experiments\n",
        "    experiment_groups = {}\n",
        "    for column in result.columns:\n",
        "        if column not in ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO', 'DJIA']:\n",
        "            exp_name = extract_experiment_name(column.split('_total_asset_value_change_test')[0])\n",
        "            if should_include_experiment(exp_name):\n",
        "                if exp_name not in experiment_groups:\n",
        "                    experiment_groups[exp_name] = []\n",
        "                experiment_groups[exp_name].append(column)\n",
        "\n",
        "    # Initialize a dictionary to store metrics for comparison\n",
        "    metrics_dict = {\n",
        "        'Method': [],\n",
        "        'Cumulative Return Mean (%)': [],\n",
        "        'Cumulative Return Std (%)': [],\n",
        "        'MDD Mean (%)': [],\n",
        "        'MDD Std (%)': [],\n",
        "        'Sharpe Ratio Mean': [],\n",
        "        'Sharpe Ratio Std': []\n",
        "    }\n",
        "\n",
        "    # Calculate metrics for each experiment group\n",
        "    experiment_stats = {}\n",
        "    for exp_name, columns in experiment_groups.items():\n",
        "        exp_data = result[columns].dropna()\n",
        "\n",
        "        if exp_data.empty:\n",
        "            print(f\"Warning: No valid data for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "            continue\n",
        "\n",
        "        # Cumulative Return: (Final - Initial) / Initial * 100 for each run\n",
        "        cumulative_returns = (exp_data.iloc[-1] - exp_data.iloc[0]) / exp_data.iloc[0] * 100\n",
        "\n",
        "        # Handle potential division by zero or invalid calculations\n",
        "        cumulative_returns = cumulative_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if cumulative_returns.empty:\n",
        "            print(f\"Warning: No valid cumulative returns for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "            continue\n",
        "\n",
        "        cumulative_return_mean = cumulative_returns.mean()\n",
        "        cumulative_return_std = cumulative_returns.std()\n",
        "\n",
        "        # MDD: Calculate MDD for each run\n",
        "        mdd_values = []\n",
        "        for col in columns:\n",
        "            asset_values = result[col].dropna()\n",
        "            if asset_values.empty:\n",
        "                continue\n",
        "            mdd_run = calculate_mdd(asset_values)\n",
        "            mdd_values.append(mdd_run)\n",
        "\n",
        "        if not mdd_values:\n",
        "            print(f\"Warning: No valid MDD values for experiment '{exp_name}'. Skipping MDD calculation.\")\n",
        "            mdd_mean = np.nan\n",
        "            mdd_std = np.nan\n",
        "        else:\n",
        "            mdd_mean = np.mean(mdd_values)\n",
        "            mdd_std = np.std(mdd_values)\n",
        "\n",
        "        # Sharpe Ratio: Calculate Sharpe for each run\n",
        "        sharpe_ratios = []\n",
        "        for col in columns:\n",
        "            asset_values = result[col].dropna()\n",
        "            if asset_values.empty:\n",
        "                continue\n",
        "            sharpe_run = calculate_sharpe_ratio(asset_values)\n",
        "            sharpe_ratios.append(sharpe_run)\n",
        "\n",
        "        if not sharpe_ratios:\n",
        "            print(f\"Warning: No valid Sharpe Ratios for experiment '{exp_name}'. Skipping Sharpe Ratio calculation.\")\n",
        "            sharpe_mean = np.nan\n",
        "            sharpe_std = np.nan\n",
        "        else:\n",
        "            sharpe_mean = np.mean(sharpe_ratios)\n",
        "            sharpe_std = np.std(sharpe_ratios)\n",
        "\n",
        "        # Append to metrics_dict with mapped label\n",
        "        mapped_exp_name = label_mapping.get(exp_name, exp_name)\n",
        "        metrics_dict['Method'].append(mapped_exp_name)\n",
        "        metrics_dict['Cumulative Return Mean (%)'].append(cumulative_return_mean)\n",
        "        metrics_dict['Cumulative Return Std (%)'].append(cumulative_return_std)\n",
        "        metrics_dict['MDD Mean (%)'].append(mdd_mean)\n",
        "        metrics_dict['MDD Std (%)'].append(mdd_std)\n",
        "        metrics_dict['Sharpe Ratio Mean'].append(sharpe_mean)\n",
        "        metrics_dict['Sharpe Ratio Std'].append(sharpe_std)\n",
        "\n",
        "        # Store in experiment_stats for plotting\n",
        "        experiment_stats[mapped_exp_name] = {'mean': exp_data.mean(axis=1), 'std': exp_data.std(axis=1)}\n",
        "\n",
        "    # Calculate metrics for individual algorithms (A2C, DDPG, TD3, SAC, PPO)\n",
        "    individual_algos = ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO']\n",
        "    for algo in individual_algos:\n",
        "        if algo in result.columns:\n",
        "            # Check if this algorithm is already part of experiment_groups\n",
        "            if label_mapping.get(algo, algo) in experiment_stats:\n",
        "                print(f\"Info: '{algo}' is already included in experiment groups. Skipping individual plotting to avoid duplication.\")\n",
        "                continue  # Skip to prevent duplicate plotting\n",
        "\n",
        "            asset_values = result[algo].dropna()\n",
        "            if asset_values.empty:\n",
        "                print(f\"Warning: No valid asset values for individual algorithm '{algo}'. Skipping metrics calculation.\")\n",
        "                continue\n",
        "            # Cumulative Return\n",
        "            cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "            # Handle potential division by zero or invalid calculations\n",
        "            if np.isinf(cum_ret) or np.isnan(cum_ret):\n",
        "                cum_ret = np.nan\n",
        "            # MDD\n",
        "            mdd = calculate_mdd(asset_values)\n",
        "            # Sharpe Ratio\n",
        "            sharpe = calculate_sharpe_ratio(asset_values)\n",
        "            # Append to metrics_dict with mapped label\n",
        "            mapped_algo = label_mapping.get(algo, algo)\n",
        "            metrics_dict['Method'].append(mapped_algo)\n",
        "            metrics_dict['Cumulative Return Mean (%)'].append(cum_ret)\n",
        "            metrics_dict['Cumulative Return Std (%)'].append(0.00)  # Single run, std is 0\n",
        "            metrics_dict['MDD Mean (%)'].append(mdd)\n",
        "            metrics_dict['MDD Std (%)'].append(0.00)  # Single run, std is 0\n",
        "            metrics_dict['Sharpe Ratio Mean'].append(sharpe)\n",
        "            metrics_dict['Sharpe Ratio Std'].append(0.00)  # Single run, std is 0\n",
        "\n",
        "            # Store in experiment_stats for plotting\n",
        "            experiment_stats[mapped_algo] = {'mean': asset_values, 'std': pd.Series([0]*len(asset_values), index=asset_values.index)}\n",
        "\n",
        "    # Convert metrics_dict to DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics_dict)\n",
        "\n",
        "    # Drop any rows with NaN metrics to ensure clean tables\n",
        "    metrics_df = metrics_df.dropna(subset=['Cumulative Return Mean (%)', 'MDD Mean (%)', 'Sharpe Ratio Mean'])\n",
        "\n",
        "    # Create summary DataFrame with formatted strings\n",
        "    metrics_summary_df = metrics_df.copy()\n",
        "    metrics_summary_df['Cumulative Return (%)'] = metrics_df['Cumulative Return Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['Cumulative Return Std (%)'].round(2).astype(str)\n",
        "    metrics_summary_df['MDD (%)'] = metrics_df['MDD Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['MDD Std (%)'].round(2).astype(str)\n",
        "    metrics_summary_df['Sharpe Ratio'] = metrics_df['Sharpe Ratio Mean'].round(2).astype(str) + \" ± \" + metrics_df['Sharpe Ratio Std'].round(2).astype(str)\n",
        "    metrics_summary_df = metrics_summary_df[['Method', 'Cumulative Return (%)', 'MDD (%)', 'Sharpe Ratio']]\n",
        "\n",
        "    # Print the comparison table\n",
        "    print(f\"\\n=== Metrics Comparison for {current_algo.upper()} ===\")\n",
        "    print(metrics_summary_df.to_string(index=False))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Create separate DataFrames for ranking\n",
        "    ranking_cum_ret = metrics_df[['Method', 'Cumulative Return Mean (%)']].copy()\n",
        "    ranking_cum_ret = ranking_cum_ret.sort_values(by='Cumulative Return Mean (%)', ascending=False)\n",
        "\n",
        "    ranking_mdd = metrics_df[['Method', 'MDD Mean (%)']].copy()\n",
        "    ranking_mdd = ranking_mdd.sort_values(by='MDD Mean (%)', ascending=True)  # Lower MDD is better\n",
        "\n",
        "    ranking_sharpe = metrics_df[['Method', 'Sharpe Ratio Mean']].copy()\n",
        "    ranking_sharpe = ranking_sharpe.sort_values(by='Sharpe Ratio Mean', ascending=False)\n",
        "\n",
        "    # Print rankings\n",
        "    print(f\"=== Rankings for {current_algo.upper()} ===\")\n",
        "\n",
        "    print(\"\\nCumulative Return (%):\")\n",
        "    for idx, row in ranking_cum_ret.iterrows():\n",
        "        print(f\"{row['Method']}: {row['Cumulative Return Mean (%)']:.2f}%\")\n",
        "\n",
        "    print(\"\\nMaximum Drawdown (MDD %) [Lower is Better]:\")\n",
        "    for idx, row in ranking_mdd.iterrows():\n",
        "        print(f\"{row['Method']}: {row['MDD Mean (%)']:.2f}%\")\n",
        "\n",
        "    print(\"\\nSharpe Ratio [Higher is Better]:\")\n",
        "    for idx, row in ranking_sharpe.iterrows():\n",
        "        print(f\"{row['Method']}: {row['Sharpe Ratio Mean']:.2f}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Debugging: Check if all means align with result.index\n",
        "    for exp_name, stats in experiment_stats.items():\n",
        "        mean_length = len(stats['mean'])\n",
        "        result_length = len(result.index)\n",
        "        if mean_length != result_length:\n",
        "            print(f\"Warning: Mean length for '{exp_name}' ({mean_length}) does not match result index length ({result_length}). Reindexing.\")\n",
        "            experiment_stats[exp_name]['mean'] = stats['mean'].reindex(result.index).fillna(method='ffill')\n",
        "            experiment_stats[exp_name]['std'] = stats['std'].reindex(result.index).fillna(0)\n",
        "\n",
        "    # Plotting section\n",
        "    plt.figure(figsize=(16, 9))  # Increased figure size for better readability\n",
        "    method_styles = {\n",
        "        'CQL': {'color': '#1f77b4', 'linestyle': '-'},           # Blue solid\n",
        "        'IQL': {'color': '#ff7f0e', 'linestyle': '--'},          # Orange dashed\n",
        "        'BC': {'color': '#2ca02c', 'linestyle': '-.'},           # Green dash-dot\n",
        "        'DT LoRA GPT2': {'color': '#d62728', 'linestyle': ':'},  # Red dotted\n",
        "        'DT LoRA Random Weight GPT2': {'color': '#9467bd', 'linestyle': '-'},  # Purple solid\n",
        "        'A2C': {'color': '#8c564b', 'linestyle': '--'},          # Brown dashed\n",
        "        'DDPG': {'color': '#e377c2', 'linestyle': '-'},          # Pink solid\n",
        "        'PPO': {'color': '#7f7f7f', 'linestyle': '-'},           # Gray solid\n",
        "        'TD3': {'color': '#bcbd22', 'linestyle': '--'},          # Olive dashed\n",
        "        'SAC': {'color': '#17becf', 'linestyle': '-'},           # Cyan solid\n",
        "        'DJIA': {'color': '#000000', 'linestyle': '-'},          # Black solid\n",
        "        # Add more methods here if needed\n",
        "    }\n",
        "    # Plot DJIA\n",
        "    plt.plot(result.index, result['DJIA'], label=\"Dow Jones Index\", linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "    # Define color palette and line styles\n",
        "    color_palette = plt.get_cmap('tab10').colors  # Colorblind-friendly palette\n",
        "    line_styles = ['-', '--', '-.', ':']  # Different line styles\n",
        "\n",
        "    # Plot experiment groups\n",
        "    for idx, (exp_name, stats) in enumerate(experiment_stats.items()):\n",
        "        mean = stats['mean']\n",
        "        std = stats['std']\n",
        "\n",
        "        # Ensure mean and std are aligned with result.index\n",
        "        mean = mean.reindex(result.index).fillna(method='ffill')\n",
        "        std = std.reindex(result.index).fillna(0)\n",
        "\n",
        "        # Assign colors and line styles\n",
        "        # color = color_palette[idx % len(color_palette)]\n",
        "        # linestyle = line_styles[idx % len(line_styles)]\n",
        "\n",
        "        def exp_name_formatter(exp_name):\n",
        "            exp_names = exp_name.split('_')\n",
        "            if len(exp_names) == 1:\n",
        "                return exp_name\n",
        "            elif len(exp_names) == 2:\n",
        "                return exp_names[1].upper()\n",
        "            elif len(exp_names) == 3:\n",
        "                return None\n",
        "            elif len(exp_names) == 4:\n",
        "                return exp_names[1].upper() + ' LoRA ' + 'GPT2'\n",
        "            elif len(exp_names) == 6:\n",
        "                return exp_names[1].upper() + ' LoRA ' + 'Random Weight ' + 'GPT2'\n",
        "            else:\n",
        "                return exp_name\n",
        "\n",
        "        # Plot mean\n",
        "        line, = plt.plot(result.index, mean, label=exp_name_formatter(exp_name), linestyle=method_styles[exp_name_formatter(exp_name)]['linestyle'], color=method_styles[exp_name_formatter(exp_name)]['color'])\n",
        "\n",
        "        # Plot error bandsy (mean ± 1 std)\n",
        "        plt.fill_between(result.index, mean - std, mean + std, color=method_styles[exp_name_formatter(exp_name)]['color'], alpha=0.2)\n",
        "\n",
        "    # Set title and labels with enhanced formatting\n",
        "    plt.title(f\"Performance Comparison Under {current_algo.upper()} Expert Agent\", fontsize=20, fontweight='bold')\n",
        "    plt.xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "    plt.ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # import matplotlib.dates as mdates\n",
        "    plt.xticks(result.index[0::30])\n",
        "    # Add 'Test Phase' annotation with date range\n",
        "    plt.text(0.5, 0.95, 'Test Phase: July 1, 2020 - October 29, 2021',\n",
        "             transform=plt.gca().transAxes, fontsize=14, ha='center',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.5))\n",
        "\n",
        "    # After all lines are plotted, sort the legend alphabetically\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    sorted_pairs = sorted(zip(labels, handles), key=lambda t: t[0].lower())  # Sort alphabetically, case-insensitive\n",
        "    sorted_labels, sorted_handles = zip(*sorted_pairs)\n",
        "\n",
        "    # Position legend outside the plot with sorted items\n",
        "    plt.legend(sorted_handles, sorted_labels, loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=12)\n",
        "\n",
        "    # Enhance layout and aesthetics\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Save the plot with an informative filename\n",
        "    plt.savefig(f'performance_comparison_DT-LoRA-GPT2_{current_algo.upper()}_Expert.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Results saved as 'performance_comparison_DT-LoRA-GPT2_{current_algo.upper()}_Expert.png'\")\n",
        "    print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "5U8J6GqaI3-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval RL only"
      ],
      "metadata": {
        "id": "Qh7UZGp3e7dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init env\n",
        "\n",
        "# Define environment parameters\n",
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": config['initial_amount'],\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": config['cost_pct'],\n",
        "    \"sell_cost_pct\": config['cost_pct'],\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "# Initialize trading environment\n",
        "e_trade_gym = StockTradingEnv(\n",
        "    df=trade,\n",
        "    turbulence_threshold=70, risk_indicator_col='vix',\n",
        "    **env_kwargs\n",
        ")\n",
        "\n",
        "env_trade, obs_trade = e_trade_gym.get_sb_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yJBp5dD3dKWQ",
        "outputId": "a6ec4f29-fcd0-42b6-c904-a8c8266181c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load models\n",
        "# os.makedirs('./checkpoints', exist_ok=True)\n",
        "\n",
        "algorithms = [\n",
        "    'a2c',\n",
        "    'ddpg',\n",
        "    'ppo',\n",
        "    'td3',\n",
        "    'sac'\n",
        "]\n",
        "\n",
        "# Define a mapping for better legend labels\n",
        "label_mapping = {\n",
        "    # 'DT_LoRA_GPT2': 'DT-LoRA-GPT2',\n",
        "    # 'DT_LoRA_Random_Weight_GPT2': 'DT-LoRA-Random-GPT2',\n",
        "    # 'CQL': 'Conservative Q-Learning',\n",
        "    # 'IQL': 'Implicit Q-Learning',\n",
        "    # 'BC': 'Behavior Cloning',\n",
        "    'A2C': 'A2C',\n",
        "    'DDPG': 'DDPG',\n",
        "    'PPO': 'PPO',\n",
        "    'TD3': 'TD3',\n",
        "    'SAC': 'SAC',\n",
        "    'DJIA': 'Dow Jones Index'\n",
        "}\n",
        "\n",
        "for current_algo in algorithms:\n",
        "    # Reset all algorithms to False\n",
        "    if_using_a2c = False\n",
        "    if_using_ddpg = False\n",
        "    if_using_ppo = False\n",
        "    if_using_td3 = False\n",
        "    if_using_sac = False\n",
        "\n",
        "    # Set the current algorithm to True\n",
        "    if current_algo == 'a2c':\n",
        "        if_using_a2c = True\n",
        "    elif current_algo == 'ddpg':\n",
        "        if_using_ddpg = True\n",
        "    elif current_algo == 'ppo':\n",
        "        if_using_ppo = True\n",
        "    elif current_algo == 'td3':\n",
        "        if_using_td3 = True\n",
        "    elif current_algo == 'sac':\n",
        "        if_using_sac = True\n",
        "\n",
        "    # Reset algos_included for each iteration\n",
        "    algos_included = ''\n",
        "\n",
        "    print(f\"Running with {current_algo.upper()} set to True\")\n",
        "\n",
        "# Load trained models\n",
        "trained_a2c = A2C.load(os.path.join(TRAINED_MODEL_DIR, \"agent_a2c\")) if if_using_a2c else None\n",
        "trained_ddpg = DDPG.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ddpg\")) if if_using_ddpg else None\n",
        "trained_ppo = PPO.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ppo\")) if if_using_ppo else None\n",
        "trained_td3 = TD3.load(os.path.join(TRAINED_MODEL_DIR, \"agent_td3\")) if if_using_td3 else None\n",
        "trained_sac = SAC.load(os.path.join(TRAINED_MODEL_DIR, \"agent_sac\")) if if_using_sac else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d974832-2792-4406-a42c-92c4a891972b",
        "cellView": "form",
        "id": "7nlQ8gmPd7K9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with A2C set to True\n",
            "Running with DDPG set to True\n",
            "Running with PPO set to True\n",
            "Running with TD3 set to True\n",
            "Running with SAC set to True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tEgROYMTd4kR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43086fd8-7800-4c01-d14a-784af86f7bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "hit end!\n",
            "hit end!\n",
            "hit end!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "Shape of DataFrame:  (319, 8)\n",
            "experiment_groups {}\n",
            "\n",
            "=== Metrics Comparison for SAC ===\n",
            "Method Cumulative Return (%)      MDD (%) Sharpe Ratio\n",
            "   A2C           14.11 ± 0.0  -7.53 ± 0.0   1.89 ± 0.0\n",
            "  DDPG           15.45 ± 0.0  -8.62 ± 0.0   1.95 ± 0.0\n",
            "   TD3           14.66 ± 0.0 -11.75 ± 0.0   1.54 ± 0.0\n",
            "   SAC           24.68 ± 0.0  -8.36 ± 0.0   2.36 ± 0.0\n",
            "   PPO           14.92 ± 0.0  -9.13 ± 0.0    2.1 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings for SAC ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "SAC: 24.68%\n",
            "DDPG: 15.45%\n",
            "PPO: 14.92%\n",
            "TD3: 14.66%\n",
            "A2C: 14.11%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "TD3: -11.75%\n",
            "PPO: -9.13%\n",
            "DDPG: -8.62%\n",
            "SAC: -8.36%\n",
            "A2C: -7.53%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "SAC: 2.36\n",
            "PPO: 2.10\n",
            "DDPG: 1.95\n",
            "A2C: 1.89\n",
            "TD3: 1.54\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved as 'performance_comparison_DT-LoRA-GPT2_SAC_Expert.png'\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#@title eval strategy\n",
        "\n",
        "# Predict using trained models\n",
        "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
        "    model=trained_a2c,\n",
        "    environment=e_trade_gym) if if_using_a2c else (None, None)\n",
        "\n",
        "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
        "    model=trained_ddpg,\n",
        "    environment=e_trade_gym) if if_using_ddpg else (None, None)\n",
        "\n",
        "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
        "    model=trained_td3,\n",
        "    environment=e_trade_gym) if if_using_td3 else (None, None)\n",
        "\n",
        "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
        "    model=trained_sac, environment=e_trade_gym) if if_using_sac else (None, None)\n",
        "\n",
        "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
        "    model=trained_ppo, environment=e_trade_gym) if if_using_ppo else (None, None)\n",
        "\n",
        "# Set indices for result merging\n",
        "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0]) if if_using_a2c else None\n",
        "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0]) if if_using_ddpg else None\n",
        "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0]) if if_using_ppo else None\n",
        "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0]) if if_using_td3 else None\n",
        "df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0]) if if_using_sac else None\n",
        "\n",
        "# Merge results\n",
        "result = pd.DataFrame()\n",
        "if if_using_a2c:\n",
        "    algos_included += '_a2c'\n",
        "    df_result_a2c.columns = ['A2C_' + col for col in df_result_a2c.columns]\n",
        "    result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
        "if if_using_ddpg:\n",
        "    algos_included += '_ddpg'\n",
        "    df_result_ddpg.columns = ['DDPG_' + col for col in df_result_ddpg.columns]\n",
        "    result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True)\n",
        "if if_using_td3:\n",
        "    algos_included += '_td3'\n",
        "    df_result_td3.columns = ['TD3_' + col for col in df_result_td3.columns]\n",
        "    result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True)\n",
        "if if_using_sac:\n",
        "    algos_included += '_sac'\n",
        "    df_result_sac.columns = ['SAC_' + col for col in df_result_sac.columns]\n",
        "    result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True)\n",
        "if if_using_ppo:\n",
        "    algos_included += '_ppo'\n",
        "    df_result_ppo.columns = ['PPO_' + col for col in df_result_ppo.columns]\n",
        "    result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "# Function to load pickle files\n",
        "def load_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Get all directories under ./checkpoints/\n",
        "checkpoint_dirs = [d for d in os.listdir('./checkpoints') if os.path.isdir(os.path.join('./checkpoints', d))]\n",
        "\n",
        "# Process each checkpoint directory\n",
        "for dir_name in checkpoint_dirs:\n",
        "    dir_path = os.path.join('./checkpoints', dir_name)\n",
        "    pkl_files = [f for f in os.listdir(dir_path) if f.startswith('total_asset_value_change_test') and f.endswith('.pkl')]\n",
        "\n",
        "    for pkl_file in pkl_files:\n",
        "        file_path = os.path.join(dir_path, pkl_file)\n",
        "        data = load_pickle(file_path)\n",
        "        data = data[:335]  # Limit data to first 335 points\n",
        "\n",
        "        # Validate data length\n",
        "        if len(data) != len(result.index):\n",
        "            print(f\"Warning: Data length mismatch for {pkl_file} in {dir_name}. Expected {len(result.index)}, got {len(data)}. Skipping this file.\")\n",
        "            continue\n",
        "\n",
        "        # Use directory name and pkl file name (without extension) as column name\n",
        "        column_name = f'{dir_name}_{os.path.splitext(pkl_file)[0]}'\n",
        "\n",
        "        df = pd.DataFrame(data, columns=[column_name])\n",
        "        df.index = result.index  # Assuming the dates align with the existing result dataframe\n",
        "        result = pd.merge(result, df, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "# Create column names list with better formatting\n",
        "col_name = []\n",
        "if if_using_a2c: col_name.append('A2C')\n",
        "if if_using_ddpg: col_name.append('DDPG')\n",
        "if if_using_td3: col_name.append('TD3')\n",
        "if if_using_sac: col_name.append('SAC')\n",
        "if if_using_ppo: col_name.append('PPO')\n",
        "col_name.extend(result.columns[len(col_name):])  # Add all other column names\n",
        "result.columns = col_name\n",
        "\n",
        "# Define test period\n",
        "TEST_START_DATE = '2020-07-01'\n",
        "TEST_END_DATE = '2021-10-29'\n",
        "\n",
        "# Fetch DJIA data for the test period\n",
        "df_dji = YahooDownloader(start_date=TEST_START_DATE,\n",
        "                            end_date=TEST_END_DATE,\n",
        "                            ticker_list=['dji']).fetch_data()\n",
        "\n",
        "df_dji = df_dji[['date','close']]\n",
        "fst_day = df_dji['close'].iloc[0]\n",
        "dji = pd.DataFrame({\n",
        "    'DJIA': df_dji['close'].div(fst_day).mul(1000000).values\n",
        "}, index=df_dji['date'])\n",
        "\n",
        "# Merge DJIA data using inner join to ensure alignment\n",
        "result = pd.merge(result, dji, how='inner', left_index=True, right_index=True).fillna(method='bfill')\n",
        "\n",
        "# Control variables\n",
        "include_ensemble = False  # Set to False to exclude ensemble experiments\n",
        "exclude_algo_experiments = False  # Set to True to exclude individual algo experiments based on if_using_... flags\n",
        "\n",
        "if include_ensemble:\n",
        "    algos_included += '_ensemble'\n",
        "\n",
        "# Function to extract experiment name from directory name\n",
        "def extract_experiment_name(dir_name):\n",
        "    # This regex matches everything up to the last underscore and number\n",
        "    match = re.match(r'(.+)_\\d+$', dir_name)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return dir_name\n",
        "\n",
        "# Function to check if an experiment should be included\n",
        "def should_include_experiment(exp_name):\n",
        "    if not include_ensemble and 'ensemble' in exp_name.lower():\n",
        "        return False\n",
        "    if exclude_algo_experiments:\n",
        "        algo_flags = {\n",
        "            'a2c': if_using_a2c,\n",
        "            'ddpg': if_using_ddpg,\n",
        "            'ppo': if_using_ppo,\n",
        "            'td3': if_using_td3,\n",
        "            'sac': if_using_sac\n",
        "        }\n",
        "        for algo, flag in algo_flags.items():\n",
        "            if algo in exp_name.lower() and not flag:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# Group similar experiments\n",
        "experiment_groups = {}\n",
        "for column in result.columns:\n",
        "    if column not in ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO', 'DJIA']:\n",
        "        exp_name = extract_experiment_name(column.split('_total_asset_value_change_test')[0])\n",
        "        if should_include_experiment(exp_name):\n",
        "            if exp_name not in experiment_groups:\n",
        "                experiment_groups[exp_name] = []\n",
        "            experiment_groups[exp_name].append(column)\n",
        "\n",
        "print('experiment_groups', experiment_groups)\n",
        "\n",
        "# Initialize a dictionary to store metrics for comparison\n",
        "metrics_dict = {\n",
        "    'Method': [],\n",
        "    'Cumulative Return Mean (%)': [],\n",
        "    'Cumulative Return Std (%)': [],\n",
        "    'MDD Mean (%)': [],\n",
        "    'MDD Std (%)': [],\n",
        "    'Sharpe Ratio Mean': [],\n",
        "    'Sharpe Ratio Std': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each experiment group\n",
        "experiment_stats = {}\n",
        "for exp_name, columns in experiment_groups.items():\n",
        "    exp_data = result[columns].dropna()\n",
        "\n",
        "    if exp_data.empty:\n",
        "        print(f\"Warning: No valid data for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "        continue\n",
        "\n",
        "    # Cumulative Return: (Final - Initial) / Initial * 100 for each run\n",
        "    cumulative_returns = (exp_data.iloc[-1] - exp_data.iloc[0]) / exp_data.iloc[0] * 100\n",
        "\n",
        "    # Handle potential division by zero or invalid calculations\n",
        "    cumulative_returns = cumulative_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    if cumulative_returns.empty:\n",
        "        print(f\"Warning: No valid cumulative returns for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "        continue\n",
        "\n",
        "    cumulative_return_mean = cumulative_returns.mean()\n",
        "    cumulative_return_std = cumulative_returns.std()\n",
        "\n",
        "    # MDD: Calculate MDD for each run\n",
        "    mdd_values = []\n",
        "    for col in columns:\n",
        "        asset_values = result[col].dropna()\n",
        "        if asset_values.empty:\n",
        "            continue\n",
        "        mdd_run = calculate_mdd(asset_values)\n",
        "        mdd_values.append(mdd_run)\n",
        "\n",
        "    if not mdd_values:\n",
        "        print(f\"Warning: No valid MDD values for experiment '{exp_name}'. Skipping MDD calculation.\")\n",
        "        mdd_mean = np.nan\n",
        "        mdd_std = np.nan\n",
        "    else:\n",
        "        mdd_mean = np.mean(mdd_values)\n",
        "        mdd_std = np.std(mdd_values)\n",
        "\n",
        "    # Sharpe Ratio: Calculate Sharpe for each run\n",
        "    sharpe_ratios = []\n",
        "    for col in columns:\n",
        "        asset_values = result[col].dropna()\n",
        "        if asset_values.empty:\n",
        "            continue\n",
        "        sharpe_run = calculate_sharpe_ratio(asset_values)\n",
        "        sharpe_ratios.append(sharpe_run)\n",
        "\n",
        "    if not sharpe_ratios:\n",
        "        print(f\"Warning: No valid Sharpe Ratios for experiment '{exp_name}'. Skipping Sharpe Ratio calculation.\")\n",
        "        sharpe_mean = np.nan\n",
        "        sharpe_std = np.nan\n",
        "    else:\n",
        "        sharpe_mean = np.mean(sharpe_ratios)\n",
        "        sharpe_std = np.std(sharpe_ratios)\n",
        "\n",
        "    # Append to metrics_dict with mapped label\n",
        "    mapped_exp_name = label_mapping.get(exp_name, exp_name)\n",
        "    metrics_dict['Method'].append(mapped_exp_name)\n",
        "    metrics_dict['Cumulative Return Mean (%)'].append(cumulative_return_mean)\n",
        "    metrics_dict['Cumulative Return Std (%)'].append(cumulative_return_std)\n",
        "    metrics_dict['MDD Mean (%)'].append(mdd_mean)\n",
        "    metrics_dict['MDD Std (%)'].append(mdd_std)\n",
        "    metrics_dict['Sharpe Ratio Mean'].append(sharpe_mean)\n",
        "    metrics_dict['Sharpe Ratio Std'].append(sharpe_std)\n",
        "\n",
        "    # Store in experiment_stats for plotting\n",
        "    experiment_stats[mapped_exp_name] = {'mean': exp_data.mean(axis=1), 'std': exp_data.std(axis=1)}\n",
        "\n",
        "# Calculate metrics for individual algorithms (A2C, DDPG, TD3, SAC, PPO)\n",
        "individual_algos = ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO']\n",
        "for algo in individual_algos:\n",
        "    if algo in result.columns:\n",
        "        # Check if this algorithm is already part of experiment_groups\n",
        "        if label_mapping.get(algo, algo) in experiment_stats:\n",
        "            print(f\"Info: '{algo}' is already included in experiment groups. Skipping individual plotting to avoid duplication.\")\n",
        "            continue  # Skip to prevent duplicate plotting\n",
        "\n",
        "        asset_values = result[algo].dropna()\n",
        "        if asset_values.empty:\n",
        "            print(f\"Warning: No valid asset values for individual algorithm '{algo}'. Skipping metrics calculation.\")\n",
        "            continue\n",
        "        # Cumulative Return\n",
        "        cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "        # Handle potential division by zero or invalid calculations\n",
        "        if np.isinf(cum_ret) or np.isnan(cum_ret):\n",
        "            cum_ret = np.nan\n",
        "        # MDD\n",
        "        mdd = calculate_mdd(asset_values)\n",
        "        # Sharpe Ratio\n",
        "        sharpe = calculate_sharpe_ratio(asset_values)\n",
        "        # Append to metrics_dict with mapped label\n",
        "        mapped_algo = label_mapping.get(algo, algo)\n",
        "        metrics_dict['Method'].append(mapped_algo)\n",
        "        metrics_dict['Cumulative Return Mean (%)'].append(cum_ret)\n",
        "        metrics_dict['Cumulative Return Std (%)'].append(0.00)  # Single run, std is 0\n",
        "        metrics_dict['MDD Mean (%)'].append(mdd)\n",
        "        metrics_dict['MDD Std (%)'].append(0.00)  # Single run, std is 0\n",
        "        metrics_dict['Sharpe Ratio Mean'].append(sharpe)\n",
        "        metrics_dict['Sharpe Ratio Std'].append(0.00)  # Single run, std is 0\n",
        "\n",
        "        # Store in experiment_stats for plotting\n",
        "        experiment_stats[mapped_algo] = {'mean': asset_values, 'std': pd.Series([0]*len(asset_values), index=asset_values.index)}\n",
        "\n",
        "# Convert metrics_dict to DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_dict)\n",
        "\n",
        "# Drop any rows with NaN metrics to ensure clean tables\n",
        "metrics_df = metrics_df.dropna(subset=['Cumulative Return Mean (%)', 'MDD Mean (%)', 'Sharpe Ratio Mean'])\n",
        "\n",
        "# Create summary DataFrame with formatted strings\n",
        "metrics_summary_df = metrics_df.copy()\n",
        "metrics_summary_df['Cumulative Return (%)'] = metrics_df['Cumulative Return Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['Cumulative Return Std (%)'].round(2).astype(str)\n",
        "metrics_summary_df['MDD (%)'] = metrics_df['MDD Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['MDD Std (%)'].round(2).astype(str)\n",
        "metrics_summary_df['Sharpe Ratio'] = metrics_df['Sharpe Ratio Mean'].round(2).astype(str) + \" ± \" + metrics_df['Sharpe Ratio Std'].round(2).astype(str)\n",
        "metrics_summary_df = metrics_summary_df[['Method', 'Cumulative Return (%)', 'MDD (%)', 'Sharpe Ratio']]\n",
        "\n",
        "# Print the comparison table\n",
        "print(f\"\\n=== Metrics Comparison for {current_algo.upper()} ===\")\n",
        "print(metrics_summary_df.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create separate DataFrames for ranking\n",
        "ranking_cum_ret = metrics_df[['Method', 'Cumulative Return Mean (%)']].copy()\n",
        "ranking_cum_ret = ranking_cum_ret.sort_values(by='Cumulative Return Mean (%)', ascending=False)\n",
        "\n",
        "ranking_mdd = metrics_df[['Method', 'MDD Mean (%)']].copy()\n",
        "ranking_mdd = ranking_mdd.sort_values(by='MDD Mean (%)', ascending=True)  # Lower MDD is better\n",
        "\n",
        "ranking_sharpe = metrics_df[['Method', 'Sharpe Ratio Mean']].copy()\n",
        "ranking_sharpe = ranking_sharpe.sort_values(by='Sharpe Ratio Mean', ascending=False)\n",
        "\n",
        "# Print rankings\n",
        "print(f\"=== Rankings for {current_algo.upper()} ===\")\n",
        "\n",
        "print(\"\\nCumulative Return (%):\")\n",
        "for idx, row in ranking_cum_ret.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Cumulative Return Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nMaximum Drawdown (MDD %) [Lower is Better]:\")\n",
        "for idx, row in ranking_mdd.iterrows():\n",
        "    print(f\"{row['Method']}: {row['MDD Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nSharpe Ratio [Higher is Better]:\")\n",
        "for idx, row in ranking_sharpe.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Sharpe Ratio Mean']:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Debugging: Check if all means align with result.index\n",
        "for exp_name, stats in experiment_stats.items():\n",
        "    mean_length = len(stats['mean'])\n",
        "    result_length = len(result.index)\n",
        "    if mean_length != result_length:\n",
        "        print(f\"Warning: Mean length for '{exp_name}' ({mean_length}) does not match result index length ({result_length}). Reindexing.\")\n",
        "        experiment_stats[exp_name]['mean'] = stats['mean'].reindex(result.index).fillna(method='ffill')\n",
        "        experiment_stats[exp_name]['std'] = stats['std'].reindex(result.index).fillna(0)\n",
        "\n",
        "# Plotting section\n",
        "plt.figure(figsize=(16, 9))  # Increased figure size for better readability\n",
        "method_styles = {\n",
        "    'CQL': {'color': '#1f77b4', 'linestyle': '-'},           # Blue solid\n",
        "    'IQL': {'color': '#ff7f0e', 'linestyle': '--'},          # Orange dashed\n",
        "    'BC': {'color': '#2ca02c', 'linestyle': '-.'},           # Green dash-dot\n",
        "    'DT LoRA GPT2': {'color': '#d62728', 'linestyle': ':'},  # Red dotted\n",
        "    'DT LoRA Random Weight GPT2': {'color': '#9467bd', 'linestyle': '-'},  # Purple solid\n",
        "    'A2C': {'color': '#8c564b', 'linestyle': '--'},          # Brown dashed\n",
        "    'DDPG': {'color': '#e377c2', 'linestyle': '-'},          # Pink solid\n",
        "    'PPO': {'color': '#7f7f7f', 'linestyle': '-'},           # Gray solid\n",
        "    'TD3': {'color': '#bcbd22', 'linestyle': '--'},          # Olive dashed\n",
        "    'SAC': {'color': '#17becf', 'linestyle': '-'},           # Cyan solid\n",
        "    'DJIA': {'color': '#000000', 'linestyle': '-'},          # Black solid\n",
        "    # Add more methods here if needed\n",
        "}\n",
        "# Plot DJIA\n",
        "plt.plot(result.index, result['DJIA'], label=\"Dow Jones Index\", linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "# Define color palette and line styles\n",
        "color_palette = plt.get_cmap('tab10').colors  # Colorblind-friendly palette\n",
        "line_styles = ['-', '--', '-.', ':']  # Different line styles\n",
        "\n",
        "# Plot experiment groups\n",
        "for idx, (exp_name, stats) in enumerate(experiment_stats.items()):\n",
        "    mean = stats['mean']\n",
        "    std = stats['std']\n",
        "\n",
        "    # Ensure mean and std are aligned with result.index\n",
        "    mean = mean.reindex(result.index).fillna(method='ffill')\n",
        "    std = std.reindex(result.index).fillna(0)\n",
        "\n",
        "    # Assign colors and line styles\n",
        "    # color = color_palette[idx % len(color_palette)]\n",
        "    # linestyle = line_styles[idx % len(line_styles)]\n",
        "\n",
        "    def exp_name_formatter(exp_name):\n",
        "        exp_names = exp_name.split('_')\n",
        "        if len(exp_names) == 1:\n",
        "            return exp_name\n",
        "        elif len(exp_names) == 2:\n",
        "            return exp_names[1].upper()\n",
        "        elif len(exp_names) == 3:\n",
        "            return None\n",
        "        elif len(exp_names) == 4:\n",
        "            return exp_names[1].upper() + ' LoRA ' + 'GPT2'\n",
        "        elif len(exp_names) == 6:\n",
        "            return exp_names[1].upper() + ' LoRA ' + 'Random Weight ' + 'GPT2'\n",
        "        else:\n",
        "            return exp_name\n",
        "\n",
        "    # Plot mean\n",
        "    line, = plt.plot(result.index, mean, label=exp_name_formatter(exp_name), linestyle=method_styles[exp_name_formatter(exp_name)]['linestyle'], color=method_styles[exp_name_formatter(exp_name)]['color'])\n",
        "\n",
        "    # Plot error bandsy (mean ± 1 std)\n",
        "    plt.fill_between(result.index, mean - std, mean + std, color=method_styles[exp_name_formatter(exp_name)]['color'], alpha=0.2)\n",
        "\n",
        "# Set title and labels with enhanced formatting\n",
        "plt.title(f\"Performance Comparison Under {current_algo.upper()} Expert Agent\", fontsize=20, fontweight='bold')\n",
        "plt.xlabel(\"Date\", fontsize=16, fontweight='bold')\n",
        "plt.ylabel(\"Total Asset Value ($)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# import matplotlib.dates as mdates\n",
        "plt.xticks(result.index[0::30])\n",
        "# Add 'Test Phase' annotation with date range\n",
        "plt.text(0.5, 0.95, 'Test Phase: July 1, 2020 - October 29, 2021',\n",
        "            transform=plt.gca().transAxes, fontsize=14, ha='center',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.5))\n",
        "\n",
        "# After all lines are plotted, sort the legend alphabetically\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "sorted_pairs = sorted(zip(labels, handles), key=lambda t: t[0].lower())  # Sort alphabetically, case-insensitive\n",
        "sorted_labels, sorted_handles = zip(*sorted_pairs)\n",
        "\n",
        "# Position legend outside the plot with sorted items\n",
        "plt.legend(sorted_handles, sorted_labels, loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=12)\n",
        "\n",
        "# Enhance layout and aesthetics\n",
        "plt.tight_layout()\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Save the plot with an informative filename\n",
        "plt.savefig(f'performance_comparison_DT-LoRA-GPT2_{current_algo.upper()}_Expert.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"Results saved as 'performance_comparison_DT-LoRA-GPT2_{current_algo.upper()}_Expert.png'\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXCLrSGJlq7t"
      },
      "source": [
        "## Eval RL-only (refactored)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init env\n",
        "\n",
        "# Define environment parameters\n",
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": config['initial_amount'],\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": config['cost_pct'],\n",
        "    \"sell_cost_pct\": config['cost_pct'],\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "# Initialize trading environment\n",
        "e_trade_gym = StockTradingEnv(\n",
        "    df=trade,\n",
        "    turbulence_threshold=70, risk_indicator_col='vix',\n",
        "    **env_kwargs\n",
        ")\n",
        "\n",
        "env_trade, obs_trade = e_trade_gym.get_sb_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "86d5b55a-3c4c-459e-c8a9-be1457bfd63c",
        "id": "BFVsIIEPBu9_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load models\n",
        "\n",
        "# Load variables from the config\n",
        "if_using_a2c = config[\"if_using_a2c\"]\n",
        "if_using_ddpg = config[\"if_using_ddpg\"]\n",
        "if_using_ppo = config[\"if_using_ppo\"]\n",
        "if_using_td3 = config[\"if_using_td3\"]\n",
        "if_using_sac = config[\"if_using_sac\"]\n",
        "\n",
        "# Ensure at least one algorithm is enabled\n",
        "if not any([if_using_a2c, if_using_ddpg, if_using_ppo, if_using_td3, if_using_sac]):\n",
        "    raise ValueError(\"At least one algorithm must be set to True for the script to run.\")\n",
        "\n",
        "# Load trained models\n",
        "trained_a2c = A2C.load(os.path.join(TRAINED_MODEL_DIR, \"agent_a2c\")) if if_using_a2c else None\n",
        "trained_ddpg = DDPG.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ddpg\")) if if_using_ddpg else None\n",
        "trained_ppo = PPO.load(os.path.join(TRAINED_MODEL_DIR, \"agent_ppo\")) if if_using_ppo else None\n",
        "trained_td3 = TD3.load(os.path.join(TRAINED_MODEL_DIR, \"agent_td3\")) if if_using_td3 else None\n",
        "trained_sac = SAC.load(os.path.join(TRAINED_MODEL_DIR, \"agent_sac\")) if if_using_sac else None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MiwF1-VifxMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG3nOgG7jh9g",
        "outputId": "3459dece-f0dd-48d0-f06b-316c39553624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hit end!\n",
            "hit end!\n",
            "hit end!\n",
            "hit end!\n",
            "hit end!\n"
          ]
        }
      ],
      "source": [
        "#@title get predictions\n",
        "\n",
        "# Reset the result DataFrame\n",
        "result = pd.DataFrame()\n",
        "\n",
        "# Predict and store results for all enabled algorithms\n",
        "for algo_name, trained_model, is_enabled in [\n",
        "    (\"A2C\", trained_a2c, if_using_a2c),\n",
        "    (\"DDPG\", trained_ddpg, if_using_ddpg),\n",
        "    (\"PPO\", trained_ppo, if_using_ppo),\n",
        "    (\"TD3\", trained_td3, if_using_td3),\n",
        "    (\"SAC\", trained_sac, if_using_sac)\n",
        "]:\n",
        "    if is_enabled:\n",
        "        df_account_value, df_actions = DRLAgent.DRL_prediction(\n",
        "            model=trained_model, environment=e_trade_gym\n",
        "        )\n",
        "        df_result = df_account_value.set_index(df_account_value.columns[0])\n",
        "        df_result.columns = [f\"{algo_name}_{col}\" for col in df_result.columns]\n",
        "        result = pd.merge(result, df_result, how=\"outer\", left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load DIJA for test period\n",
        "\n",
        "# Define test period\n",
        "TEST_START_DATE = trade['date'].iloc[0]\n",
        "TEST_END_DATE = trade['date'].iloc[-1]\n",
        "\n",
        "# Fetch DJIA data for the test period\n",
        "df_dji = YahooDownloader(\n",
        "    start_date=TEST_START_DATE,\n",
        "    end_date=TEST_END_DATE,\n",
        "    ticker_list=['dji']\n",
        ").fetch_data()\n",
        "\n",
        "# Merge DJIA data\n",
        "df_dji = df_dji[['date','close']]\n",
        "fst_day = df_dji['close'].iloc[0]\n",
        "dji = pd.DataFrame({\n",
        "    'DJIA': df_dji['close'].div(fst_day).mul(1000000).values\n",
        "}, index=df_dji['date'])\n",
        "\n",
        "# Merge DJIA data using inner join to ensure alignment\n",
        "result = pd.merge(result, dji, how='inner', left_index=True, right_index=True).fillna(method='bfill')\n",
        "\n",
        "# Create column names list with better formatting\n",
        "col_name = []\n",
        "if if_using_a2c: col_name.append('A2C')\n",
        "if if_using_ddpg: col_name.append('DDPG')\n",
        "if if_using_td3: col_name.append('TD3')\n",
        "if if_using_sac: col_name.append('SAC')\n",
        "if if_using_ppo: col_name.append('PPO')\n",
        "col_name.extend(result.columns[len(col_name):])  # Add all other column names\n",
        "result.columns = col_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GTJrVlMbg4--",
        "outputId": "9d699abc-4db8-48ac-e01a-720a2e3df10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of DataFrame:  (240, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [DEPRECATED] filter experiments\n",
        "\n",
        "# Control variables\n",
        "include_ensemble = False  # Set to False to exclude ensemble experiments\n",
        "exclude_algo_experiments = False  # Set to True to exclude individual algo experiments based on if_using_... flags\n",
        "\n",
        "if include_ensemble:\n",
        "    algos_included += '_ensemble'\n",
        "\n",
        "# Function to extract experiment name from directory name\n",
        "def extract_experiment_name(dir_name):\n",
        "    # This regex matches everything up to the last underscore and number\n",
        "    match = re.match(r'(.+)_\\d+$', dir_name)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return dir_name\n",
        "\n",
        "# Function to check if an experiment should be included\n",
        "def should_include_experiment(exp_name):\n",
        "    if not include_ensemble and 'ensemble' in exp_name.lower():\n",
        "        return False\n",
        "    if exclude_algo_experiments:\n",
        "        algo_flags = {\n",
        "            'a2c': if_using_a2c,\n",
        "            'ddpg': if_using_ddpg,\n",
        "            'ppo': if_using_ppo,\n",
        "            'td3': if_using_td3,\n",
        "            'sac': if_using_sac\n",
        "        }\n",
        "        for algo, flag in algo_flags.items():\n",
        "            if algo in exp_name.lower() and not flag:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# Group similar experiments\n",
        "experiment_groups = {}\n",
        "for column in result.columns:\n",
        "    if column not in ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO', 'DJIA']:\n",
        "        exp_name = extract_experiment_name(column.split('_total_asset_value_change_test')[0])\n",
        "        if should_include_experiment(exp_name):\n",
        "            if exp_name not in experiment_groups:\n",
        "                experiment_groups[exp_name] = []\n",
        "            experiment_groups[exp_name].append(column)\n",
        "\n",
        "experiment_groups"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7j-tzDnmiSvM",
        "outputId": "1075fcf6-41dd-4bda-bf92-da072227e3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [DEPRECATED] Calculate metrics for each experiment group\n",
        "\n",
        "# SHOULD USE ONE metrics_dict when running with groups (delete second initialization)\n",
        "metrics_dict = {\n",
        "    'Method': [],\n",
        "    'Cumulative Return Mean (%)': [],\n",
        "    'Cumulative Return Std (%)': [],\n",
        "    'MDD Mean (%)': [],\n",
        "    'MDD Std (%)': [],\n",
        "    'Sharpe Ratio Mean': [],\n",
        "    'Sharpe Ratio Std': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each experiment group\n",
        "experiment_stats = {}\n",
        "for exp_name, columns in experiment_groups.items():\n",
        "    exp_data = result[columns].dropna()\n",
        "\n",
        "    if exp_data.empty:\n",
        "        print(f\"Warning: No valid data for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "        continue\n",
        "\n",
        "    # Cumulative Return: (Final - Initial) / Initial * 100 for each run\n",
        "    cumulative_returns = (exp_data.iloc[-1] - exp_data.iloc[0]) / exp_data.iloc[0] * 100\n",
        "\n",
        "    # Handle potential division by zero or invalid calculations\n",
        "    cumulative_returns = cumulative_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    if cumulative_returns.empty:\n",
        "        print(f\"Warning: No valid cumulative returns for experiment '{exp_name}'. Skipping metrics calculation.\")\n",
        "        continue\n",
        "\n",
        "    cumulative_return_mean = cumulative_returns.mean()\n",
        "    cumulative_return_std = cumulative_returns.std()\n",
        "\n",
        "    # MDD: Calculate MDD for each run\n",
        "    mdd_values = []\n",
        "    for col in columns:\n",
        "        asset_values = result[col].dropna()\n",
        "        if asset_values.empty:\n",
        "            continue\n",
        "        mdd_run = calculate_mdd(asset_values)\n",
        "        mdd_values.append(mdd_run)\n",
        "\n",
        "    if not mdd_values:\n",
        "        print(f\"Warning: No valid MDD values for experiment '{exp_name}'. Skipping MDD calculation.\")\n",
        "        mdd_mean = np.nan\n",
        "        mdd_std = np.nan\n",
        "    else:\n",
        "        mdd_mean = np.mean(mdd_values)\n",
        "        mdd_std = np.std(mdd_values)\n",
        "\n",
        "    # Sharpe Ratio: Calculate Sharpe for each run\n",
        "    sharpe_ratios = []\n",
        "    for col in columns:\n",
        "        asset_values = result[col].dropna()\n",
        "        if asset_values.empty:\n",
        "            continue\n",
        "        sharpe_run = calculate_sharpe_ratio(asset_values)\n",
        "        sharpe_ratios.append(sharpe_run)\n",
        "\n",
        "    if not sharpe_ratios:\n",
        "        print(f\"Warning: No valid Sharpe Ratios for experiment '{exp_name}'. Skipping Sharpe Ratio calculation.\")\n",
        "        sharpe_mean = np.nan\n",
        "        sharpe_std = np.nan\n",
        "    else:\n",
        "        sharpe_mean = np.mean(sharpe_ratios)\n",
        "        sharpe_std = np.std(sharpe_ratios)\n",
        "\n",
        "    # Append to metrics_dict with mapped label\n",
        "    mapped_exp_name = label_mapping.get(exp_name, exp_name)\n",
        "    metrics_dict['Method'].append(mapped_exp_name)\n",
        "    metrics_dict['Cumulative Return Mean (%)'].append(cumulative_return_mean)\n",
        "    metrics_dict['Cumulative Return Std (%)'].append(cumulative_return_std)\n",
        "    metrics_dict['MDD Mean (%)'].append(mdd_mean)\n",
        "    metrics_dict['MDD Std (%)'].append(mdd_std)\n",
        "    metrics_dict['Sharpe Ratio Mean'].append(sharpe_mean)\n",
        "    metrics_dict['Sharpe Ratio Std'].append(sharpe_std)\n",
        "\n",
        "    # Store in experiment_stats for plotting\n",
        "    experiment_stats[mapped_exp_name] = {'mean': exp_data.mean(axis=1), 'std': exp_data.std(axis=1)}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E5apsbA0il_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate metrics for individual algorithms\n",
        "\n",
        "metrics_dict = {\n",
        "    'Method': [],\n",
        "    'Cumulative Return Mean (%)': [],\n",
        "    'Cumulative Return Std (%)': [],\n",
        "    'MDD Mean (%)': [],\n",
        "    'MDD Std (%)': [],\n",
        "    'Sharpe Ratio Mean': [],\n",
        "    'Sharpe Ratio Std': []\n",
        "}\n",
        "\n",
        "result.index = pd.to_datetime(result.index)\n",
        "individual_algos = ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO']\n",
        "for algo in individual_algos:\n",
        "    if algo in result.columns:\n",
        "        # # Check if this algorithm is already part of experiment_groups\n",
        "        # if label_mapping.get(algo, algo) in experiment_stats:\n",
        "        #     print(f\"Info: '{algo}' is already included in experiment groups. Skipping individual plotting to avoid duplication.\")\n",
        "        #     continue  # Skip to prevent duplicate plotting\n",
        "\n",
        "        asset_values = result[algo].dropna()\n",
        "        if asset_values.empty:\n",
        "            print(f\"Warning: No valid asset values for individual algorithm '{algo}'. Skipping metrics calculation.\")\n",
        "            continue\n",
        "        # Cumulative Return\n",
        "        cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "        # Handle potential division by zero or invalid calculations\n",
        "        if np.isinf(cum_ret) or np.isnan(cum_ret):\n",
        "            cum_ret = np.nan\n",
        "        # MDD\n",
        "        mdd = calculate_mdd(asset_values)\n",
        "        # Sharpe Ratio\n",
        "        sharpe = calculate_sharpe_ratio(asset_values)\n",
        "        # Append to metrics_dict with mapped label\n",
        "        mapped_algo = label_mapping.get(algo, algo)\n",
        "        metrics_dict['Method'].append(mapped_algo)\n",
        "        metrics_dict['Cumulative Return Mean (%)'].append(cum_ret)\n",
        "        metrics_dict['Cumulative Return Std (%)'].append(0.00)  # Single run, std is 0\n",
        "        metrics_dict['MDD Mean (%)'].append(mdd)\n",
        "        metrics_dict['MDD Std (%)'].append(0.00)  # Single run, std is 0\n",
        "        metrics_dict['Sharpe Ratio Mean'].append(sharpe)\n",
        "        metrics_dict['Sharpe Ratio Std'].append(0.00)  # Single run, std is 0\n",
        "\n",
        "        # Store in experiment_stats for plotting\n",
        "        experiment_stats[mapped_algo] = {'mean': asset_values, 'std': pd.Series([0]*len(asset_values), index=asset_values.index)}\n",
        "\n",
        "# Convert metrics_dict to DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_dict)\n",
        "\n",
        "# Drop any rows with NaN metrics to ensure clean tables\n",
        "metrics_df = metrics_df.dropna(subset=['Cumulative Return Mean (%)', 'MDD Mean (%)', 'Sharpe Ratio Mean'])\n",
        "\n",
        "# Create summary DataFrame with formatted strings\n",
        "metrics_summary_df = metrics_df.copy()\n",
        "metrics_summary_df['Cumulative Return (%)'] = metrics_df['Cumulative Return Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['Cumulative Return Std (%)'].round(2).astype(str)\n",
        "metrics_summary_df['MDD (%)'] = metrics_df['MDD Mean (%)'].round(2).astype(str) + \" ± \" + metrics_df['MDD Std (%)'].round(2).astype(str)\n",
        "metrics_summary_df['Sharpe Ratio'] = metrics_df['Sharpe Ratio Mean'].round(2).astype(str) + \" ± \" + metrics_df['Sharpe Ratio Std'].round(2).astype(str)\n",
        "metrics_summary_df = metrics_summary_df[['Method', 'Cumulative Return (%)', 'MDD (%)', 'Sharpe Ratio']]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p_q54wrPnAuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Um3RstNhSmR",
        "outputId": "4def23c1-ed2b-4f51-946d-288b4b2acf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Metrics Comparison ===\n",
            "Method Cumulative Return (%)      MDD (%) Sharpe Ratio\n",
            "   A2C           11.16 ± 0.0 -19.65 ± 0.0   0.52 ± 0.0\n",
            "  DDPG             3.5 ± 0.0 -23.02 ± 0.0   0.27 ± 0.0\n",
            "   TD3           16.08 ± 0.0 -19.77 ± 0.0   0.72 ± 0.0\n",
            "   SAC            2.82 ± 0.0 -22.32 ± 0.0   0.24 ± 0.0\n",
            "   PPO           13.18 ± 0.0 -33.54 ± 0.0   0.54 ± 0.0\n",
            "\n",
            "\n",
            "=== Rankings ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "TD3: 16.08%\n",
            "PPO: 13.18%\n",
            "A2C: 11.16%\n",
            "DDPG: 3.50%\n",
            "SAC: 2.82%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower is Better]:\n",
            "PPO: -33.54%\n",
            "DDPG: -23.02%\n",
            "SAC: -22.32%\n",
            "TD3: -19.77%\n",
            "A2C: -19.65%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "TD3: 0.72\n",
            "PPO: 0.54\n",
            "A2C: 0.52\n",
            "DDPG: 0.27\n",
            "SAC: 0.24\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Print the comparison table\n",
        "\n",
        "print(f\"\\n=== Metrics Comparison ===\")\n",
        "print(metrics_summary_df.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create separate DataFrames for ranking\n",
        "ranking_cum_ret = metrics_df[['Method', 'Cumulative Return Mean (%)']].copy()\n",
        "ranking_cum_ret = ranking_cum_ret.sort_values(by='Cumulative Return Mean (%)', ascending=False)\n",
        "\n",
        "ranking_mdd = metrics_df[['Method', 'MDD Mean (%)']].copy()\n",
        "ranking_mdd = ranking_mdd.sort_values(by='MDD Mean (%)', ascending=True)  # Lower MDD is better\n",
        "\n",
        "ranking_sharpe = metrics_df[['Method', 'Sharpe Ratio Mean']].copy()\n",
        "ranking_sharpe = ranking_sharpe.sort_values(by='Sharpe Ratio Mean', ascending=False)\n",
        "\n",
        "# Print rankings\n",
        "print(f\"=== Rankings ===\")\n",
        "\n",
        "print(\"\\nCumulative Return (%):\")\n",
        "for idx, row in ranking_cum_ret.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Cumulative Return Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nMaximum Drawdown (MDD %) [Lower is Better]:\")\n",
        "for idx, row in ranking_mdd.iterrows():\n",
        "    print(f\"{row['Method']}: {row['MDD Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nSharpe Ratio [Higher is Better]:\")\n",
        "for idx, row in ranking_sharpe.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Sharpe Ratio Mean']:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Debugging: Check if all means align with result.index\n",
        "for exp_name, stats in experiment_stats.items():\n",
        "    mean_length = len(stats['mean'])\n",
        "    result_length = len(result.index)\n",
        "    if mean_length != result_length:\n",
        "        print(f\"Warning: Mean length for '{exp_name}' ({mean_length}) does not match result index length ({result_length}). Reindexing.\")\n",
        "        experiment_stats[exp_name]['mean'] = stats['mean'].reindex(result.index).fillna(method='ffill')\n",
        "        experiment_stats[exp_name]['std'] = stats['std'].reindex(result.index).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate metrics for individual algorithms (w/annualized returns)\n",
        "\n",
        "def calculate_metrics(result):\n",
        "    metrics_dict = {\n",
        "        'Method': [],\n",
        "        'Cumulative Return Mean (%)': [],\n",
        "        # 'Cumulative Return Std (%)': [],\n",
        "        'Annualized Return Mean (%)': [],\n",
        "        # 'Annualized Return Std (%)': [],\n",
        "        'MDD Mean (%)': [],\n",
        "        'MDD Std (%)': [],\n",
        "        'Sharpe Ratio Mean': [],\n",
        "        'Sharpe Ratio Std': []\n",
        "    }\n",
        "\n",
        "    individual_algos = ['A2C', 'DDPG', 'TD3', 'SAC', 'PPO']\n",
        "    for algo in individual_algos:\n",
        "        if algo in result.columns:\n",
        "            # # Check if this algorithm is already part of experiment_groups\n",
        "            # if label_mapping.get(algo, algo) in experiment_stats:\n",
        "            #     print(f\"Info: '{algo}' is already included in experiment groups. Skipping individual plotting to avoid duplication.\")\n",
        "            #     continue  # Skip to prevent duplicate plotting\n",
        "\n",
        "            asset_values = result[algo].dropna()\n",
        "            if asset_values.empty:\n",
        "                print(f\"Warning: No valid asset values for individual algorithm '{algo}'. Skipping metrics calculation.\")\n",
        "                continue\n",
        "\n",
        "            cum_ret = (asset_values.iloc[-1] - asset_values.iloc[0]) / asset_values.iloc[0] * 100\n",
        "\n",
        "            # Handle potential division by zero or invalid calculations\n",
        "            if np.isinf(cum_ret) or np.isnan(cum_ret):\n",
        "                cum_ret = np.nan\n",
        "\n",
        "            # Calculate annualized return\n",
        "            num_days = (asset_values.index[-1] - asset_values.index[0]).days\n",
        "            ann_ret = ( (1 + cum_ret / 100) ** (365 / num_days) - 1 ) * 100\n",
        "\n",
        "            # MDD\n",
        "            mdd = calculate_mdd(asset_values)\n",
        "            # Sharpe Ratio\n",
        "            sharpe = calculate_sharpe_ratio(asset_values)\n",
        "            # Append to metrics_dict with mapped label\n",
        "            mapped_algo = label_mapping.get(algo, algo)\n",
        "            metrics_dict['Method'].append(mapped_algo)\n",
        "            metrics_dict['Cumulative Return Mean (%)'].append(cum_ret)\n",
        "            # metrics_dict['Cumulative Return Std (%)'].append(0.00)  # Single run, std is 0\n",
        "            metrics_dict['Annualized Return Mean (%)'].append(ann_ret)\n",
        "            # metrics_dict['Annualized Return Std (%)'].append(0.00)  # Single run, std is 0\n",
        "            metrics_dict['MDD Mean (%)'].append(mdd)\n",
        "            metrics_dict['MDD Std (%)'].append(0.00)  # Single run, std is 0\n",
        "            metrics_dict['Sharpe Ratio Mean'].append(sharpe)\n",
        "            metrics_dict['Sharpe Ratio Std'].append(0.00)  # Single run, std is 0\n",
        "\n",
        "            # Store in experiment_stats for plotting\n",
        "            experiment_stats[mapped_algo] = {'mean': asset_values, 'std': pd.Series([0]*len(asset_values), index=asset_values.index)}\n",
        "\n",
        "    # Convert metrics_dict to DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics_dict)\n",
        "\n",
        "    # Drop any rows with NaN metrics to ensure clean tables\n",
        "    metrics_df = metrics_df.dropna(subset=['Cumulative Return Mean (%)', 'Annualized Return Mean (%)', 'MDD Mean (%)', 'Sharpe Ratio Mean'])\n",
        "\n",
        "    # Create summary DataFrame with formatted strings\n",
        "    metrics_summary_df = metrics_df.copy()\n",
        "    metrics_summary_df['Cumulative Return (%)'] = metrics_df['Cumulative Return Mean (%)'].round(2).astype(str) \\\n",
        "        #  + \" ± \" + metrics_df['Cumulative Return Std (%)'].round(2).astype(str)\n",
        "    metrics_summary_df['Annualized Return (%)'] = metrics_df['Annualized Return Mean (%)'].round(2).astype(str) \\\n",
        "        #  + \" ± \" + metrics_df['Annualized Return Std (%)'].round(2).astype(str)\n",
        "    metrics_summary_df['MDD (%)'] = metrics_df['MDD Mean (%)'].round(2).astype(str) \\\n",
        "        #  + \" ± \" + metrics_df['MDD Std (%)'].round(2).astype(str)\n",
        "    metrics_summary_df['Sharpe Ratio'] = metrics_df['Sharpe Ratio Mean'].round(2).astype(str) \\\n",
        "        #  + \" ± \" + metrics_df['Sharpe Ratio Std'].round(2).astype(str)\n",
        "    metrics_summary_df = metrics_summary_df[['Method', 'Cumulative Return (%)', 'Annualized Return (%)', 'MDD (%)', 'Sharpe Ratio']]\n",
        "\n",
        "    return metrics_summary_df\n",
        "\n",
        "metrics_summary_df = calculate_metrics(result)\n",
        "metrics_summary_df"
      ],
      "metadata": {
        "id": "oqjPITSj99E9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5a01fd02-96c5-42ab-f149-0d24c1c74897"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Method Cumulative Return (%) Annualized Return (%) MDD (%) Sharpe Ratio\n",
              "0    A2C                 11.16                 11.29  -19.65         0.52\n",
              "1   DDPG                   3.5                  3.54  -23.02         0.27\n",
              "2    TD3                 16.08                 16.27  -19.77         0.72\n",
              "3    SAC                  2.82                  2.85  -22.32         0.24\n",
              "4    PPO                 13.18                 13.34  -33.54         0.54"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f383defb-f8db-4a74-abca-ceb2089269ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Cumulative Return (%)</th>\n",
              "      <th>Annualized Return (%)</th>\n",
              "      <th>MDD (%)</th>\n",
              "      <th>Sharpe Ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A2C</td>\n",
              "      <td>11.16</td>\n",
              "      <td>11.29</td>\n",
              "      <td>-19.65</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDPG</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.54</td>\n",
              "      <td>-23.02</td>\n",
              "      <td>0.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TD3</td>\n",
              "      <td>16.08</td>\n",
              "      <td>16.27</td>\n",
              "      <td>-19.77</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SAC</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.85</td>\n",
              "      <td>-22.32</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPO</td>\n",
              "      <td>13.18</td>\n",
              "      <td>13.34</td>\n",
              "      <td>-33.54</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f383defb-f8db-4a74-abca-ceb2089269ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f383defb-f8db-4a74-abca-ceb2089269ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f383defb-f8db-4a74-abca-ceb2089269ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-07fb8f4f-f7c5-4f54-b61c-6d6a98e4a4b7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07fb8f4f-f7c5-4f54-b61c-6d6a98e4a4b7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-07fb8f4f-f7c5-4f54-b61c-6d6a98e4a4b7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_summary_df",
              "summary": "{\n  \"name\": \"metrics_summary_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"DDPG\",\n          \"PPO\",\n          \"TD3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cumulative Return (%)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"3.5\",\n          \"13.18\",\n          \"16.08\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Annualized Return (%)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"3.54\",\n          \"13.34\",\n          \"16.27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MDD (%)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"-23.02\",\n          \"-33.54\",\n          \"-19.77\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sharpe Ratio\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"0.27\",\n          \"0.54\",\n          \"0.72\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print the comparison table (w/annualized returns)\n",
        "\n",
        "print(f\"\\n=== Metrics Comparison ===\")\n",
        "print(metrics_summary_df.to_string(index=False))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create separate DataFrames for rankings\n",
        "ranking_cum_ret = metrics_df[['Method', 'Cumulative Return Mean (%)']].copy()\n",
        "ranking_cum_ret = ranking_cum_ret.sort_values(by='Cumulative Return Mean (%)', ascending=False)\n",
        "\n",
        "ranking_annualized_ret = metrics_df[['Method', 'Annualized Return Mean (%)']].copy()\n",
        "ranking_annualized_ret = ranking_annualized_ret.sort_values(by='Annualized Return Mean (%)', ascending=False)\n",
        "\n",
        "ranking_mdd = metrics_df[['Method', 'MDD Mean (%)']].copy()\n",
        "ranking_mdd = ranking_mdd.sort_values(by='MDD Mean (%)', ascending=False)  # Lower abs(MDD) is better\n",
        "\n",
        "ranking_sharpe = metrics_df[['Method', 'Sharpe Ratio Mean']].copy()\n",
        "ranking_sharpe = ranking_sharpe.sort_values(by='Sharpe Ratio Mean', ascending=False)\n",
        "\n",
        "# Print rankings\n",
        "print(f\"=== Rankings ===\")\n",
        "\n",
        "print(\"\\nCumulative Return (%):\")\n",
        "for idx, row in ranking_cum_ret.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Cumulative Return Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nAnnualized Return (%):\")\n",
        "for idx, row in ranking_annualized_ret.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Annualized Return Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nMaximum Drawdown (MDD %) [Lower absolute values is Better]:\")\n",
        "for idx, row in ranking_mdd.iterrows():\n",
        "    print(f\"{row['Method']}: {row['MDD Mean (%)']:.2f}%\")\n",
        "\n",
        "print(\"\\nSharpe Ratio [Higher is Better]:\")\n",
        "for idx, row in ranking_sharpe.iterrows():\n",
        "    print(f\"{row['Method']}: {row['Sharpe Ratio Mean']:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Debugging: Check if all means align with result.index\n",
        "for exp_name, stats in experiment_stats.items():\n",
        "    mean_length = len(stats['mean'])\n",
        "    result_length = len(result.index)\n",
        "    if mean_length != result_length:\n",
        "        print(f\"Warning: Mean length for '{exp_name}' ({mean_length}) does not match result index length ({result_length}). Reindexing.\")\n",
        "        experiment_stats[exp_name]['mean'] = stats['mean'].reindex(result.index).fillna(method='ffill')\n",
        "        experiment_stats[exp_name]['std'] = stats['std'].reindex(result.index).fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htgsl-iu5dU_",
        "outputId": "92a84c48-6a88-4c5d-9614-9c17b874f75c",
        "cellView": "form"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Metrics Comparison ===\n",
            "Method Cumulative Return (%) Annualized Return (%) MDD (%) Sharpe Ratio\n",
            "   A2C                 11.16                 11.29  -19.65         0.52\n",
            "  DDPG                   3.5                  3.54  -23.02         0.27\n",
            "   TD3                 16.08                 16.27  -19.77         0.72\n",
            "   SAC                  2.82                  2.85  -22.32         0.24\n",
            "   PPO                 13.18                 13.34  -33.54         0.54\n",
            "\n",
            "\n",
            "=== Rankings ===\n",
            "\n",
            "Cumulative Return (%):\n",
            "TD3: 16.08%\n",
            "PPO: 13.18%\n",
            "A2C: 11.16%\n",
            "DDPG: 3.50%\n",
            "SAC: 2.82%\n",
            "\n",
            "Annualized Return (%):\n",
            "TD3: 16.27%\n",
            "PPO: 13.34%\n",
            "A2C: 11.29%\n",
            "DDPG: 3.54%\n",
            "SAC: 2.85%\n",
            "\n",
            "Maximum Drawdown (MDD %) [Lower absolute values is Better]:\n",
            "A2C: -19.65%\n",
            "TD3: -19.77%\n",
            "SAC: -22.32%\n",
            "DDPG: -23.02%\n",
            "PPO: -33.54%\n",
            "\n",
            "Sharpe Ratio [Higher is Better]:\n",
            "TD3: 0.72\n",
            "PPO: 0.54\n",
            "A2C: 0.52\n",
            "DDPG: 0.27\n",
            "SAC: 0.24\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title log_test_metrics\n",
        "def log_test_metrics():\n",
        "    # Calculate maximum values for each metric\n",
        "    max_cum_return = metrics_summary_df['Cumulative Return (%)'].max()\n",
        "    max_ann_return = metrics_summary_df['Annualized Return (%)'].max()\n",
        "    max_mdd = metrics_summary_df['MDD (%)'].max()\n",
        "    max_sharpe_ratio = metrics_summary_df['Sharpe Ratio'].max()\n",
        "\n",
        "    # Log metrics for each model and maximum values\n",
        "    for _, row in metrics_summary_df.iterrows():\n",
        "        model_name = row['Method'].lower()  # Convert the model name to lowercase\n",
        "\n",
        "        # Log per-model metrics\n",
        "        wandb.run.log({\n",
        "            'test': {\n",
        "                f'{model_name}/cum_return': row['Cumulative Return (%)'],\n",
        "                f'{model_name}/ann_return': row['Annualized Return (%)'],\n",
        "                f'{model_name}/mdd': row['MDD (%)'],\n",
        "                f'{model_name}/sharpe_ratio': row['Sharpe Ratio']\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Log maximum values across models\n",
        "    wandb.run.log({\n",
        "        'test': {\n",
        "            'max_cum_return': max_cum_return,\n",
        "            'max_ann_return': max_ann_return,\n",
        "            'max_mdd': max_mdd,\n",
        "            'max_sharpe_ratio': max_sharpe_ratio\n",
        "        }\n",
        "    })"
      ],
      "metadata": {
        "id": "lAk7W7YEdlYm"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plot results\n",
        "\n",
        "# Plotting section\n",
        "plt.figure(figsize=(16, 9))  # Increased figure size for better readability\n",
        "method_styles = {\n",
        "    'CQL': {'color': '#1f77b4', 'linestyle': '-'},           # Blue solid\n",
        "    'IQL': {'color': '#ff7f0e', 'linestyle': '--'},          # Orange dashed\n",
        "    'BC': {'color': '#2ca02c', 'linestyle': '-.'},           # Green dash-dot\n",
        "    'DT LoRA GPT2': {'color': '#d62728', 'linestyle': ':'},  # Red dotted\n",
        "    'DT LoRA Random Weight GPT2': {'color': '#9467bd', 'linestyle': '-'},  # Purple solid\n",
        "    'A2C': {'color': '#8c564b', 'linestyle': '--'},          # Brown dashed\n",
        "    'DDPG': {'color': '#e377c2', 'linestyle': '-'},          # Pink solid\n",
        "    'PPO': {'color': '#7f7f7f', 'linestyle': '-'},           # Gray solid\n",
        "    'TD3': {'color': '#bcbd22', 'linestyle': '--'},          # Olive dashed\n",
        "    'SAC': {'color': '#17becf', 'linestyle': '-'},           # Cyan solid\n",
        "    'DJIA': {'color': '#000000', 'linestyle': '-'},          # Black solid\n",
        "    # Add more methods here if needed\n",
        "}\n",
        "# Plot DJIA\n",
        "plt.plot(result.index, result['DJIA'], label=\"Dow Jones Index\", linestyle=method_styles['DJIA']['linestyle'], color=method_styles['DJIA']['color'])\n",
        "\n",
        "# Define color palette and line styles\n",
        "color_palette = plt.get_cmap('tab10').colors  # Colorblind-friendly palette\n",
        "line_styles = ['-', '--', '-.', ':']  # Different line styles\n",
        "\n",
        "# Plot experiment groups\n",
        "for idx, (exp_name, stats) in enumerate(experiment_stats.items()):\n",
        "    mean = stats['mean']\n",
        "    std = stats['std']\n",
        "\n",
        "    # Ensure mean and std are aligned with result.index\n",
        "    mean = mean.reindex(result.index).fillna(method='ffill')\n",
        "    std = std.reindex(result.index).fillna(0)\n",
        "\n",
        "    # Assign colors and line styles\n",
        "    # color = color_palette[idx % len(color_palette)]\n",
        "    # linestyle = line_styles[idx % len(line_styles)]\n",
        "\n",
        "    def exp_name_formatter(exp_name):\n",
        "        exp_names = exp_name.split('_')\n",
        "        if len(exp_names) == 1:\n",
        "            return exp_name\n",
        "        elif len(exp_names) == 2:\n",
        "            return exp_names[1].upper()\n",
        "        elif len(exp_names) == 3:\n",
        "            return None\n",
        "        elif len(exp_names) == 4:\n",
        "            return exp_names[1].upper() + ' LoRA ' + 'GPT2'\n",
        "        elif len(exp_names) == 6:\n",
        "            return exp_names[1].upper() + ' LoRA ' + 'Random Weight ' + 'GPT2'\n",
        "        else:\n",
        "            return exp_name\n",
        "\n",
        "    # Plot mean\n",
        "    line, = plt.plot(result.index, mean, label=exp_name_formatter(exp_name), linestyle=method_styles[exp_name_formatter(exp_name)]['linestyle'], color=method_styles[exp_name_formatter(exp_name)]['color'])\n",
        "\n",
        "    # Plot error bandsy (mean ± 1 std)\n",
        "    plt.fill_between(result.index, mean - std, mean + std, color=method_styles[exp_name_formatter(exp_name)]['color'], alpha=0.2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_z4soDAjnutM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k_IxXsJXHbl_",
        "M_55bSnxe_3R",
        "Qh7UZGp3e7dH"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPnracKPFuC03ljYCiVk2H1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}